the association for computational heresy
presents

a record of the proceedings of

SIGBOVIK 2020

the fourteenth annual intercalary robot dance party in celebration
of workshop on symposium about 26 th birthdays; in particular,
that of harry q. bovik

cover art by chris yu
global chaos courtesy of sars-cov-2

carnegie mellon university
pittsburgh, pa
april 1, 2020

i

SIGBOVIK
A Record of the Proceedings of SIGBOVIK 2020
ISSN 2155-0166
April 1, 2020

Copyright is maintained by the individual authors, though obviously this all gets posted to the
Internet and stuff, because it’s 2020.
Permission to make digital or hard copies of portions of this work for personal use is granted;
permission to make digital or hard copies of portions of this work for classroom use is also granted,
but seems ill-advised. Abstracting with credit is permitted; abstracting with credit cards seems
difficult.
Additional copies of this work may be ordered from Lulu; refer to http://sigbovik.org for
details.

ii

SIGBOVIK 2020
Message from the Organizing Committee

For generality’s sake, we have templatized the message of the organizing committee. The actual
message may be produced by running the TeX command at the end.
\newcommand{\Message2020}[3]{
% TODO: generalize ordinal indicators
Friends, family, colleagues, acquaintances for whom we have not yet overcome the activation
energy to engage with regularly, and strangers doomed to the same fate: The Association of Computational Heresy welcomes you to the #1th annual meeting of the Special Interest Group on Harry
Q#2 Bovik in celebration of Bovik’s #3th birthday. In lieu of data about the submission and review
process this year, we encourage you to ponder the perils of empiricism as well as our innovations
in the publishing process: most notably, triple-blind peer review. In plain English, we have the
following definition.
triple-blind peer review /"trIpl-blaInd pi:r rI"vju:/ noun
1. Scholarly peer review that minimizes bias by concealing not only the identities of the authors and reviewers from each other, but also by concealing the papers from the reviewers.
Compare: single- and double-blind peer review.
However, let us undertake some performative formalization. Given a set of names of authors (auth)
and reviewers (rev), we consider the role of author and reviewer as, under the Curry-Howard correspondence, proofs of the following higher-order linear logic propositions/processes conforming
to the following protocols.
` P :: (c : ∃a:auth,p:paperBy(a) ∀r:rev reviewOfBy(p, r) ( bool)
` Q :: (d : ∀a:auth,p:paperBy(a) ∃r:rev reviewOfBy(p, r))
Under duality, it is clear that communication between P and Q is sound. Then, each level of blindness (single to triple) is achieved by successively abstracting the definition of rev for P then auth
and paperBy for Q respectively; we refer to the work of Harper and Lillibridge [1] on translucent

iii

Figure 1: never say NEVER
sums to achieve this. Higher-order notions of blind review require a stronger metatheory like a
linear temporal-linear logical framework (that’s TWO linears!); we encourage future work to investigate this idea. Now that the general chair has redeemed himself for not submitting a paper
this year, we would like to thank our authors and reviewers for their phenomenal work and for
adjusting to the new review process as well as the continuous effort of volunteers who have made
this year’s conference possible, which include but are not limited to: Chris Yu for the cover art,
Catherine Copetas for managing our finances and other administrative concerns, and Ryan Kavanagh for organising the organisers. Moreover, the program chair also thanks Brandon Bohrer
and Stefan Muller for further advice. Lastly, we would like to thank Sol Boucher for assembling
the proceedings as well as Jenny Lin and Siva Somayyajula for never working in various capacities
(see figure 1).
The SIGBOVIK 2020 Organising Committee
Pittsburgh, PA
Jenny Lin (easy chair)
Siva Somayyajula (generalized hard-ass chair)
Sol Boucher (fold-out couch)
Brandon Bohrer (reclining chair)
Ryan Kavanagh (rockin’ chair)
Chris Yu (swivel chair)
Stefan Muller (ergonomic office chair)
}
\Message2020{14}{uarantine}{$2ˆ6$}

References
[1] H ARPER , R., AND L ILLIBRIDGE , M. A type-theoretic approach to higher-order modules with
sharing. pp. 123–137.

iv

Blindsight is 2020
: Reading Skills
3
1
Anonymous paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2
State-of-the-Art Reviewing: A radical proposal to improve scientific publication 9
: SIGBOVIK
3
A thorough investigation of the degree to which the COVID-19 pandemic has
enabled subpar-quality papers to make it into the proceedings of SIGBOVIK,
by reducing the supply of authors willing to invest the necessary effort to
produce high-quality papers . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
Is this the longest Chess game? . . . . . . . . . . . . . . . . . . . . . . . . .
5
Optimizing the SIGBOVIK 2018 speedrun . . . . . . . . . . . . . . . . . . .
6
Retraction of a boring follow-up paper to “Which ITG stepcharts are turniest?” titled, “Which ITG stepcharts are crossoveriest and/or footswitchiest?”

17

: Programming Languages
7
Ntinuation copassing style . . . . . . . . . . . . . . . . . . . . .
8
Formulating the syntactically simplest programming language .
9
Type-directed decompilation of shell scripts . . . . . . . . . . .
10
Verified proof of P=NP in the Silence theorem prover language

.
.
.
.

37
38
40
48
51

: Systems
11
SIGBOVIK ’75 technical note: Conditional move for shell script acceleration
12
NaN-gate synthesis and hardware deceleration . . . . . . . . . . . . . . . . .
13
hbc: A celebration of undefined behavior . . . . . . . . . . . . . . . . . . . .

55
56
61
66

: Theory
14
A polynomial-time SAT algorithm . . . . . . . . . . . . . . . .
15
Empire Machines and beyond . . . . . . . . . . . . . . . . . . .
16
Artificial General Relativity . . . . . . . . . . . . . . . . . . . .
17
Lower gauge theory: Dead duck or phoenix? . . . . . . . . . . .
18
Making explicit a constant appearing in an impossible equation
Moroz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

71
72
73
81
87

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
of Carl and
. . . . . . .

18
20
32
35

88

: Security
93
19 Determining the laziest way to force other people to generate random numbers
for us using IoT vulnerabilities . . . . . . . . . . . . . . . . . . . . . . . . . . 94
20
Putting the ceremony in “authentication ceremony” . . . . . . . . . . . . . 102
: Artificial Intelligence & Machine Learning
21
Image2image neural network for addition and subtraction evaluation of a pair
of not very large numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
Robot ethics: Dangers of reinforcement learning . . . . . . . . . . . . . . . .
23
Learning to be wrong via gradient ascent: A broken clock is right twice a
day, but a clock that runs backwards can be used to tell the time! . . . . . .

1

109
110
113
116

24

GradSchoolNet: Robust end-to-end *-shot unsupervised deepAF neural attention model for convexly optimal (artifically intelligent) success in computer
vision research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121

: Natural Intelligence & Human Learning

257

25

Sorting with human intelligence . . . . . . . . . . . . . . . . . . . . . . . . . 258

26

Synthesizing programs by asking people for help online . . . . . . . . . . . . 264

27

HonkFast, PreHonk, HonkBack, PreHonkBack, HERS, AdHonk and AHC:
The missing keys for autonomous driving . . . . . . . . . . . . . . . . . . . . 266

: Education

271

28

A disproof of the Theorem of Triviality . . . . . . . . . . . . . . . . . . . . 272

29

Visualizing player engagement with virtual spaces using GIS . . . . . . . . . 273

30

How to git bisect when your test suite is having a bad day . . . . . . . . 282

: Blockchain Gang
31

289

RegisToken . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290

: The SIGBOVIK 2020 Field Guide to Conspiracy Theories

299

32

Faking ancient computer science: A special SIGBOVIK tutorial . . . . . . . 300

33

MORSE OF COURSE: Paper reveals time dimension wasted . . . . . . . . 308

34

The Pacman effect and the Disc-Sphere Duality . . . . . . . . . . . . . . . . 310

: Serious Business

315

35

The SIGBOVIK 2020 field guide to plants . . . . . . . . . . . . . . . . . . . 316

36

Deep industrial espionage . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320

37

What is the best game console?: A free-market–based approach . . . . . . . 327

: Funny Business

337

38

Can a paper be written entirely in the title? 1. Introduction: The title pretty
much says it all. 2. Evaluation: It gets the job done. However, the title is a
little long. 3. Conclusion: Yes. . . . . . . . . . . . . . . . . . . . . . . . . . . 338

39

Erdös-Bacon-Sabbath numbers: Reductio ad absurdum . . . . . . . . . . . . 341

2

Reading Skills
1

Anonymous paper
Anonymous Author
Keywords: anonymous, triple blind, review

2

State-of-the-Art Reviewing: A radical proposal to improve
scientific publication
Samuel Albanie, Jaime Thewmore, Robert McCraith, João F. Henriques
Keywords: peer review, SotA, State-of-the-Art, computer vision,
machine learning

3

Dear Author,
Thank you very much for your submission

1
'Anonymous Paper'

We are sorry to inform you that your paper was rejected. The venue received a great many
submissions of high quality. Each submission was thoroughly and extensively reviewed by our expert
panels. After week-long online and offline discussions, we selected a small subset of papers to be
accepted. Unfortunately, your paper was not one of them.
We have enclosed detailed reviews of your paper below. We hope that these will help you in your
scientific work and look forward to your future submissions!

Reviewer 2
Generally speaking, this is interesting work which may well find some readers in the community.
However, in its present form, the paper suffers from a number of issues, which are outlined below.

Abstract
This paper, titled 'Anonymous Paper,' one more time addresses the problem of Anonymity in
SIGBOVIK. The approach is novel, and is well-described in the paper. Yet, it is only incremental with
respect to the state of the art. This especially holds when compared against the latest works of
Anonymous Author. The approach is shown to work only in theory; but despite its obvious limitations,
it is shown to have potential under specific circumstances.

Area
SIGBOVIK is not new. The past decade has seen a flurry of papers in the area, which would indicate
progress and interest. However, the impact outside of SIGBOVIK has been very limited, as is easily
reflected in the ever-decreasing number of citations. Unfortunately, SIGBOVIK finds itself in a
sandwich position between a more theoretical community and a more practical community, which can
both claim a bigger intellectual challenge and a greater relevance in practical settings. This is
especially true for the field of Anonymity, where the last big progress is now decades ago. As
refreshing it feels to finally again see progress claimed in the field, the present paper must be read
and evaluated in this context.

Originality

4

The central problem of the paper is its novelty with respect to related work. Just last month, this
reviewer attended a talk on Anonymous Paper by Anonymous Author at Anonymous Institution
where a very similar solution to the exact same problem was presented. The paper is actually online
at
http://scigen.csail.mit.edu/cgi-bin/scigen.cgi?author=Anonymous%20Author

Please relate your work carefully against this existing work, precisely highlighting the advances.

Approach
The approach is novel in the sense that the exact same approach has not been examined before.
A central problem with the approach is that the difference with respect to earlier work is simply too
small. An incremental approach such as this one may be appreciated in practice, as it may easily
integrate into existing processes; but for a research paper, the community expects bigger leaps.
The paper suffers from an excess of formalism. Several important definitions are introduced formally
only, without a chance to clearly understand their motivation or usage. The authors should be
encouraged to make more use of informal definitions, building on the well-established formal
definitions in the community.
It is good to see the approach scale to real-life settings. The price of scalability, however, is having to
cope with a multitude of details, which the paper only glosses over, without ever providing a complete
picture. The authors would be better served to provide a tangible abstraction of their approach;
maybe it only works in limited settings, but at least, for these, we can understand how and why.
The authors should be happy to see their approach being used outside of academia. However, this
challenges the novelty of the present submission: Not only is the approach is no longer superior to
the state of the art, it even is not longer superior to the state of the practice - ironically, because it
now defines what the current state of practice is. The submission had better be framed as an
experience report and target non-academic readers.

Evaluation
As nice as it is to see the approach defined and its properties discussed, it has to be evaluated
whether it works on actual subjects. Without a detailed evaluation, we can never know whether the
claimed properties also hold under less abstract and less favorable circumstances.

Limitations
The section on 'threats to validity' pretty much sums everything up: The approach cannot claim
external validity (no surprise, given the evaluation results); on top, the authors themselves list

5

important threats to internal and construct validity.
The fact that the authors themselves apparently cannot list future work fits into this very picture. This
is a clear indication of work being stuck in an impasse; if one ever needed a living proof how
Anonymity has become the laughingstock of the SIGBOVIK community, or how SIGBOVIK is more
and more becoming a dead branch of science, here it is. With so many threats and limitations, it is
unclear whether the paper can be published at all, even in a thoroughly revised form.

Reproducibility
Your code and data are not available as open source. Being unavailable for the general public, there
is no way for readers (or this reviewer) to validate your claims, which is a strict requirement for a
scientific contribution. Please submit it as an electronic appendix with the next revision.

Presentation
The typography of your paper is a disgrace. Respect the style instructions as given by the publisher.
Respect paper lengths. Do not cheat with super-small fonts. Learn what good typography is. Learn
how to use LaTeX, and how to use LaTeX properly. Do not use multi-letter identifiers in LaTeX math
mode. Distinguish - (hyphen) between elements of compound words, -- (en-dash) in ranges, $-$
(minus) for math, --- (em-dash) for digressions in a sentence. use the correct quotes (`` and ''). In
BibTeX, capitalize titles properly. Use \dots rather than ... . This reviewer stops here.
The paper has numerous presentation issues. The paper contains numerous typos - Page 2, for
instance, has a period '.' instead of a comma ','. This is a sign of careless proofreading, and ultimately
disrespect - if the authors do not care about their paper, why should the reader care? At least try to
produce a polished version for submission.
Keep in mind that a high number of presentation issues eventually will hinder the readers and
reviewers to understand what your work is about. Should you find misunderstandings in the above
review, ask yourself what you could have done to avoid these.

Summary
Points in favor: (+) An interesting approach to Anonymity in SIGBOVIK (+) Paper does a good
attempt at describing the approach
Points against: (-) Far away from scientific mainstream (-) Incremental (-) Insufficient evaluation (-)
Substantial presentation issues

Recommendation
Reject.

6

Footnote
Generated by autoreject.org

7

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 1: Anonymous paper
Reviewer: Definitely not the SIGBOVIK webmaster
Rating: 3rd grade reading comprehension
Confidence: Confident that some cool people said to email reviews to
the sigbovik-reviews mailing list and not to easychair

Figure 1: The easychair paper submission site

Figure 2: The SIGBOVIK 2020 website instructions for submitting reviews

8

2

S TATE - OF - THE -A RT R EVIEWING :

A RADICAL
PROPOSAL TO IMPROVE SCIENTIFIC PUBLICATION
Samuel Albanie, Jaime Thewmore, Robert McCraith, Joao F. Henriques
SOAR Laboratory,
Shelfanger, UK

A BSTRACT
Peer review forms the backbone of modern scientific manuscript evaluation. But
after two hundred and eighty-nine years of egalitarian service to the scientific
community, does this protocol remain fit for purpose in 2020? In this work, we
answer this question in the negative (strong reject, high confidence) and propose
instead State-Of-the-Art Review (SOAR), a novel reviewing pipeline that serves
as a “plug-and-play” replacement for peer review. At the heart of our approach is
an interpretation of the review process as a multi-objective, massively distributed
and extremely-high-latency optimisation, which we scalarise and solve efficiently
for PAC and CMT-optimal solutions.
We make the following contributions: (1) We propose a highly scalable, fully
automatic methodology for review, drawing inspiration from best-practices from
premier computer vision and machine learning conferences; (2) We explore several instantiations of our approach and demonstrate that SOAR can be used to both
review prints and pre-review pre-prints; (3) We wander listlessly in vain search of
catharsis from our latest rounds of savage CVPR rejections1 .
If a decision tree in a forest makes marginal
improvements, and no one is around to
publish it, is it really “state-of-the-art”?
George Berkeley,
A Treatise Concerning the Principles of
Human Knowledge (1710)

1

I NTRODUCTION

The process of peer review—in which a scientific work is subjected to the scrutiny of experts in the
relevant field—has long been lauded an effective mechanism for quality control. Surgically inserted
into the medical field by the cutting-edge work of (Ali al Rohawi, CE 854-–931), it ensured that
treatment plans prescribed by a physician were open to criticism by their peers. Upon discovery of a
lengthy medical bill and a dawning realization that theriac was not the “wonder drug” they had been
promised, unhappy patients could use these “peer reviews” as evidence in the ensuing friendly legal
proceedings.
Despite this auspicious start, it took many years for the peer review protocol to achieve the popular form that would be recognised by the layperson on the Cowley Road omnibus today. Credit
for this transformation may be at least partially attributed to the Royal Society of Edinburgh who
were among the first to realise the benefits of out-sourcing complex quality assessments to unpaid
experts (Spier, 2002).
Effacing the names of these heroic contributors, in a process euphemistically called anonymous
review, was a natural progression. Attempts to go further and have the reviewers retroactively pay
1
W.A/W.A/B → Reject. A single heavily caffeinated tear, glistening in the flickering light of a faulty office
desk lamp, rolls down a weary cheek and falls onto the page. The footnote is smudged. The author soldiers on.

9

for the privilege of reading a now-copyrighted manuscript (at the discounted price of £50) somehow
did not catch on, despite the publishers’ best intentions. Peer review (not to be confused with
the French tradition of Pierre review, or indeed the spectacle of a pier revue) has since gone from
strength-to-strength, and is now the primary quality filtration system for works of merit in both the
scientific and TikTok communities.
Still, something is rotten in the state of reviewing. To determine what exactly is causing the smell,
our starting point in this work is a critical review of peer review. We begin by highlighting three key
shortcomings of the existing system.
Ability to Scale. As anyone who has prepared for a tech internship interview knows, scale is important. And so are corner cases. And so is good communication. But the greatest of these is scale.
To avoid carelessly ruling out future careers at Google, we therefore demonstrate an appreciation of
the critical importance of this phenomenon. Indeed, it is here that we must mount our first attack
on peer review: the algorithm is provably O(p), where p is the number of peers. To concretise the
implications of this runtime complexity, consider the nation of the United Kingdom which occupies
a small number of green and pleasant islands ’twixt the United States and Europe. There are, at
the time of writing, 814 hereditary peers in the UK who can be called upon as professional peers.
Of these, 31 are dukes (7 of which are royal dukes), 34 are marquesses, 193 are earls, 112 are viscounts, and 444 are barons. Many of these, however, do not sit in the House of Lords (an Airbnb
property in which peers can be recruited to review documents), and so cannot be relied upon here.
Fortunately, the vast majority of the 789 members of the House are instead peerages “#4lyf”—these
are ephemeral honours which are somewhat easier to create because they do not require building
new humans from scratch from a limited set of eligible bloodlines. Nevertheless, short of a fairly
sizeable second “Loans for Lordships” political scandal, it is hard to foresee the kind rapid growth in
the peerage ranks that is needed to meet reviewing demand. We also note here a second concern: for
various technical reasons2 , only one hereditary position of the house is held by a woman (Margaret,
31st Countess of Mar), which raises questions about not only the scale, but also the diversity we can
expect among the potential reviewing pool.
Speed. The mean age of the House of Lords was 70 in 2017. With a lack of young whippersnappers
amidst their ranks, how can we expect these venerable statesmen and stateswomen to do the allnighters required to review ten conference papers when they are only reminded of the deadline
with two days notice because of a bug in their self-implemented calendar app? One solution is to
ensure that they take care when sanitising date/time inputs across time-zones. But still, the question
remains: how long does peer review really take? Since public data on this question is scarce, we turn
to anecdotal evidence from our latest round of reviewing. The results were striking. We found that
any conference review paper batch is likely to contain at least one paper that takes at least ten hours
to review. The blame for these “time bombs” lies with both authors and reviewers, since they arise
from the combination of: (1) a review bidding process that allows the reviewer access to only the
paper title and abstract; (2) authors who write paper titles and abstracts that bear little resemblance
to their work. As a consequence of this mismatch, the unsuspecting reviewer may, on occasion,
volunteer for a 47 page appendix of freshly minted mathematical notation, ruining their weekend
and forcing them to miss a movie they really wanted to see. Of course, the fact that they actively
bid on the paper and were therefore responsible for its assignment ensures that they feel too guilty
to abandon the review. The proof of why this is problematic is left as an exercise for the reader.
Consistency. The grand 2014 NeurIPS review experiment (Lawrence & Cortes, 2015) provides
some insight into the consistency of the peer review process. When a paper was assigned to two
independent review committees, about 57% of the papers accepted by the first committee were rejected by the second one and vice versa (Price, 2014). While these numbers provide a great deal
of hope for anyone submitting rushed work to future editions of the conference, it is perhaps nevertheless worth observing that it brings some downsides. For one thing, it places great emphasis on
the role of registering at the right time to get a lucky paper ID. This, in turn, leads to a great deal
of effort on the part of the researcher, who must then determine whether a given ID (for example
57383 ) is indeed, a lucky number, or whether they are best served by re-registering. A similar phe2

See Sec. A.2 in the appendix for historical conditions under which a peerage would pass to a female heir.
Thankfully, numerology is on hand to supply an answer. “5738: You are a step away from the brink that
separates big money from lawlessness. Take care, because by taking this step, you will forever cut off your
ways to retreat. Unless it is too late.” (numeroscop.net, 2020)
3

10

Figure 1: (Left) The state-of-the-art according to Cattelan et al. (2020), (Right) Some marginal improvements by various authors, with questionable added artistic and nutritional value (as measured
in calories and milligrams of potassium).4
nomenon is observed in large-scale deep learning experiments, which generally consist of evaluating
several random initialisations, a job that is made harder by confounders such as hyper-parameters or
architectural choices.
By examining the points above, we derive the key following principle for review process design. Human involvement—particularly that of elderly hereditary peers—should be minimised in the modern
scientific review process. In this work, we focus on a particular instantiation of this principle, StateOf-the-Art Reviewing (SOAR), and its mechanisms for addressing these weaknesses.
The remainder of the work is structured as follows. In Sec. 2, we review related work; in Sec. 3,
we describe SOAR, our bullet-proof idea for automatic reviewing; in Sec. 4 we develop a practical
implementation of the SOAR framework, suitable for popular consumption. Finally, in Sec. 5, we
conclude with our findings and justification for why we anticipate swift community adoption.

2
2.1

R ELATED W ORK
I NTEREST IN THE STATE - OF - THE - ART

Since the discovery of art (Blombos Cave Engravings, ca. 70000 BC) there has been a rising interest
in this form of expression, and consequently, the state thereof. From the Medici family of Florence to
theatre buff King James I, much effort has been dedicated to patronage of the arts, and much prestige
associated with acquiring the latest advances. Pope Julius II was keen to raise the papal state of the
art to new heights, namely the ceiling, enlisting the help of renaissance main man Michelangelo.
The score of Sistine remains competitive in chapel-based benchmarks, and Michelangelo became
a testudine martial artist (with the help of his three equally-talented brothers) (Eastman & Laird,
1984).
From early on, the importance of adding depth was appreciated (Studies on perspective,
Brunelleschi, 1415), which continues to this day (He et al., 2016). Recently, the critically acclaimed
work of Crowley & Zisserman (2014) illustrated how the state-of-the-art can be used to assess the
state of art, calling into question the relevance of both hyphens and definite articles in modern computer vision research. Of least relevance to our work, Fig. 1 depicts state-of-the-art developments in
the art world.
4
Photo credits: (left): NYT-Photography (2019) (top-centre): Noennig (2019), (top-right): Durian (2019),
(bottom-center): Tampa-Police-Department (2019), (bottom-right): Popeyes (2019)

11

Figure 2: (Left) The number of PhDs granted annually exhibits exponential growth (figure reproduced from Gastfriend (2015)), (Right) Google retrieved ngram counts of “State of the Art” over
the past 200 years of literature. Note that even when the axes are rotated slightly, it remains difficult
to preserve an upwards trend. This evidence suggests that either PhDs are becoming exponentially
less productive than their predecessors or that the existing reviewing system does not provide sufficient incentivise to use the term “state-of-the-art” in modern manuscripts. Our proposal directly
addresses the latter.
2.2

L ITERATURE R EVIEW

The Grapes of Wrath. In this classic portrayal of the American Dust Bowl, Steinbeck captures
the extremes of human despair and oppression against a backdrop of rural American life in all its
grittiness. A masterpiece. HHHHH
Flyer for (redacted) startup, left on a table at NeurIPS 2019 next to a bowl of tortillas. Hastily
put together in PowerPoint and printed in draft-mode amid the death throes of an ageing HP printer,
this call for “dedicated hackers with an appetite for Moonshots, ramen noodles and the promise of
stock options” comes across slightly desperate. HH

3

M ETHOD
Science is often distinguished from other
domains of human culture by its
progressive nature: in contrast to art,
religion, philosophy, morality, and politics,
there exist clear standards or normative
criteria for identifying improvements and
advances in science.
Stanford Encyclopedia of Philosophy

In Sec. 1, we identified three key weaknesses in the peer review process: (1) inability to scale; (2)
slow runtime and (3) inconsistent results. In the following, we describe the SOAR review scheme
which seeks to resolve each of these shortcomings, and does so at minimal cost to the taxpayer or
ad-funded research lab, enabling the purchase of more GPUs, nap-pods and airpods.
3.1

S TATE - OF - THE - ART R EVIEWING (SOAR)

It is well known is that the quality of a scientific work can be judged along three axes: efficacy,
significance and novelty. Our key insight is that each of these factors can be measured automatically.
Assessing efficacy. Efficacy is best assessed by determining if the proposed method achieves a new
SotA (State-of-the-Art). Thankfully, from an implementation perspective, the authors can be relied
upon to state this repeatedly in the text. Thus, rather than parsing results table formats (an errorprone process involving bold fonts and asterisks), we simply word count the occurrences of “stateof-the-art” (case insensitive) in the text. It stands to reason that a higher SotA count is preferable.

12

Moreover, such an approach avoids the embarrassment of realising that one cannot remember what
kind of statistical significance test should be applied since all SotA is significant.
Assessing significance. Significance is measured by efficacy. Thus, the efficacy term is weighted
twice in the formula.
Assessing novelty. The assessment of novelty requires close familiarity with prior art and an appreciation for the relative significance of ideas. We make the key observation that the individuals
best placed to make this judgement are the author themselves since they have likely read at least one
of the works cited in the bibliography. We further assume that they will convey this judgement by
using the word “novel” throughout the document in direct proportion to the perceived novelty of the
work.
With the strategies defined above, we are now in a position to define the SOAR score as follows.
SOAR Score ,

p
3

SSotA · SSotA · Snovelty /10.

(1)

Here SSotA and Snovelty represent the total occurrences in the manuscript of the terms “state-of-theart” and “novel”, respectively. In both cases, we exclude the related work section (it is important to
avoid assigning SotA/novelty credit to the paper under review simply because they cite SotA/novel
work). A geometric mean is used to trade-off each factor, but note that a paper must be both SotA
and novel to achieve a positive SOAR score. Lastly, we attach a suffix string “/10” to every SOAR
score. This plays no role in the computation of the score.
Note that several factors are not assessed: vague concepts like “mathematical proofs” and “insights”
should be used sparingly in the manuscript and are assigned no weight in the review process. If the
proof or insight was useful, the authors should use it to improve their numbers. SotA or it didn’t
happen.
A key advantage of the SOAR formula is that it renders explicit the relationship between the key
scientific objective (namely, more State-of-the-Art results) and the score. This lies in stark contrast
to peer review, which leaves the author unsure what to optimise. Consider the findings of Fig. 2: we
observe that although the number of PhDs granted worldwide continues to grow steadily, usage of
the term “State-of-the-Art” peaked in the mid 1980’s. Thus, under peer review, many PhD research
hours are invested every year performing work that is simply not on the cutting edge of science. This
issue is directly addressed by measuring the worthiness of papers by their state-of-the-artness rather
than the prettiness of figures, affiliation of authors or explanation of methods.
With an appropriately increased focus on SotA we can also apply a filter to conference submissions
to immediately reduce the number of papers to be accepted. With top conferences taking tens of
thousands of submissions each typically requiring three or more reviewers to dedicate considerable
time to perform each review, the time savings over an academic career could be readily combined
to a long sabbatical, a holiday to sunny Crete, or an extra paper submission every couple of weeks.

4

I MPLEMENTATION

In this section, we outline several implementations of SOAR and showcase a use case.
4.1

S OFTWARE I MPLEMENTATION AND C OMPLEXITY A NALYSIS

We implement the SOAR algorithm by breaking the submission into word tokens and passing them
through a Python 3.7.2 collections.Counter object. We then need a handful of floating-point
operations to produce the scalar component of Eqn. 1, together with a string formatting call and a
concatenation with the “/10”. The complexity of the overall algorithm is judged reasonable.

13

In defense of revisiting Adapting Adaptations:
Are convolutions convolutional enough?
Novel Neville

SOAR Score:
7/10
Recommendation: You
should probably read this

In recent years, the humble convolution has drawn praise from friends and foes alike for its enviable
equivariance, parameter sharing and strong theoretical connection to Joseph Fourier. But is the convolution
"convolutional" enough? This question forms the basis of the current work, in which we highlight scenarios in
which one does not simply "convolve" a standard convolutional opeartor, willy-nilly, with all desired inputs.

Figure 3: Proposed arXiv-integration: The arXiv server is an invaluable resource that has played
a critical role in the dissemination of scientific knowledge. Nevertheless, a key shortcoming of the
current implementation is that it is unopinionated, and offers little guidance in whether to invest time
in reading each article. The SOAR plugin takes a different approach: summarising the scientific
value of the work as an easily digestible score (out of ten) and offering a direct read/don’t read
recommendation, saving the reader valuable time. Future iterations will focus on removing the next
bottleneck, the time-consuming “reading” stage.
4.2

W ETWARE I MPLEMENTATION AND C OMPLEXITY A NALYSIS

In the absence of available silicon, SOAR scoring can also be performed by hand by an attentive
graduate student (GS) with a pencil and a strong tolerance to boredom. Much of the complexity
here lies in convincing the GS that it’s a good use of time. Initial trials have not proved promising.
4.3

AR X IV INTEGRATION

We apply the SOAR scoring software implementation to the content of arXiv papers as a convenient
Opera browser plugin. The effect of the plugin can be seen in Fig. 3: it provides a high-quality
review of the work in question. Beyond the benefits of scalability, speed and consistency, this tool
offers a direct “read/don’t read” recommendation, thereby saving the reader valuable time which can
otherwise be re-invested into rejecting reviewer invitations emails to compound its savings effect.
We hope that this pre-review for pre-prints model will be of great utility to the research community.

5

C ONCLUSION

In this work, we have introduced SOAR, a plug-and-play replacement for peer review. By striking an
appropriate balance between pragmatism and our lofty goals, we anticipate near-instantaneous community adoption. In future work, we intend to further optimise our implementation of SOAR (from
2 LoC to potentially 1 or 0 LoC, in a ludic exercise of code golf). Other avenues of future research
include peer-to-peer ego-limiting protocols and Tourette-optimal author feedback mechanisms.

R EFERENCES
Ishāq bin Ali al Rohawi. Adab al–tabib (practical ethics of the physician). CE 854-–931.
Maurizio Cattelan, a tar-covered seagull, and a very strange trip to the local 7-Eleven. Comedian. Art
Basel Miami Beach, 2020. (presumably also visible while not under the influence of psychotropic
substances).
Elliot J Crowley and Andrew Zisserman. The state of the art: Object retrieval in paintings using
discriminative regions. 2014.
99 Old Trees Durian. Durian tape to white wall, 2019. URL https://www.facebook.com/
99oldtrees/posts/2575690792538528:0. [Online; accessed 27-March-2020].
Kevin Eastman and Peter Laird. Teenage Mutant Ninja Turtles. Mirage Studios, 1984.

14

Eric Gastfriend.
90% of all the scientists that ever lived are alive today,
2015.
URL
https://futureoflife.org/2015/11/05/
90-of-all-the-scientists-that-ever-lived-are-alive-today/
?cn-reloaded=1. [Online; accessed 27-March-2020].
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.
Neil Lawrence and Corinna Cortes.
Examining the repeatability of peer review, 2015.
URL
http://inverseprobability.com/talks/slides/nips_radiant15.
slides.html#/. [Online; accessed 27-March-2020].
Jordyn Noennig.
Banana duct-taped to the wall is art. but how
about sausage taped to the wall because,
wisconsin,
2019.
URL
https://eu.jsonline.com/story/entertainment/2019/12/10/
banana-duct-taped-wall-sparks-vanguard-milwaukees-hot-dog-wall/
4384303002/. [Online; accessed 27-March-2020].
numeroscop.net.
On 5738, 2020.
URL https://numeroscop.net/numerology_
number_meanings/four_digit_numbers/number_5738.html. [Online; accessed
27-March-2020].
NYT-Photography. The 120, 000bananawinsartbasel, 2019. U RL. [Online; accessed 27-March2020].
Popeyes. Chicken taped to wall, 2019. URL https://twitter.com/popeyeschicken/
status/1203140095005605888. [Online; accessed 27-March-2020].
Eric Price. The nips experiment, 2014. URL http://blog.mrtz.org/2014/12/15/
the-nips-experiment.html. [Online; accessed 27-March-2020].
Ray Spier. The history of the peer-review process. TRENDS in Biotechnology, 20(8):357–358,
2002.
Tampa-Police-Department.
Sgt. donut, 2019.
URL https://www.facebook.com/
TampaPD/posts/3297203563685153. [Online; accessed 27-March-2020].
Wikipedia contributors. Hereditary peer — Wikipedia, the free encyclopedia, 2020. URL
https://en.wikipedia.org/w/index.php?title=Hereditary_peer&oldid=
946076588. [Online; accessed 27-March-2020].

A

A PPENDIX

In the sections that follow, we provide additional details that were carefully omitted from the main
paper.
A.1

T ITLE P RONUNCIATION

In common with prior works, we hope that the arguments put forward in this paper will spark useful
discussion amongst the community. Where appropriate, we encourage the reader to use the official
title pronunciation guide in Fig. 4.
A.2

I NHERITANCE OF PEERAGES

One historical challenge with the expansion of the UK peerage system has been something of a
pre-occupation with preventing the passage of peerages to women. We note that this has grave
implications for the ability to scale peer review. Consider the progressive rules for inheritance under
Henry IV of England (Wikipedia contributors, 2020), which were as follows:

15

Figure 4: Official Title Pronunciation Guide (gustoso).
“If a man held a peerage, his son would succeed to it; if he had no children, his
brother would succeed. If he had a single daughter, his son-in-law would inherit
the family lands, and usually the same peerage; more complex cases were decided
depending on circumstances. Customs changed with time; earldoms were the first
to be hereditary, and three different rules can be traced for the case of an Earl who
left no sons and several married daughters. In the 13th century, the husband of
the eldest daughter inherited the earldom automatically; in the 15th century, the
earldom reverted to the Crown, who might re-grant it (often to the eldest son-inlaw); in the 17th century, it would not be inherited by anybody unless all but one of
the daughters died and left no descendants, in which case the remaining daughter
(or her heir) would inherit.”
Note that by avoiding the necessity of a direct bloodline between peers, SOAR neatly sidesteps
this scalability concern, further underlining its viability as a practical alternative to traditional peer
review.
A.3

N EW I NSIGHTS : A MEMORYLESS MODEL FOR SCIENTIFIC PROGRESS

Figure 5: By introducing a State-of-the-Art state transition diagram, we show how the progression
of research can be modelled as a memoryless automaton.
Beyond time savings for reviewers, we note here that the SOAR score further provides insights into
the scientific method itself, yielding time savings for authors too. To illustrate this, we provide a
state transition diagram in Fig. 5 which models the evolution of research progress. Importantly,
this model guarantees a Markov-optimal approach to research: a researcher must only ever read the
paper which represents the current State-of-the-Art to make further progress.

16

SIGBOVIK
3

A thorough investigation of the degree to which the COVID-19
pandemic has enabled subpar-quality papers to make it into the
proceedings of SIGBOVIK, by reducing the supply of authors
willing to invest the necessary effort to produce high-quality papers
Shalin Shah
Keywords: COVID-19, SIGBOVIK, lazy, quality, sadness, big Oof

4

Is this the longest Chess game?
Dr. Tom Murphy VII Ph.D.
Keywords: chess, chesses, chessing, G. K. Chesterton

5

Optimizing the SIGBOVIK 2018 speedrun
leo60228
Keywords: speedrun, SIGBOVIK, pathfinding, DFS, Rust

6

Retraction of a boring follow-up paper to “Which ITG stepcharts
are turniest?” titled, “Which ITG stepcharts are crossoveriest
and/or footswitchiest?”
Ben Blum
Keywords: groove, in, retraction, the

17

A Thorough Investigation of the Degree to which the
3
COVID-19
Pandemic has Enabled Subpar-Quality Papers to
Make it into the Proceedings of SIGBOVIK, by Reducing the
Supply of Authors Willing to Invest the Necessary Effort to
Produce High-Quality Papers
Shalin Shah
Carnegie Mellon University
April 1, 2020
Abstract:
Based on the inclusion of this paper in the proceedings of SIGBOVIK 2020, we find that the COVID-19 pandemic has in
fact enabled subpar-quality papers to make their way into the proceedings of SIGBOVIK, through a drastic reduction in
the supply of authors willing to invest the necessary effort to produce high-quality papers.
Introduction:
Y’all know what COVID-19 is.
Methods and Materials:
You’re looking at the materials. Note that, in order to emphasize the subpar quality of this paper, we have opted to use
extremely lazy Microsoft-Word default formatting, rather than LaTeX. Also, we have restricted the contents of this paper
to a single page, to highlight its lack of substance. Meanwhile, our method was to simply submit this paper to SIGBOVIK
2020 and see what happened.
Results:
As evidenced by the fact that you’re currently reading this in the SIGBOVIK 2020 proceedings, this paper successfully
made it into the SIGBOVIK 2020 proceedings.
Discussion:
The results indicate that SIGBOVIK 2020’s standards of quality have indeed fallen significantly relative to 2019,
presumably due to the COVID-19 pandemic decreasing the supply of authors willing to invest the necessary effort to
produce high-quality paper submissions.
Conclusions:
In conclusion, COVID-19 sucks.
References:
n/a
1

18

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 36: A thorough investigation of the degree to which the COVID-19 pandemic has
enabled subpar-quality papers to make it into
the proceedings of SIGBOVIK, by reducing the
supply of authors willing to invest the necessary effort to produce high-quality papers
Reviewer: Exponentiator
Rating: Better and better every day, by leaps and bounds and
leaping leaps and bounding bounds
Confidence: Those guys have faith, why shouldn’t I?

19

Is this the longest Chess game?
4

Dr. Tom Murphy VII Ph.D.*
1 April 2020

1

Introduction

In my experience, most chess games end in a few moves. If you
want to play a lot of chess moves, you just play a lot of chess
games. Still, there are games that seem to go on foreverrrrrrrrrr.
Perhaps the players are trying to lull each other into a false sense
of security while waiting for the moment to strike, or perhaps they
are stalling in a game of Chess to the Death.
Although many people “know how to play” chess, almost nobody fully understands the rules of chess, most authoritatively
given by FIDE [1]. (See for example Figure 1 for a minor chess
scandal that erupted in 2019 over an obscure corner case in the
rules.) Several of these “deep cuts” have to do with game-ending
conditions that were introduced to avoid interminable games.
Many chess moves are reversible (e.g. moving the knight forward and back to its starting position [4]), so informal games of
chess could last forever with the players repeating a short cycle. In AD 1561, Ruy López added the “fifty-move rule” to prevent infinite games.1 This rule (detailed below) ensures that irreversible moves are regularly played, and so the game always
makes progress towards an end state. Another rule, “threefold
repetition” also guarantees termination as a sort of backup plan
(either of these rules would suffice on its own).
So, chess is formally a finite game. This is good for computer
scientists, since it means that chess has a trivial O(1) optimal
solution. This allows us to move onto other important questions,
like: What is the longest chess game? In this paper I show how
to compute such a game, and then gratuitously present all of its
17,697 moves. Even if you are a chess expert (“chexpert”), I bet
you will be surprised at some of the corner cases in the rules that
are involved.
Speaking of rules, let’s first detail the three main rules that
limit the length of the game. These rules cause the game to end
in a draw (tie) when certain conditions are met.

9.3. The game is drawn, upon a correct claim by the
player having the move, if:
a. he writes his move on his scoresheet and declares to
the arbiter his intention to make this move, which
shall result in the last 50 moves having been made
by each player without the movement of any pawn
and without any capture, or
b. the last 50 consecutive moves have been made by
each player without the movement of any pawn and
without any capture.
Note that what is provided here is the option for a player to
claim a draw (and the two provisions essentially allow either player
to claim the draw at the moment of the 50th move). If neither
player is interested in a draw, either because they think their
position is still winning, or are just trying to create the longest
ever chess game, the game legally continues. That’s why what is
actually relevant for this paper is provision 9.6, which defines a
draw:

0Zns0s0j
obZnZplp
6
0o0ZpZpZ
5
ZNZpM0Z0
4
0Z0Z0O0Z
3
ZPZ0Z0O0
2
PZ0OPL0O
1
Z0S0J0SB
8
7

a

1.1

The seventy-five move rule

b

c

d

e

f

g

h

Figure 1: (Nepomniachtchi – So, 2019.) White to move during
a speed Chess960 (aka “Fischer Random”) tournament. In this
variant, the pieces start in different positions, but castling rules
are such that the king and rook end up on the same squares that
they would in normal chess. As a result, it is possible for the king
or rook to not move during castling, or for the destination square
for the king to already be occupied by the rook. Attempting to
castle in the position depicted, grandmaster Ian Nepomniachtchi
* Copyright © 2020 the Regents of the Wikiplia Foundation. Appears in first touched the rook to move it out of the way. However, pieceSIGBOVIK 2020 with the ShortMoveString of the Association for Computa- touching rules require that when castling, the player first moves
tional Heresy; IEEEEEE! press, Verlag-Verlag volume no. 0x40-2A. ¤17,697
the king (and “Each move must be played with one hand only”);
1 This rule only applied to games started after its introduction, so it is
but how? The rook is occupying g1! One commenter suggested
possible that some pre-1561 games are still in progress and may never end.
2 Other irreversible moves include: Castling, moving a piece so as to lose
tossing the king into the air, then sliding the rook to f1 while
castling rights, and declining to capture en passant (such capture must be the king is airborne, and then watching the king land dead center
made immediately, so if the option is not taken, it cannot be regained). The
on its target. The arbiter required Nepo to make a rook move
fifty-move rule could be soundly expanded to include these, but, that’s just
instead, but this was later appealed, and the game replayed.
like, not the rule, man.
The “fifty-move rule,” [8] as it is usually known, requires that
an irreversible move is played at a minimum pace. For the sake
of this rule, irreversible moves are considered captures and pawn
moves,2 including promotion and en passant (which is also a capturing move) but not the movement of a previously promoted
piece. Specifically [1]:

20

9.6. If one or both of the following occur(s) then the
game is drawn:

0Z0Z0Z0Z
s0Z0Z0Zn
6
0Z0Z0Z0Z
5
Z0Z0Z0Z0
4
0Z0Z0j0Z
3
Z0ZKZ0Z0
2
0Z0Z0Z0M
1
Z0ZbZ0ZQ
8

. . . 9.6.2. any series of at least 75 moves have been made
by each player without the movement of any pawn and
without any capture. If the last move resulted in checkmate, that shall take precedence.

7

Draws after 75 moves (per player, so really 150 moves) are compulsory.
Interestingly (at least as interesting as anything in this dubious
affair) it is known that some otherwise winning endgame positions
require more than 50 moves to execute (Figure 2). The rules of
chess have at various times allowed for longer timers in such known
situations, but were later simplified to the fixed 50 (and 75) move
limit.

1.2

a

b

c

d

e

f

g

h

Fivefold repetition

Figure 2: Black to move and mate in 545 moves (!). The position
The 75-move rule is rarely applied in practice, but its counterpart, was found (by Zakharov and Makhnichev [10]) while building an
“threefold repetition” [9] is often the cause of draws in modern endgame tablebase of all possible 7-piece positions. Of course, the
chess. This rule states that if the same position appears three game ends prematurely in a draw because of the 75-move rule.
times, the players can claim a draw:
9.2.2. Positions are considered the same if and only if the
same player has the move, pieces of the same kind
and colour occupy the same squares and the possible moves of all the pieces of both players are the
same. [...]

1.3

Dead position

The informal version of this rule (“insufficient material”) states
that if neither side has enough pieces to mate the opponent (for
example, a king and bishop can never mate a bare king) then the
game is drawn. Again, the formal rule is more subtle:

Like the 75-move rule, this rule has an optional version (upon
three repetitions) and a mandatory one in 9.6:

5.2.2. The game is drawn when a position has arisen in
which neither player can checkmate the opponent’s king
with any series of legal moves. The game is said to end
in a ‘dead position’. This immediately ends the game. . .

[The game is a draw if . . . ]
9.6.1. the same position has appeared, as in 9.2.2, at
least five times.

This clearly includes the well-known material-based cases like
king and knight vs. king, but it also surprisingly includes many
Fascinatingly (at least as fascinating as anything in this ques- other specific positions, especially those with forced captures (Figtionable undertaking), this rule used to require consecutive repe- ure 3).
tition of moves. However, there exist infinite sequences of moves
with no consecutive n-fold repetition. For example, in the start8
ing position, white and black can move either of their knights
out and back. Let 0 be Nc3 nc6 Nb1 nb8 (queenside knights
7
move, returning to the starting position) and 1 be Nf3 nf6
6
Ng1 ng8 (kingside). Now the Prouhet–Thue–Morse sequence [7]
3
0110100110010110. . . can be executed. This infinite sequence is
5
cube-free (does not contain SSS for any non-empty string S) [6],
and therefore never violates the consecutive threefold repetition
4
rule [2].
3
Many implementations of chess ignore these rules or treat them
2
incorrectly. Implementation of the seventy-five move rule simply
requires a count of how many moves have transpired since a pawn
1
move or capture, but programs typically do not force a draw after
a
b
c
d
e
f
g h
75 moves. Repetition requires more work, since the program must
keep track of each of the states reached since the last irreversible
move. There are also some corner cases, such as ambiguity as Figure 3: Black to move and draw in 0 (!). Most players and even
to whether the starting position has “appeared” before the first chexperts believe that the only legal move is Kxa2, and that the
move [4]. The ubiquitous FEN notation for describing chess po- game then ends in a draw with “insufficient material.” However,
sitions does not even include any information about states previ- this game is already over. Since neither black nor white can win
via any series of legal moves, by rule 5.2.2 the game immediately
ously reached.
ends in a ‘dead position’. (Although see Section 1.3.1 for possible
3 Let s be the string "0", then s is s
ambiguity in this rule.)
0
i
i−1 si−1 where 0 = 1 and 1 = 0.

0Z0Z0Z0Z
Z0Z0Z0Z0
0Z0Z0J0Z
Z0Z0Z0Z0
0Z0Z0Z0Z
Z0Z0Z0Z0
QZ0Z0Z0Z
j0Z0Z0Z0

21

The insufficient material rule is curious in that it requires nontrivial computation to implement. In order to know whether the
position is a draw, an implementation needs to be able to decide
whether or not a series of moves that results in checkmate exists.
Note that this is not nearly as bad as normal game tree search
because the two sides can collaborate to produce the mate (it is
not a ∃∀∃∀ . . . but rather ∃∃∃∃ . . .). Still, such “helpmates” can
still be quite deep (dozens of moves) and are interesting enough to
be a common source of chess puzzles. Proving the non-existence
of a helpmate can be very difficult indeed (Figure 4).4

kZbA0Z0Z
7
Z0Z0Z0Z0
6
0ZpZpZpZ
5
ZpOpOpO0
4
0O0O0O0Z
3
Z0Z0Z0Z0
2
0Z0Z0Z0Z
1
J0AbZ0Z0
8

a

b

c

d

e

f

g

BAKARSnj
7
OPOPOPmn
6
0Z0O0ARS
5
Z0ZPAPOR
4
0Z0O0O0O
3
Z0Z0Z0Z0
2
0Z0Z0Z0Z
1
Z0Z0Z0Z0
8

a

b

c

d

e

(a)

f

g

h

BAKARS0A
OPOPmPZn
6
0Z0O0ZRS
5
Z0ZPAPOR
4
0Z0O0O0O
3
Z0Z0Z0Z0
2
0Z0Z0Z0Z
1
Z0Z0Z0Z0
8
7

a

b

c

d

e

f

g

h

(b)

Figure 5: In this contrived and impossible position (a), white
has many easy paths to mate. All of black’s pieces are pinned.
There is also a “series of legal moves” where black mates white:
Bxg7++ Bxh8?? nxe7++. Capturing the king (by moving twice
in a row) is a “legal move” (despite not being allowed by other
rules), and doing so unpins black’s knight to deliver a smothered
mate (b). Of course, abuse of this dubious technical possibility
doesn’t change the status of the position, since we already have a
mating sequence by white.

h

be considered dead if there is a checkmating sequence, but it requires entering a fivefold repetition or exhaustion of the 75-move
rule? If so, this would end the game prematurely, and so it has
implications for the longest possible game (Section 2.1). Even
more esoterically, this interpretation causes the rule to be selfreferential: A sequence must also be allowed by the rule being defined. A normal person would take the “least fixed point” (in the
Kripke sense [3]) of this self-referential definition (fewest positions
1.3.1 Ambiguity
are drawn). But it is also consistent to interpret it maximally—in
Moreover, this rule contains some ambiguity. The phrase “any which case the longest chess game is zero moves!
series of legal moves” is usually taken to mean something like,
For completeness, note that there are other routes to a draw
“the players alternate legal moves and follow most of the normal
(stalemate,
draw offers) which we can ignore; it is easy to avoid
rules of chess.” In my opinion it is hard to justify an interpretation
these
situations
when generating the longest game.
like this.
First, the rules specifically define “legal move”, with 3.10.1 saying “A move is legal when all the relevant requirements of ArtiGenerating the longest game
cles 3.1 – 3.9 have been fulfilled.” These requirements describe 2
the movement of each piece as you are familiar (e.g. 3.3 “The
rook may move to any square along the file or the rank on which It is generally not hard to avoid repeating positions, so the main
it stands.”). They also disallow capturing one’s own pieces, or obstacle we’ll face is the 75-move rule. Let’s call an irreversible
moving when in check. However, they allow as legal some moves move that resets the 75-move counter a critical move; this is a
that would otherwise be prohibited, like capturing the opponent’s pawn move or capturing move (or both). The structure of the
king (this is excluded by 1.4.1, outside the definition of “legal”). game will be a series of critical moves (I will call these “critical”
Capturing the opponent’s king is generally not useful for demon- moves) with a maximal sequence of pointless reversible moves in
strating that checkmate is possible (Figure 5), so this is mostly a between them. If we execute the maximum number of critical
moves and make 149 moves (just shy of triggering the compulsory
curiosity.
Second, what is a “series” of legal moves? It seems completely version of the 75-move rule) between them, then this will be fairly
consistent to allow the white player to make several legal moves easily seen as a maximal game.
In fact, for most positions, it is easy to waste 149 moves and
in a row, for example. The rules about alternating moves are
again outside the definition of “legal move” and “series” is never return to the exact same position. So, the strategy for generating
the longest game can mostly be broken into two tasks: Make
defined.
We could instead interpret “any series of legal moves” as “taking a game with a maximum number of critical moves (it can also
the entirety of the rules of chess, any continuation of the game that contain other moves) and then pad that game out to maximumends in checkmate for either player.” I like this better, although length excursions in between its critical moves.5
it creates its own subtle issues. For example, should the position
Figure 4: Thinking of implementing the rules of chess? To be
correct, you’ll need your program to be able to deduce that no
helpmates are possible in this position and thus the game is over.
Stockfish rates this as +0.4 for white, even searching to depth 92.
(Position is due to user supercat on Chess StackExchange.)

4 Perhaps

an enterprising reader can prove that for generalized chess, it is
NP-hard to decide whether the game has ended due to this rule?

5 The first phase was constructed by hand. Software for inserting excursions and checking the result is available at: sf.net/p/tom7misc/svn/HEAD/
tree/trunk/chess/longest.cc

22

2.1

Maximal critical moves

Critical moves are pawn moves and captures. There are 16 pawns,
each of which has 6 squares to move into before promoting, so this
is 16 × 6 critical moves. There are 14 capturable pieces, plus the
16 pawns (they can be captured after promoting); capturing them
nets an additional 16 + 14 critical moves for a total of 126. Each
critical move can be made after a maximum of 75 + 74 reversible
moves, giving (75 + 74 + 1) × 126 = 18900 moves. The final move
would capture the last piece, yielding a draw due to the remaining
kings being insufficient material (“dead position”). This is our
starting upper bound.
We will not quite be able to use the entire critical move budget.
If pawns only move forward in single steps, they will eventually
get blocked by the opposing pawn on the same file. Pawns can
move diagonally off their starting file only by capturing. We have
plenty of capturing to do anyway, so this in no problem. With
four captures per side, the pawns can be doubled, with a clear
route to promotion, like in Figure 6.

black plays 74, and the 150th move is black making the first critical
move.
Note that white is quite constrained during this beginning
phase, as pawn moves are critical moves and must be avoided.
Only the white knights can escape the back rank. When we try to
insert 149 pointless moves, we’ll only be able to move the knights
and rooks, and doing this 75 times must leave e.g. one of the
knights on an opposite-colored square.6 So we have to be a little
careful about the position in which we make black’s first critical
move.
Since white can only free their two knights, these are the only
pieces that can be captured by black pawns. So it will not be
possible for black to double four sets of their pawns as in Figure 6.
This would require white to have a phase of critical moves to
free pieces to capture, then black again to finish doubling pawns,
then white again to promote its freed pawns. Each switch costs
one move off the naı̈ve max. We can be more efficient with an
asymmetric approach.
Black’s first phase of critical moves instead results in this:

rmblkans
7
o0Zpo0Zp
6
0Z0Z0Z0Z
5
Z0o0ZpZ0
4
0ZpZ0o0Z
3
Z0Z0Z0Z0
2
POPOPOPO
1
S0AQJBZR
8

0Zblka0Z
7
ZpZpZpZp
6
0o0o0o0o
5
Z0Z0Z0Z0
4
0Z0Z0Z0Z
3
O0O0O0O0
2
PZPZPZPZ
1
Z0AQJBZ0
8

a

b

c

d

e

f

g

a

h

Figure 6: One way to clear the promotion routes for all pawns
with eight captures.

However, each of these captures is both a pawn move and a
capturing move. This means that we lose 4 + 4 critical moves off
our total budget. 150×(126−8) = 17700 is the new upper bound.

b

c

d

e

f

g

h

The white b and g pawns have a clear route to promotion. White
can free each of the remaining 6 pawns with a single capture.
Between this and black’s two pawn captures, this is the optimal
8 pawn moves that are also captures. Since black has plenty of
freed pieces, white can promote all of their pawns during their
own phase of critical moves, resulting in:

0Z0ZRSNL
o0Zpo0Zp
6
0j0Z0Z0Z
5
Z0o0ZpZN
4
0ZpZ0oQZ
3
S0Z0Z0ZQ
2
0Z0M0Z0Z
1
Z0ANZBJR
8
7

Parity. If a critical move (such as a pawn move) is made by
white, then the first non-critical move is made by black. The
players alternate these pointless moves until black has made 75
and white 74. Now it is white’s move, and white must make a
critical move or the game ends due to the 75-move rule. If instead
we wanted black to make the next critical move, this would happen
after black has made 74 and white 74 non-critical moves. Any time
this happens we lose one move against the upper bound. So, we
want to minimize the number of times we switch which player
is making the critical moves. Obviously we must switch at least
once, because both black and white must make critical moves.
This reduces the upper bound to 17699.

a

b

c

d

e

f

g

h

Now black can promote all of its pawns and capture white’s pieces.
Starting condition. The first critical move should be made by We actually leave the white queen; this turns out to be essential:
black. The starting position (with white to move) is analogous to
6 Knight moves always change the color of the knight’s square, and same
the situation just described, as if black has just made a critical
move, and it’s white’s turn. White will play 75 reversible moves, for rook moves constrained to the a1/b1 or g1/h1 squares.

23

0s0Z0Z0Z
7
Z0Z0Z0Z0
6
0j0Z0J0Z
5
Z0Z0Z0Z0
4
0ZbZ0Z0Z
3
Z0m0Z0Zr
2
0Z0Z0Z0L
1
Z0s0l0sr
8

a
b
c
d
e
f
g h
Finally, white captures all of black’s pieces, and mates the black
king:

0Z0j0Z0Z
7
Z0ZQZ0Z0
6
0Z0ZKZ0Z
5
Z0Z0Z0Z0
4
0Z0Z0Z0Z
3
Z0Z0Z0Z0
2
0Z0Z0Z0Z
1
Z0Z0Z0Z0
8

a
b
c
d
e
f
g h
Since we switch which color is making critical moves a total of
three times, we must come shy of the naı̈ve maximum of 17,700
moves by three. This gives us an upper bound of 17,697 for this
approach, which we will be able to achieve.
Ending condition. The way this game ends is subtle for several
reasons. First, note that we left the white queen on the board and
used it for mate. It is required that white be the one mating for
parity reasons, similar to the reason black must make the first
critical move. Since white makes critical moves in the last phase,
black leads on non-critical moves; at the moment white makes
the checkmating move, black has made 75 non-critical moves, and
white 74. White’s 75th move mates.
But doesn’t this trigger the 75-move rule? No, this rule (9.6.2;
Section 1.1) has a special exception for checkmate: “If the last
move resulted in checkmate, that shall take precedence.” Essentially, we can treat checkmate as a type of critical move.
Why not capture the white queen too? This would be a critical
move, but once two kings remain, the game ends immediately in
a draw. So there is no length advantage here over checkmate. In
fact, attempting to capture may shorten the game: Consider the
position in Figure 3 where black is forced to capture the queen.
This game is over prior to the capture, so this is shorter than the
checkmating sequence! Even if the king had an escape square, it is
arguable (Section 1.3.1) that the game is over in any case: There
is no “series of legal moves” that leads to mate. Either the king
captures the queen (and then clearly no mate is possible with just
the two kings) or the king escapes, but in doing so plays the 75th
non-critical move, and triggers a draw by the 75-move rule.
The most foolproof way to ensure that mate is always possible is
for the game itself to end in mate. This sidesteps any ambiguity

about the way the 75-move condition should be interpreted, as
well.

2.2

Inserting excursions

The game described in Section 2.1 has 118 critical moves in 289
total moves. There is some inefficiency between the critical moves,
but this doesn’t matter since we are trying to generate a long game
anyway. In fact, the next step will be to add as much inefficiency
as possible in between the critical moves.
Each “critical section,” which is the series of moves ending in
a critical move, can be treated independently. If black ends the
section with a critical move, then we want 75 + 74 non-critical
moves to be played, and then black’s critical move. For white,
of course, 74 + 75. There are many ways we could try to make
these 149 moves; we don’t even have to use the moves that are
already there as long as we end up in the right position to make
the critical move. But a simple approach suffices.
For each critical section, we loop over all of the positions encountered, and attempt inserting excursions that return us to the
same position but waste moves. There are two types of excursions
we try: Even excursions (each player makes two moves) and odd
(each player makes three moves).
Even excursions. This four move sequence moves two pieces
of X1 and X2 of opposite colors. X1 moves from s1 to d1 then X2
from s2 to d2 , then X1 moves from d1 back to s1 , and X2 from
d2 back to s2 . Easy. Any piece can perform this maneuver other
than pawns (which would be critical moves anyway) as long as
there are legal squares (considering check, etc.). All of s1 , d1 , s2 ,
d2 must be distinct. No shorter excursions are possible.
Odd excursions. This is the straightforward extension to three
squares (si → mi → di → si ), for a six-move sequence. The
squares for each piece must be distinct but it is possible for e.g. m2
to equal s1 . Knights cannot perform this trick; each move changes
the color of the square the knight sits on, which causes a contradiction with a cycle of length three. All other pieces can do it
with sufficient room. The king, for example, can move horizontally, then diagonally, then vertically back to its starting square.
Note that odd excursions are not possible early in the game
(prior to white moving any pawns), because even when the knights
are free, the rooks only have two squares (and thus the same color
parity argument applies as knights). Some opportunity can be
created by having a black knight capture one of white’s bishops.
Fortunately, we do not need any odd excursions at this point in
the game.
We find excursions by just looping over possible moves that satisfy the criteria, prioritizing odd excursions if the target (divided
by two) is odd. In order to avoid triggering the fivefold repetition
rule, we also keep track of all of the positions encountered, and
never enter a position more than two times. (Here we avoid even
threefold repetition.) It is not necessary to look beyond the critical section, because critical moves make it impossible to return
to a prior position.
This process is not at all guaranteed to work; it may fail to
fill the critical section. Indeed, as discussed in Section 2.1, we
must have at least one move of slack whenever we switch from a
critical move by one player to the other. In practice this approach
succeeds readily, and manages to waste 149 moves in each critical
section of the input game, save for the three times that parity
requires one move of slack. The full game is uselessly included in
a very tiny font in Section 4.

24

3

Reader’s guide

The paper demonstrates a game with three “switches” of which
side is making critical moves; each costs a move against the naı̈ve
maximum due to parity. We clearly need at least one switch (both
sides must make critical moves), but is it possible to do it with
only two? If not, can this lower bound be proved?
The game given is believed to be maximal, as measured in the
number of moves. But, other metrics exist. For example, the
letter g is slightly wider than f, so moving qg3 is typographically
longer than bf1. PGN format itself can be stretched by making
moves that need to be disambiguated (Bff7 means “move the B
on the f file to f7” (not “the giant sword from Final Fantasy 7”,
as many believe)) or checking the opponent’s king for a bonus +.
Some moves are longer in terms of distance√traveled; moving the
queen or bishop between opposite corners is 2×7 squares! What
is the longest game according to these or other metrics?
In Chess, it is impossible to capture your own pieces. How does
this limitation apply to your own life?
In many games of Chess, the black and white pieces are found
to disagree. What does this say about society?
How do you feel about the ending of the game? Is it disappointing that the result is not symmetric (i.e. a draw), or elegant that
it demonstrates its own avoidance of the “dead position” rule?
Was it what you expected?
The character called Q often plays an important role in the
story. How would you describe her personality? How does she
develop over the course of the game?
Is this the longest Chess game? is the author’s sixth paper
about chess for SIGBOVIK, but it is widely believed that nobody
wants to read this kind of thing. What is wrong with him?

4

A longest game

There are jillions of possible games that satisfy the description
above and reach 17,697 moves; here is one of them.7 Critical
moves are marked with [bold]. Note that in the standard PGN
format for listing games, move numbers are “full moves” consisting
of a move by white and then black, whereas we use “move” in
this paper to mean “half-move”; an individual move by one of the
players. Thus the game ends during full move 8,849, one half of
17,697.
1. Na3 nf6 2. Nc4 ng4 3. Na5 ne3 4. Nb3 na6 5. Nc5 nf5 6. Ne6 nd4 7. Nc5 ne6 8. Nb3
nb8 9. Nc5 nf4 10. Na6 nh3 11. Nc5 na6 12. Nb3 nc5 13. Nf3 na6 14. Nc5 ng1 15. Ng5 nh3
16. Nge4 ng1 17. Na4 nb8 18. Ng3 nc6 19. Nc5 ne5 20. Nf5 nh3 21. Ne6 ng1 22. Nc5 ng4
23. Ng3 ne5 24. Nce4 nc6 25. Nc5 nh3 26. Na4 ng1 27. Nf5 nb8 28. Ng3 nh3 29. Ne4 ng1
30. Nac5 na6 31. Na4 nh3 32. Nac5 nf4 33. Ng5 nh3 34. Nb3 ng1 35. Nc5 nb4 36. Nf3 na6
37. Ne5 nh3 38. Nf3 nb4 39. Nb3 na6 40. Na5 nc5 41. Nb3 ng5 42. Ng1 nh3 43. Na5 na6 44.
Nb3 nb4 45. Nc5 na6 46. Na4 nb8 47. Nc5 ng5 48. Na6 nh3 49. Nb4 nf4 50. Na6 ng6 51. Nc5
nf4 52. Nb3 ne6 53. Nc5 na6 54. Nb3 nd4 55. Nc5 nb4 56. Ne6 na6 57. Nf4 nf5 58. Ne6 nh4
59. Nc5 nf5 60. Na4 ne3 61. Nc5 ng4 62. Nb3 ne3 63. Nh3 nb8 64. Ng1 nf5 65. Na5 ne3 66.
Nc6 ng4 67. Na5 nc6 68. Nc4 nb8 69. Nf3 nf6 70. Ng1 na6 71. Na3 nb8 72. Nf3 ng8 73. Ng1
na6 74. Nb1 nb8 75. Nf3 [b6] 76. Ne5 ba6 77. Nc4 nf6 78. Nca3 nh5 79. Nc3 nf4 80. Nc4
bb7 81. Na5 nh5 82. Nc4 be4 83. Na5 bc6 84. Nb1 qc8 85. Nb7 bb5 86. Na5 ba6 87. Nb7 nf6
88. Nd8 nc6 89. Nb7 na5 90. Nc3 nb3 91. Na4 qb8 92. Nc3 ng8 93. Na5 qc8 94. Nd5 bd3
95. Nb4 nh6 96. Na6 ng8 97. Nb4 qb7 98. Nd5 qc8 99. Ne3 ba6 100. Nd5 qd8 101. Nc3 qc8
102. Nc6 qb8 103. Na5 nf6 104. Nb7 ng8 105. Na4 nf6 106. Na5 qc8 107. Nb7 nd4 108. Nc3
nb3 109. Nb1 na5 110. Nc3 nc6 111. Nb1 ne4 112. Nd8 nf6 113. Ne6 nb8 114. Nd8 bd3 115.
Nb7 ba6 116. Nc5 nh5 117. Nb7 nf4 118. Na5 nh5 119. Nc6 bb5 120. Na5 bc4 121. Nb7 bb5
122. Nc3 bc6 123. Nb1 na6 124. Na5 nb8 125. Na3 qd8 126. Nb1 nf6 127. Nc3 nh5 128. Nb7
be4 129. Na5 bf5 130. Nc4 be4 131. Na4 bb7 132. Nc3 ng3 133. Na5 nh5 134. Nb3 nf4 135.
Na5 bc6 136. Nc4 bb7 137. Na3 ba6 138. Nc4 nh5 139. Na3 bb7 140. Ncb1 ba6 141. Nb5 nf6
142. N5a3 nd5 143. Nc4 nf6 144. Nba3 ng8 145. Nb1 qc8 146. Ne5 qd8 147. Nc3 bc8 148.
Nb1 nf6 149. Nf3 ng8 150. Nc3 [g6] 151. Ng5 bb7 152. Nce4 bh6 153. Nc5 bf3 154. Nce4 nc6
155. Nc3 bd5 156. Na4 ne5 157. Ne6 bf8 158. Nc3 nh6 159. Nc5 nc6 160. Nb5 bb3 161. Na6
ba4 162. Nb4 na5 163. Na6 ng4 164. Nc3 nc6 165. Ne4 bb3 166. Nb4 bc4 167. Ng5 bd3 168.
Nf3 nge5 169. Nd5 nb8 170. Ng1 bb5 171. Nf3 bd3 172. Nc3 nbc6 173. Nd5 bc4 174. Nb4
bd3 175. Ng5 ng4 176. Nf3 bc4 177. Ng5 qb8 178. Ne4 qd8 179. Nc5 bb3 180. Ne4 nd4 181.
Na6 nc6 182. Ng3 ba4 183. Ne4 bg7 184. Nc3 bf8 185. Nb1 na5 186. Nc3 bb3 187. Nb5 ba4
188. Nb8 nh6 189. Na6 bb3 190. Nb4 ba4 191. Nd3 nc6 192. Nb4 nb8 193. Na6 nc6 194. Na3
bb3 195. Nb5 nb8 196. Nc5 nc6 197. Ne6 bd5 198. Nc5 nf5 199. Nc3 nh6 200. Nb1 ne5 201.
Nc3 nf5 202. Ne6 nh6 203. Ne4 ng8 204. Nc3 be4 205. Na4 bd5 206. Nd4 bh6 207. Ne6 nf6
208. Ng5 ng8 209. Ne4 nc6 210. Ng5 qb8 211. Nc3 qd8 212. Nb5 bf3 213. Nc3 nf6 214. Nce4
ng8 215. Ne6 nb8 216. N6g5 bh5 217. Nc5 bf3 218. Na4 bb7 219. Nc5 na6 220. Nce4 nb8

7 It can also be downloaded at tom7.org/chess/longest.pgn. Many chess
programs fail to load the whole game, but this is because they decided not
to implement the full glory of chess.

221. Nc3 bf8 222. Nce4 bc8 223. Nc3 nh6 224. Nf3 ng8 225. Ne5 [g5] 226. Nb5 nc6 227. Na3
ba6 228. Nd3 qc8 229. Nb4 nd8 230. Nc6 bb7 231. Nb8 be4 232. Nc6 bf5 233. Nb1 nh6 234.
Na3 bd3 235. Nc4 qa6 236. Nb8 qc8 237. Na5 bf5 238. Na6 ng4 239. Nb4 nc6 240. Nb7 nf6
241. Na5 nd4 242. Nc4 qa6 243. Ne5 bh3 244. Nbc6 nh5 245. Na5 bh6 246. Nf3 qb7 247. Ne5
ne6 248. Nf3 nd4 249. Nb3 qa6 250. Na5 qb5 251. Ne5 qa6 252. Nac6 bf8 253. Na5 nf6 254.
Nac6 bf5 255. Nb4 bh3 256. Nc4 bf5 257. Nd5 qc8 258. Nb4 nc6 259. Na5 nd4 260. Nb7 nc6
261. Nd3 ng4 262. Nb4 be4 263. Na5 bf5 264. Na6 nd8 265. Nb4 nh6 266. Na6 ng8 267. Nb8
nh6 268. Nc4 bd3 269. Na5 qa6 270. Nc4 qa5 271. Nc6 qa6 272. Nd4 qc8 273. Nc6 nf5 274.
Na3 nh6 275. Na5 bf5 276. Nc6 be4 277. Nb1 bf5 278. Na5 ng8 279. Nc6 bg4 280. Na3 bf5
281. Nb4 be4 282. Nc6 bf3 283. Nb8 be4 284. Nb1 bb7 285. Na3 bh6 286. Nc6 bf8 287. Nc4
ba6 288. Na3 qb7 289. Nb4 qc8 290. Nd5 nc6 291. Nb4 nb8 292. Nd3 nc6 293. Ne5 qd8 294.
Nd3 bc8 295. Ne5 bh6 296. Nb5 bf8 297. Nd3 nb8 298. Ne5 bg7 299. Nc3 bf8 300. Nd5 [b5]
301. Ng6 bh6 302. Nh4 nf6 303. Nf4 bb7 304. Nd5 ba6 305. Ng6 bf8 306. Ne3 nd5 307. Nc4
bh6 308. Nb6 nf4 309. Nf8 ne6 310. Ng6 nc5 311. Nc8 nb3 312. Nf8 na5 313. Ng6 nbc6 314.
Nh4 nb8 315. Nb6 bf8 316. Nc8 nac6 317. Nb6 ne5 318. Nc8 bh6 319. Ng6 nec6 320. Nb6 ne5
321. Nc8 nbc6 322. Nh4 nb8 323. Nf3 bf8 324. Nh4 nc4 325. Nb6 ne5 326. Nf3 nec6 327. Nh4
bg7 328. Nc8 bf8 329. Ng6 na5 330. Nh4 nc4 331. Nb6 na5 332. Nd5 bh6 333. Nb6 nac6 334.
Nc8 na5 335. Nf5 nbc6 336. Nh4 bg7 337. Ng6 bh6 338. Nf4 nb8 339. Ng6 nb7 340. Nf8 na5
341. Nb6 nb3 342. Nc8 bb7 343. Ng6 ba6 344. Nf4 nc5 345. Ng6 nb7 346. Nb6 nc5 347. Nd5
ne6 348. Nb6 bc8 349. Nf8 ba6 350. Na4 nf4 351. Nb6 bb7 352. Ng6 ba6 353. Nc8 nd5 354.
Nb6 ne3 355. Nc4 nd5 356. Ne3 bf8 357. Nc4 nf6 358. Ne3 qc8 359. Nd5 qd8 360. Nb6 bh6
361. Nd5 ng4 362. Nh4 nf6 363. Nf5 bb7 364. Nh4 bc6 365. Nf4 bb7 366. Nf5 bc8 367. Nh4
na6 368. Nd5 nb8 369. Nc3 ng8 370. Nd5 bg7 371. Ng6 bh6 372. Ne3 bf8 373. Nd5 nh6 374.
Ne5 ng8 375. Nc4 [bxc4] 376. Nb4 bh6 377. Nc6 bg7 378. Nb4 bf8 379. Nc6 na6 380. Nb4
bg7 381. Nd3 bb7 382. Ne5 bh6 383. Nc6 qb8 384. Nd8 nb4 385. Nc6 qc8 386. Na5 qd8 387.
Nb3 bd5 388. Nd4 nc6 389. Nf3 na5 390. Nh4 nb3 391. Ng6 bg7 392. Nf4 bc3 393. Ne6 bb4
394. Nd4 qb8 395. Ne6 ba5 396. Nd8 be4 397. Nc6 qc8 398. Nb4 bd3 399. Na6 nc5 400. Nb4
nb3 401. Nd5 be4 402. Nb4 qd8 403. Nc6 qc8 404. Nd8 qb8 405. Nc6 bd5 406. Nd8 qb7 407.
Ne6 qb8 408. Nc5 bb4 409. Ne6 bf3 410. Nd4 bd5 411. Nf3 qd8 412. Nd4 ba3 413. Ne6 bb4
414. Nf8 bc3 415. Ne6 bf3 416. Nf4 bd5 417. Nh3 bg7 418. Nf4 bh6 419. Ng6 bg7 420. Nh4
bh6 421. Nf5 na5 422. Nh4 nc6 423. Nf3 na5 424. Nd4 nc6 425. Nb3 nb4 426. Nd4 bb7 427.
Nb3 nf6 428. Na5 ng8 429. Nc6 qc8 430. Na5 qb8 431. Nc6 ba6 432. Nd8 bb7 433. Ne6 na6
434. Nd8 bg7 435. Nc6 bh6 436. Nb4 qd8 437. Nc6 nc5 438. Ne5 na6 439. Nd3 bg7 440. Ne5
bc8 441. Nd3 be5 442. Nb4 bg7 443. Nc6 bf8 444. Nb4 nb8 445. Nc6 bg7 446. Nb4 bh6 447.
Nc6 bf8 448. Nb4 ba6 449. Nd5 bc8 450. Nf4 [gxf4] 451. Rg1 bb7 452. Rh1 bh6 453. Rg1
bg5 454. Rh1 na6 455. Rg1 bh4 456. Rh1 bc6 457. Rg1 qb8 458. Rh1 ba4 459. Rg1 nf6 460.
Rh1 qb5 461. Rg1 qe5 462. Rh1 qa5 463. Rg1 qh5 464. Rh1 bg5 465. Rg1 qg6 466. Rh1 ne4
467. Rg1 bc6 468. Rh1 bb5 469. Rg1 qh5 470. Rh1 qh4 471. Rg1 nac5 472. Rh1 qh3 473. Rg1
nb7 474. Rh1 qg3 475. Rg1 nec5 476. Rh1 nb3 477. Rg1 nd6 478. Rh1 qh4 479. Rg1 nc5 480.
Rh1 bh6 481. Rg1 na4 482. Rh1 qh3 483. Rg1 bc6 484. Rh1 nb7 485. Rg1 qg3 486. Rh1 nd8
487. Rg1 qg6 488. Rh1 qg3 489. Rg1 nb7 490. Rh1 qh3 491. Rg1 nd6 492. Rh1 bb5 493. Rg1
qh4 494. Rh1 nc5 495. Rg1 bg5 496. Rh1 nb3 497. Rg1 qg3 498. Rh1 nb7 499. Rg1 n3c5 500.
Rh1 ne4 501. Rg1 qh3 502. Rh1 nbc5 503. Rg1 qh4 504. Rh1 na6 505. Rg1 qh5 506. Rh1 qg6
507. Rg1 bc6 508. Rh1 ba4 509. Rg1 nf6 510. Rh1 qh5 511. Rg1 bh4 512. Rh1 qa5 513. Rg1
qe5 514. Rh1 qb5 515. Rg1 qb8 516. Rh1 ng8 517. Rg1 bc6 518. Rh1 qd8 519. Rg1 bb7 520.
Rh1 bg5 521. Rg1 nb8 522. Rh1 bh6 523. Rg1 bf8 524. Rh1 bc8 525. Rg1 [c6] 526. Rh1 nf6
527. Rg1 na6 528. Rh1 nh5 529. Rg1 nc7 530. Rh1 nb5 531. Rg1 ba6 532. Rh1 qc8 533. Rg1
nd4 534. Rh1 ng7 535. Rg1 bb5 536. Rh1 nh5 537. Rg1 nf5 538. Rh1 nhg3 539. Rg1 qa6 540.
Rh1 nd4 541. Rg1 qc8 542. Rh1 ne4 543. Rg1 ba4 544. Rh1 nf5 545. Rg1 bg7 546. Rh1 neg3
547. Rg1 be5 548. Rh1 bc3 549. Rg1 ng7 550. Rh1 n3h5 551. Rg1 qd8 552. Rh1 bb4 553. Rg1
bb3 554. Rh1 bc3 555. Rg1 qc7 556. Rh1 qb8 557. Rg1 qd6 558. Rh1 bf6 559. Rg1 qe6 560.
Rh1 qh3 561. Rg1 qg3 562. Rh1 ba4 563. Rg1 bb3 564. Rh1 qh3 565. Rg1 qe6 566. Rh1 qd6
567. Rg1 bc3 568. Rh1 qb8 569. Rg1 qc7 570. Rh1 qd8 571. Rg1 bb4 572. Rh1 ba4 573. Rg1
bc3 574. Rh1 qc8 575. Rg1 ng3 576. Rh1 n7f5 577. Rg1 be5 578. Rh1 bg7 579. Rg1 ne4 580.
Rh1 bf8 581. Rg1 nd4 582. Rh1 bb5 583. Rg1 ng3 584. Rh1 qa6 585. Rg1 ndf5 586. Rh1 qc8
587. Rg1 nh5 588. Rh1 nd4 589. Rg1 ng7 590. Rh1 ba6 591. Rg1 nh5 592. Rh1 nb5 593. Rg1
qd8 594. Rh1 bc8 595. Rg1 nc7 596. Rh1 na6 597. Rg1 nf6 598. Rh1 nb8 599. Rg1 ng8 600.
Rh1 [c5] 601. Rg1 ba6 602. Rh1 nf6 603. Rg1 qb6 604. Rh1 ng4 605. Rg1 bc8 606. Rh1 ne3
607. Rg1 nc6 608. Rh1 qa6 609. Rg1 ng4 610. Rh1 nce5 611. Rg1 nf6 612. Rh1 bb7 613. Rg1
qb5 614. Rh1 bf3 615. Rg1 qa5 616. Rh1 qc3 617. Rg1 ng6 618. Rh1 ng4 619. Rg1 qd4 620.
Rh1 qg7 621. Rg1 qg8 622. Rh1 be4 623. Rg1 qg7 624. Rh1 n4e5 625. Rg1 bc6 626. Rh1 ba4
627. Rg1 nh4 628. Rh1 bb5 629. Rg1 qg3 630. Rh1 ba4 631. Rg1 qg6 632. Rh1 bc6 633. Rg1
bf3 634. Rh1 qg4 635. Rg1 nc6 636. Rh1 be4 637. Rg1 qg7 638. Rh1 qg4 639. Rg1 bf3 640.
Rh1 ne5 641. Rg1 qg6 642. Rh1 bc6 643. Rg1 ba4 644. Rh1 qg3 645. Rg1 bb5 646. Rh1 qg7
647. Rg1 ba4 648. Rh1 nhg6 649. Rg1 bc6 650. Rh1 be4 651. Rg1 ng4 652. Rh1 qg8 653. Rg1
bf3 654. Rh1 qg7 655. Rg1 qd4 656. Rh1 qc3 657. Rg1 nf6 658. Rh1 ne5 659. Rg1 qa5 660.
Rh1 qb5 661. Rg1 bb7 662. Rh1 qa6 663. Rg1 bc8 664. Rh1 nfg4 665. Rg1 nc6 666. Rh1 ne3
667. Rg1 qb6 668. Rh1 nb8 669. Rg1 ng4 670. Rh1 ba6 671. Rg1 nf6 672. Rh1 qd8 673. Rg1
ng8 674. Rh1 bc8 675. Rg1 [f6] 676. Rh1 bh6 677. Rg1 bb7 678. Rh1 qa5 679. Rg1 qb5 680.
Rh1 qc6 681. Rg1 qe4 682. Rh1 qg6 683. Rg1 qf5 684. Rh1 bg7 685. Rg1 qh5 686. Rh1 bd5
687. Rg1 bh6 688. Rh1 qg5 689. Rg1 nc6 690. Rh1 bg7 691. Rg1 be4 692. Rh1 qf5 693. Rg1
ne5 694. Rh1 bd3 695. Rg1 bf8 696. Rh1 qg4 697. Rg1 nc6 698. Rh1 qe6 699. Rg1 nb8 700.
Rh1 bg6 701. Rg1 qd6 702. Rh1 bd3 703. Rg1 be4 704. Rh1 qb6 705. Rg1 nc6 706. Rh1 qc7
707. Rg1 qa5 708. Rh1 qb5 709. Rg1 nd4 710. Rh1 qa4 711. Rg1 nc6 712. Rh1 qb4 713. Rg1
qa4 714. Rh1 nd4 715. Rg1 qb5 716. Rh1 nc6 717. Rg1 qa5 718. Rh1 qc7 719. Rg1 qb6 720.
Rh1 nb8 721. Rg1 qd6 722. Rh1 bd3 723. Rg1 bg6 724. Rh1 qe6 725. Rg1 bd3 726. Rh1 nc6
727. Rg1 qg4 728. Rh1 ne5 729. Rg1 qf5 730. Rh1 bg7 731. Rg1 be4 732. Rh1 nc6 733. Rg1
qg5 734. Rh1 bd5 735. Rg1 bh6 736. Rh1 nb8 737. Rg1 qh5 738. Rh1 bg7 739. Rg1 bb7 740.
Rh1 qf5 741. Rg1 bh6 742. Rh1 qg6 743. Rg1 qe4 744. Rh1 qc6 745. Rg1 qb5 746. Rh1 qa5
747. Rg1 qd8 748. Rh1 bc8 749. Rg1 bf8 750. Rh1 [f5] 751. Rg1 qc7 752. Rh1 bh6 753. Rg1
qb7 754. Rh1 qd5 755. Rg1 ba6 756. Rh1 qe5 757. Rg1 bf8 758. Rh1 qc3 759. Rg1 qe3 760.
Rh1 bg7 761. Rg1 qb3 762. Rh1 qa4 763. Rg1 bc8 764. Rh1 bd4 765. Rg1 be3 766. Rh1 qb3
767. Rg1 bb7 768. Rh1 bf3 769. Rg1 qb7 770. Rh1 qc8 771. Rg1 qd8 772. Rh1 be4 773. Rg1
bd5 774. Rh1 bf7 775. Rg1 qa5 776. Rh1 qb6 777. Rg1 qc7 778. Rh1 qc8 779. Rg1 qc6 780.
Rh1 qe6 781. Rg1 qh6 782. Rh1 bd4 783. Rg1 qf8 784. Rh1 nc6 785. Rg1 be3 786. Rh1 bd5
787. Rg1 ne5 788. Rh1 nc6 789. Rg1 bf7 790. Rh1 bd4 791. Rg1 nb8 792. Rh1 qh6 793. Rg1
be3 794. Rh1 qe6 795. Rg1 qc6 796. Rh1 qc8 797. Rg1 qc7 798. Rh1 qb6 799. Rg1 qa5 800.
Rh1 qd8 801. Rg1 bd5 802. Rh1 be4 803. Rg1 bf3 804. Rh1 qc8 805. Rg1 qb7 806. Rh1 qb3
807. Rg1 bb7 808. Rh1 bc8 809. Rg1 qa4 810. Rh1 bd4 811. Rg1 bg7 812. Rh1 ba6 813. Rg1
qb3 814. Rh1 qe3 815. Rg1 bf8 816. Rh1 qc3 817. Rg1 qe5 818. Rh1 bh6 819. Rg1 qd5 820.
Rh1 bc8 821. Rg1 qb7 822. Rh1 qc7 823. Rg1 bf8 824. Rh1 qd8 825. [b3] nh6 826. Rg1 nc6
827. Ba3 nb8 828. Qc1 qb6 829. Bb2 qf6 830. Bd4 qc6 831. Be5 qb5 832. Qd1 ng4 833. Qb1
qb7 834. Bg7 qb4 835. Qc1 qc3 836. Rh1 qh3 837. Qb1 qh4 838. Bb2 qh6 839. Bg7 qd6 840.
Qd1 nh6 841. Be5 qg6 842. Bc3 qg7 843. Rg1 qg3 844. Qb1 bb7 845. Bg7 bd5 846. Bc3 bb7
847. Bf6 bc8 848. Bc3 qf3 849. Qd1 qg3 850. Bd4 qg7 851. Bc3 bb7 852. Rh1 bc8 853. Bb4
qg6 854. Bc3 qb6 855. Be5 qg6 856. Qb1 qd6 857. Qd1 ba6 858. Bg7 bc8 859. Qc1 ng4 860.
Qd1 ba6 861. Qb1 bc8 862. Bb2 qh6 863. Bg7 qh4 864. Bb2 qh3 865. Bg7 na6 866. Qc1 nb8
867. Bh6 qc3 868. Bg7 ne3 869. Rg1 ng4 870. Bf6 qb4 871. Bg7 nf6 872. Qb1 ng4 873. Bc3
qb7 874. Bg7 nc6 875. Be5 nb8 876. Bc7 qb5 877. Be5 bh6 878. Qd1 bf8 879. Bg7 nh6 880.
Be5 nf7 881. Qc1 nh6 882. Bd6 qc6 883. Be5 qc7 884. Bd4 qc6 885. Qb1 qf6 886. Qc1 bg7
887. Bb2 bf8 888. Ba3 qb6 889. Bb2 qd8 890. Ba3 bb7 891. Qd1 bc8 892. Bb2 nc6 893. Ba3
ne5 894. Bc1 nc6 895. Bb2 nb8 896. Bc1 bg7 897. Rh1 bf8 898. Ba3 ng8 899. Bc1 nc6 900.
[b4] ba6 901. Ba3 bh6 902. Bb2 qc7 903. Rg1 na5 904. Bg7 qc8 905. Bf6 nb3 906. Rh1 bb7
907. Bg5 ba6 908. Qb1 qc7 909. Rg1 qd6 910. Qb2 qc6 911. Qb1 qe6 912. Qc1 nd4 913. Qd1
bf8 914. Bf6 nh6 915. Qb1 bb7 916. Qc1 qg8 917. Qa3 qg3 918. Rh1 bf3 919. Qa5 qh3 920.
Rg1 qg3 921. Rh1 bd5 922. Qa3 bf3 923. Qc1 bb7 924. Qa3 qd3 925. Rg1 qg3 926. Be5 qg8
927. Bf6 ng4 928. Qc1 nh6 929. Qb2 qe6 930. Qc1 nf7 931. Qb1 nh6 932. Qb2 ba6 933. Qb1
qb6 934. Qd1 qe6 935. Be5 ng8 936. Bf6 qe3 937. Bg5 qe6 938. Bh4 bh6 939. Bg5 qd6 940.
Qc1 qe6 941. Bh4 nb3 942. Bg5 qf6 943. Qb1 qe6 944. Bf6 qc6 945. Bg5 qg6 946. Qb2 qc6
947. Qa3 qd6 948. Qb2 bb7 949. Qb1 ba6 950. Bf6 qc7 951. Bg5 qb8 952. Rh1 qc7 953. Bf6
qc8 954. Bg5 nd4 955. Qd1 nb3 956. Rg1 bb7 957. Rh1 qd8 958. Bf6 qc8 959. Rg1 ba6 960.
Rh1 na5 961. Rg1 qd8 962. Bg7 qc8 963. Qc1 qc7 964. Qd1 qb7 965. Bb2 qc7 966. Qb1 nc6
967. Qd1 nb8 968. Rh1 nc6 969. Qc1 qd8 970. Qd1 bf8 971. Ba3 bh6 972. Bc1 bf8 973. Bb2
bc8 974. Bc1 nd4 975. [b5] qb6 976. Rg1 qd6 977. Bb2 bg7 978. Rh1 nf6 979. Qc1 qb6 980.
Ba3 ng8 981. Bb2 bf6 982. Rg1 nb3 983. Be5 qe6 984. Bb8 qd5 985. Qb2 nd4 986. Bd6 be5
987. Bc7 bf6 988. Bb6 qf7 989. Rh1 bg7 990. Qc1 bf8 991. Bc7 qd5 992. Qb1 bb7 993. Qb2
qd6 994. Qa3 qf6 995. Be5 qg7 996. Bc7 qf6 997. Qd3 qd6 998. Qa3 nh6 999. Qb2 ng8 1000.
Qb4 qd5 1001. Qb2 qe4 1002. Qb1 qd5 1003. Qb4 bc8 1004. Qb1 qb7 1005. Qc1 qd5 1006.
Be5 qf7 1007. Bc7 nc6 1008. Bb6 nd4 1009. Qa3 bg7 1010. Qc1 qf6 1011. Qb2 qf7 1012. Qa3
bf6 1013. Qb2 nb3 1014. Rg1 nd4 1015. Qc1 qd5 1016. Qb2 bb7 1017. Bc7 bc8 1018. Qb4
be5 1019. Qb2 ba6 1020. Bd6 bc8 1021. Rh1 bf6 1022. Rg1 qe5 1023. Bb8 qd5 1024. Qb1
nb3 1025. Qb2 qe5 1026. Qc1 qd5 1027. Qa3 qe6 1028. Qc1 nh6 1029. Be5 ng8 1030. Bc3
qb6 1031. Be5 qc6 1032. Bb2 qb6 1033. Ba3 nd4 1034. Bb2 qc7 1035. Rh1 qb6 1036. Ba3
bg7 1037. Bb2 nf6 1038. Ba3 qd6 1039. Bb2 nb3 1040. Qd1 nd4 1041. Ba3 ng8 1042. Bb2
nh6 1043. Rg1 ng8 1044. Bc3 bf8 1045. Bb2 qc6 1046. Bc1 qd6 1047. Rh1 qb6 1048. Rg1 qd8

25

1049. Rh1 nb3 1050. [cxb3] bg7 1051. Qc2 nh6 1052. Qc3 ng4 1053. Qc2 bf8 1054. Qe4
ba6 1055. Qc2 bb7 1056. Qb1 nh6 1057. Bb2 qc7 1058. Bc3 bg7 1059. Bd4 ng4 1060. Qb2
qd8 1061. Rg1 bh6 1062. Rh1 qa5 1063. Bg7 nf6 1064. Rg1 qb4 1065. Qb1 bf3 1066. Qd3
nh5 1067. Qd5 qa3 1068. Rh1 nf6 1069. Bf8 qb2 1070. Bg7 qa3 1071. Qe6 nh5 1072. Qd5
be4 1073. Rg1 bf3 1074. Qd3 qb4 1075. Qd5 nf6 1076. Qd3 bd5 1077. Qb1 bf3 1078. Rh1
bb7 1079. Rg1 ba6 1080. Qb2 bb7 1081. Qe5 qa5 1082. Qb2 bg5 1083. Rh1 bh6 1084. Qb1
ng4 1085. Qb2 qa3 1086. Bd4 qa5 1087. Qc3 qd8 1088. Qb2 bc8 1089. Rg1 bb7 1090. Bf6
bg7 1091. Bd4 ba6 1092. Rh1 bb7 1093. Be5 qc7 1094. Bd4 ne5 1095. Qb1 ng4 1096. Qd1
nh6 1097. Qb1 bf3 1098. Bc3 bb7 1099. Qc2 bf8 1100. Qb1 qd6 1101. Bb2 qc7 1102. Bf6
qd8 1103. Bb2 qb8 1104. Bc1 qd8 1105. Qe4 ng4 1106. Qb1 bc8 1107. Qc2 bb7 1108. Qb2
ba6 1109. Qc2 qc8 1110. Qe4 qd8 1111. Qe5 bc8 1112. Qe4 nf6 1113. Qc2 ng4 1114. Bb2
bg7 1115. Bc1 bf6 1116. Qc3 bg7 1117. Qf6 nh6 1118. Qc3 bf6 1119. Qc2 bg7 1120. Ba3 ng8
1121. Bc1 be5 1122. Qd1 bg7 1123. Rg1 bf8 1124. Rh1 bb7 1125. [a3] qb8 1126. Rg1 bf3
1127. Qc2 bh5 1128. Qa2 qe5 1129. Rh1 qd4 1130. Qc2 qg7 1131. Qa2 qh6 1132. Rg1 qa6
1133. Qc2 bg6 1134. Qd3 bf7 1135. Qe3 qe6 1136. Qe4 qh6 1137. Qe5 qe6 1138. Qc7 qb6
1139. Bb2 nf6 1140. Bd4 bg7 1141. Qe5 bg6 1142. Qd6 bf7 1143. Qb8+ qd8 1144. Qe5 bh5
1145. Qd6 ng4 1146. Qb8 nf6 1147. Qd6 bf8 1148. Qe5 bg7 1149. Rh1 bf7 1150. Rg1 qb6
1151. Qb8+ qd8 1152. Qd6 qb6 1153. Rh1 bg6 1154. Rg1 qa5 1155. Qe5 qb6 1156. Qc7 bf7
1157. Qe5 bf8 1158. Qc7 bg6 1159. Bb2 bf7 1160. Rh1 ng8 1161. Rg1 bh6 1162. Bc1 bf8 1163.
Qe5 qe6 1164. Qc7 qh6 1165. Qe5 qh5 1166. Qe4 qh6 1167. Qb7 qe6 1168. Qe4 bh5 1169.
Qe3 bf7 1170. Qh3 qa6 1171. Qe3 bd5 1172. Qd3 bf7 1173. Qc3 bg6 1174. Qd3 bg7 1175.
Qc2 bf8 1176. Bb2 bh5 1177. Bc1 bg7 1178. Qa2 bf8 1179. Qb1 qh6 1180. Qa2 qg7 1181. Rh1
qh6 1182. Qc2 qg7 1183. Qd1 qd4 1184. Qc2 bh6 1185. Qa2 bf8 1186. Qb2 qe5 1187. Qa2
qg7 1188. Rg1 qe5 1189. Qb1 qb8 1190. Qa2 qd8 1191. Qc2 qb8 1192. Qd3 bf3 1193. Qc2
bg7 1194. Qd1 bf8 1195. Bb2 bb7 1196. Bc1 qd6 1197. Rh1 qb8 1198. Qc2 qd8 1199. Qd1
rc8 1200. [a4] bc6 1201. Ba3 rb8 1202. Bb4 nh6 1203. Qc2 rb7 1204. Qb2 qc7 1205. Qc1
bg7 1206. Ba5 bf3 1207. Qb1 nf7 1208. Qd3 bd5 1209. Qg3 be5 1210. Qf3 nd8 1211. Qe4
be6 1212. Bc3 bg8 1213. Qc2 qd6 1214. Qa2 qc6 1215. Qc2 bf7 1216. Qe4 be6 1217. Qb1
bf6 1218. Bd4 qa6 1219. Qe4 be5 1220. Qc6 bg8 1221. Qe4 be6 1222. Qd3 bf6 1223. Qe4
bf7 1224. Qb1 be6 1225. Qc2 qc6 1226. Qb1 qe4 1227. Bc3 qc6 1228. Bb2 be5 1229. Bc3
bb8 1230. Qe4 be5 1231. Rg1 bf7 1232. Rh1 rb6 1233. Qc2 rb7 1234. Qa2 bg8 1235. Qc2
qd6 1236. Qa2 qc7 1237. Qc2 nf7 1238. Qe4 nd8 1239. Bd4 be6 1240. Bc3 nc6 1241. Ba5
nd8 1242. Bb4 bd5 1243. Ba5 bg8 1244. Qf3 bd5 1245. Bb4 nf7 1246. Ba5 qb6 1247. Qg3
qc7 1248. Rg1 bg7 1249. Rh1 bb2 1250. Qd3 bg7 1251. Qb1 bf3 1252. Qd3 nh6 1253. Qb1
bc3 1254. Qc1 bg7 1255. Bb6 bc6 1256. Ba5 qc8 1257. Bb4 qc7 1258. Qb1 bf8 1259. Qc1
bd5 1260. Qb2 bc6 1261. Qa2 qd8 1262. Qb2 rc7 1263. Qc2 rb7 1264. Bc3 rb8 1265. Bb4
ba8 1266. Qd1 bc6 1267. Qb1 ng8 1268. Qd1 qa5 1269. Ba3 qd8 1270. Qb1 rc8 1271. Qd1
bg7 1272. Bc1 bf8 1273. Bb2 bb7 1274. Bc1 rc6 1275. [a5] rf6 1276. Qc2 rb6 1277. Rg1
bd5 1278. Qd1 qc7 1279. Ba3 nh6 1280. Qc2 rg6 1281. Qc1 ra6 1282. Qc2 ba8 1283. Qb2
qd8 1284. Qa2 bd5 1285. Qb1 bc6 1286. Qc2 be4 1287. Rh1 ng8 1288. Qd3 qa8 1289. Qc2
rb6 1290. Qa2 qd5 1291. Qc2 nf6 1292. Qb1 nh5 1293. Qd3 qa8 1294. Qg3 qc6 1295. Qg4
qa8 1296. Qg3 bd5 1297. Qd3 be4 1298. Bc1 qd5 1299. Ba3 re6 1300. Qb1 rb6 1301. Qc2
nf6 1302. Qb1 ng8 1303. Qc2 qd4 1304. Qa2 qd5 1305. Bb2 qa8 1306. Ba3 qc8 1307. Qc2
qa8 1308. Qd1 ra6 1309. Qc2 nf6 1310. Qd3 ng8 1311. Qd4 qd8 1312. Qd3 bh6 1313. Qc2
bf8 1314. Qd1 nh6 1315. Qc2 bd3 1316. Rg1 be4 1317. Qc3 bc6 1318. Qc2 qc8 1319. Qb1
qd8 1320. Bb2 bd5 1321. Ba3 rf6 1322. Qa2 ra6 1323. Bb2 ba8 1324. Ba3 qb8 1325. Qb2
qd8 1326. Qc3 qc7 1327. Qb2 qd6 1328. Qc2 qc7 1329. Bb4 bd5 1330. Ba3 ng4 1331. Qc1
nh6 1332. Rh1 rg6 1333. Rg1 bg8 1334. Qc2 bd5 1335. Qb2 rb6 1336. Qc2 qc8 1337. Qd1
qc7 1338. Qc1 ng8 1339. Qd1 bh6 1340. Bc1 bf8 1341. Qc2 qd8 1342. Qd1 bb7 1343. Qc2
re6 1344. Rh1 rb6 1345. Bb2 rf6 1346. Bc1 be4 1347. Qd1 bb7 1348. Bb2 rc6 1349. Bc1 rd6
1350. [a6] nh6 1351. Ba3 qb6 1352. Qc2 rd5 1353. Bb2 bc8 1354. Ba3 qg6 1355. Bc1 ng8
1356. Qe4 nf6 1357. Qb1 qg7 1358. Bb2 qg3 1359. Qa2 re5 1360. Ba3 qh3 1361. Qc2 nd5
1362. Rg1 qh4 1363. Qb2 nf6 1364. Qa2 qg3 1365. Qb1 qg8 1366. Qb2 ng4 1367. Qc1 re6
1368. Qd1 bh6 1369. Qb1 rd6 1370. Qd1 re6 1371. Qb1 bf8 1372. Qd1 bb7 1373. Qc1 bc8
1374. Qc2 re5 1375. Qc1 bh6 1376. Qb2 bf8 1377. Qc2 nf6 1378. Qb2 qe6 1379. Qb1 qg8
1380. Qe4 qg3 1381. Qb1 bb7 1382. Qa2 bc8 1383. Bc1 qh4 1384. Ba3 bg7 1385. Qb2 bf8
1386. Qc1 nd5 1387. Qb2 nb6 1388. Qc2 nd5 1389. Qe4 qh3 1390. Qc2 qh6 1391. Rh1 qh3
1392. Qa2 nf6 1393. Qc2 qg3 1394. Qa2 nh5 1395. Bb2 nf6 1396. Qa3 rd5 1397. Qa2 bb7
1398. Qb1 bc8 1399. Rg1 qg7 1400. Rh1 qg8 1401. Bc1 qg7 1402. Ba3 qg6 1403. Bc1 qg4
1404. Qe4 qg6 1405. Qe5 ng8 1406. Qe4 bh6 1407. Qc2 bf8 1408. Qb2 nh6 1409. Qc2 re5
1410. Ba3 rd5 1411. Qc3 qb6 1412. Qc2 rd4 1413. Bb2 rd5 1414. Bd4 bb7 1415. Bb2 bg7
1416. Ba3 bf8 1417. Rg1 rd6 1418. Rh1 qc7 1419. Qd1 qb6 1420. Qc1 qd8 1421. Qd1 ba8
1422. Bc1 bb7 1423. Bb2 ng8 1424. Bc1 re6 1425. [axb7] nh6 1426. Bb2 nf7 1427. Bc1 ng5
1428. Rg1 rb6 1429. Bb2 rf6 1430. Bd4 rf7 1431. Rh1 qc7 1432. Qb1 qc8 1433. Qb2 ne6
1434. Be5 nc7 1435. Rg1 bh6 1436. Qa3 bg7 1437. Qa4 qa8 1438. Bc3 na6 1439. Bb4 qd8
1440. Ba5 bf8 1441. Qa2 rg7 1442. Bc3 rg3 1443. Qa5 rd3 1444. Be5 re3 1445. Qc3 rd3 1446.
Bg7 rd4 1447. Qg3 nc7 1448. Qc3 na6 1449. Be5 rd3 1450. Bg7 re3 1451. Be5 nc7 1452. Qa5
na6 1453. Qc7 rd3 1454. Qa5 rg3 1455. Bc3 rd3 1456. Qa2 rg3 1457. Ba5 rg7 1458. Bc3
rf7 1459. Ba5 bg7 1460. Qa4 bf8 1461. Bb4 bg7 1462. Ba3 qa8 1463. Bb4 rf6 1464. Bc3 rf7
1465. Qb4 nc7 1466. Qa4 bf6 1467. Be5 bg7 1468. Bd6 qc8 1469. Be5 nd5 1470. Qa3 nc7
1471. Qa5 bh6 1472. Qa3 qb8 1473. Qb2 qc8 1474. Qc2 bf8 1475. Qb2 nd5 1476. Rh1 nc7
1477. Bd4 ne6 1478. Be5 ng5 1479. Bd4 qa8 1480. Qb1 qc8 1481. Bf6 qc7 1482. Bd4 rf6
1483. Qd1 rf7 1484. Bc3 qd8 1485. Bd4 rg7 1486. Rg1 rf7 1487. Bb2 rf6 1488. Bd4 rb6 1489.
Bb2 ne4 1490. Bc1 ng5 1491. Rh1 re6 1492. Rg1 nf7 1493. Rh1 rc6 1494. Bb2 re6 1495. Bg7
nh6 1496. Bb2 qb6 1497. Bc1 qd8 1498. Rg1 ng8 1499. Rh1 rd6 1500. [b8=N] qc7 1501.
Qc2 bg7 1502. Qe4 be5 1503. Qa8 qc6 1504. Bb2 rf6 1505. Bc1 bc3 1506. Ba3 bb4 1507. Bb2
nh6 1508. Bc1 qc7 1509. Qf3 qd8 1510. Qg3 ng4 1511. Na6 nh6 1512. Qd3 qb8 1513. Qh3
ba3 1514. Qd3 qc8 1515. Qc3 rd6 1516. Qf3 ng8 1517. Qh3 rh6 1518. Qg4 qc7 1519. Qh3
qe5 1520. Qd3 qe6 1521. Qh3 qe5 1522. Qc3 qc7 1523. Qh3 rh4 1524. Qg4 rh6 1525. Nb8
qc8 1526. Na6 rh5 1527. Qh3 rh6 1528. Bb2 rd6 1529. Bc1 qb8 1530. Qf3 qc8 1531. Qe3
nh6 1532. Qf3 rd5 1533. Qc3 rd6 1534. Qh3 rf6 1535. Qc3 rf7 1536. Qd3 rf6 1537. Qb1 qb8
1538. Qd3 qb6 1539. Qh3 qb8 1540. Qg3 bb4 1541. Qh3 rb6 1542. Qd3 rf6 1543. Qc2 qd8
1544. Qd3 bc3 1545. Qg3 bb4 1546. Ba3 ng4 1547. Bc1 rc6 1548. Nb8 rf6 1549. Qf3 nh6
1550. Qg3 qc7 1551. Qf3 rd6 1552. Qa8 rf6 1553. Bb2 qc6 1554. Bc1 ng8 1555. Bb2 bc3
1556. Ba3 bb4 1557. Bc1 bc3 1558. Rg1 be5 1559. Rh1 qe4 1560. Bb2 qc6 1561. Bd4 rd6
1562. Bb2 bg7 1563. Bc1 be5 1564. Ba3 qc7 1565. Bc1 rd3 1566. Qe4 rd6 1567. Rg1 bg7
1568. Rh1 bh6 1569. Qc2 bg7 1570. Nc6 bf8 1571. Nb8 rb6 1572. Qd1 rd6 1573. Nc6 qd8
1574. Nb8 re6 1575. [b6] bh6 1576. Ba3 re4 1577. Rg1 re3 1578. Bb4 qc8 1579. Na6 rc3
1580. Qc1 bf8 1581. Qb2 bg7 1582. Qa3 re3 1583. Ba5 qb7 1584. Qb2 bd4 1585. Qb1 re4
1586. Bc3 qa8 1587. Qc2 qc8 1588. Nb4 nh6 1589. Qc1 be3 1590. Qa3 bd4 1591. Rh1 be3
1592. Na6 re6 1593. Rg1 qa8 1594. Rh1 rc6 1595. Bg7 rc7 1596. Bc3 rc6 1597. Ba5 re6 1598.
Bc3 ng8 1599. Rg1 nh6 1600. Bd4 qc8 1601. Bc3 ng4 1602. Rh1 nh6 1603. Qa2 re4 1604. Qa3
nf7 1605. Nb4 nh6 1606. Nc6 bd4 1607. Nb4 re6 1608. Rg1 re4 1609. Nc2 be3 1610. Nb4 re6
1611. Qc1 re4 1612. Bb2 bd4 1613. Bc3 qc6 1614. Qc2 qc8 1615. Na6 ng8 1616. Nb4 qa8
1617. Na6 qd5 1618. Qb1 qa8 1619. Bb2 qb7 1620. Bc3 qc8 1621. Ba5 qb7 1622. Nb8 re3
1623. Na6 rg3 1624. Qb2 re3 1625. Qc1 bg7 1626. Qb2 qb8 1627. Qa3 qb7 1628. Qb4 qc8
1629. Qa3 rd3 1630. Bb4 re3 1631. Qb2 rc3 1632. Qa3 bf8 1633. Qb2 re3 1634. Qc1 rc3
1635. Nb8 bh6 1636. Na6 re3 1637. Qd1 rc3 1638. Nb8 re3 1639. Bc3 qd8 1640. Bb4 re5
1641. Ba3 re3 1642. Qb1 re4 1643. Qd1 qc8 1644. Rh1 qd8 1645. Bb2 re6 1646. Ba3 qc8
1647. Bc1 qd8 1648. Nc6 bf8 1649. Nb8 nh6 1650. [b7] rg6 1651. Qc2 bg7 1652. Bb2 qc8
1653. Be5 re6 1654. Qe4 bf8 1655. Qd5 rd6 1656. Bg7 ng4 1657. Bb2 ra6 1658. Bd4 nf6
1659. Qe6 bh6 1660. Rg1 ra4 1661. Qb6 ne4 1662. Qb5 ra5 1663. Qa4 bg5 1664. Qb5 ra2
1665. Qa6 rb2 1666. Qf6 qd8 1667. Qe6 ra2 1668. Bg7 rb2 1669. Bh6 qa5 1670. Bg7 qd8
1671. Na6 ra2 1672. Nb8 ra5 1673. Bd4 ra2 1674. Be5 rb2 1675. Bd4 nc3 1676. Qf6 ne4
1677. Qg7 qc8 1678. Qf6 rc2 1679. Qa6 rb2 1680. Nc6 ra2 1681. Nb8 bh4 1682. Qb5 bg5
1683. Be3 ra5 1684. Bd4 nc3 1685. Qa4 ne4 1686. Na6 bh6 1687. Nb8 qc6 1688. Qb5 qc8
1689. Be3 ra4 1690. Bd4 bg5 1691. Qb6 bh6 1692. Qe6 nf6 1693. Qb6 ra6 1694. Qe6 bf8
1695. Rh1 bh6 1696. Qd5 bf8 1697. Qc6 ng4 1698. Qd5 rg6 1699. Bb2 ra6 1700. Qe5 rd6
1701. Qd5 qc6 1702. Bg7 qc8 1703. Na6 nh6 1704. Nb8 nf7 1705. Be5 nh6 1706. Bc3 re6
1707. Be5 ng8 1708. Qe4 nh6 1709. Bb2 bg7 1710. Be5 ng8 1711. Qc2 nh6 1712. Qc3 rg6
1713. Qc2 rd6 1714. Bb2 rg6 1715. Bf6 qd8 1716. Bb2 qc7 1717. Bc1 qd8 1718. Qe4 bf8
1719. Qc2 qa5 1720. Qd1 qd8 1721. Ba3 rd6 1722. Bb2 re6 1723. Bc1 ng4 1724. Nc6 bh6
1725. [b8=Q] re3 1726. Qb6 rd3 1727. Ne5 rd6 1728. Qb8 rf6 1729. Qb4 rg6 1730. Nc6
re6 1731. Qb7 qc7 1732. Qb5 re5 1733. Qa5 ne3 1734. Qb5 re6 1735. Nb8 re5 1736. Qb4
nd5 1737. Na6 qc8 1738. Qb8 nc7 1739. Qa8 ne6 1740. Qd5 ng7 1741. Nb4 qb7 1742. Na2
qc7 1743. Ba3 qb6 1744. Qd3 qc6 1745. Qh3 ne6 1746. Qd3 ng7 1747. Q1b1 qb6 1748. Qd1
qa5 1749. Qd5 qb6 1750. Qe4 qc7 1751. Qd5 qc8 1752. Bc1 qc7 1753. Qe6 qb7 1754. Qd5
qa8 1755. Nb4 qb7 1756. Rg1 qc8 1757. Rh1 qc6 1758. Na6 qc8 1759. Qd6 ne6 1760. Qd5
bg7 1761. Qa8 bh6 1762. Nb8 nc7 1763. Na6 nb5 1764. Qb8 nc7 1765. Qb6 nd5 1766. Qb8
ne3 1767. Qb4 nd5 1768. Nb8 qc7 1769. Na6 ne3 1770. Nb8 bf8 1771. Qb5 bh6 1772. Qa5
re6 1773. Qb5 rd6 1774. Nc6 re6 1775. Bb2 re5 1776. Bc1 bg7 1777. Qa5 bh6 1778. Qa3 ng4
1779. Qa5 rd5 1780. Qb5 re5 1781. Qb7 re6 1782. Qb5 qd8 1783. Qb7 ne3 1784. Qb4 ng4
1785. Qb5 rg6 1786. Qb4 rg8 1787. Ne5 rg6 1788. Qa5 rf6 1789. Qb4 rd6 1790. Qb8 rf6
1791. Qb6 rd6 1792. Qc6 rd3 1793. Qb6 qc8 1794. Nc6 qd8 1795. Ba3 re3 1796. Bc1 rf3
1797. Qb8 re3 1798. Rg1 re6 1799. Rh1 kf8 1800. [b4] ke8 1801. Qc8 bg7 1802. Qb8 re4
1803. Qb7 rf8 1804. Na5 bd4 1805. Qd5 re6 1806. Qa4 rh6 1807. Qb5 rf7 1808. Rg1 rd6

1809. Qe6 qa8 1810. Bb2 kf8 1811. Bc3 nf6 1812. Qa6 be3 1813. Nc6 rd5 1814. Qe5 rg7
1815. Bb2 rg5 1816. Qb8+ kg7 1817. Qa5 re5 1818. Qg8+ kh6 1819. Qe6 rg6 1820. Qd5 rg4
1821. Nd8 rg6 1822. Nc6 kg5 1823. Qe6 kh6 1824. Nb8 rg5 1825. Nc6 kg7 1826. Qg8+ kh6
1827. Qb8 kg7 1828. Qa3 rd5 1829. Qa5 rd3 1830. Qa6 rd5 1831. Qbb5 kf8 1832. Qb8+ kf7
1833. Qe5 kf8 1834. Bc3 rg7 1835. Bb2 rf7 1836. Bc3 nh5 1837. Qe6 nf6 1838. Qe4 rd6 1839.
Qe6 bd4 1840. Na5 be3 1841. Qb5 bd4 1842. Qc6 ng4 1843. Qb5 rf6 1844. Bb2 rf7 1845. Nc6
ke8 1846. Na5 bc3 1847. Bc1 bd4 1848. Nb7 qd8 1849. Na5 qb6 1850. Qd5 qd8 1851. Ba3
rh6 1852. Bc1 kf8 1853. Rh1 ke8 1854. Nb7 rf8 1855. Na5 rb6 1856. Qa4 rh6 1857. Qe5 re6
1858. Qd5 ne3 1859. Qd1 ng4 1860. Qc6 re4 1861. Qd5 nh6 1862. Qb7 ng4 1863. Bb2 bg7
1864. Bc1 rf7 1865. Nc6 rf8 1866. Rg1 rh8 1867. Rh1 re5 1868. Qb8 re4 1869. Qb3 re6 1870.
Qd1 rg8 1871. Qc8 rh8 1872. Qa8 bh6 1873. Qc8 kf8 1874. Qb8 ke8 1875. [b5] re5 1876.
Qb6 nf6 1877. Qa5 rg8 1878. Qda4 bg7 1879. Qa2 bh6 1880. Qb3 rg4 1881. Qb2 ng8 1882.
Qab4 qc7 1883. Qd4 re4 1884. Qf6 re3 1885. Bb2 rd3 1886. Bc3 rdg3 1887. Be5 qd8 1888.
Bb2 qa8 1889. Qh8 rf3 1890. Bf6 rb3 1891. Bd4 rgg3 1892. Qg7 qb8 1893. Nd8 rbd3 1894.
Rg1 rb3 1895. Rh1 rg6 1896. Nc6 rgg3 1897. Na5 qa8 1898. Nc6 rbd3 1899. Qh8 rb3 1900.
Ne5 rg4 1901. Nc6 rh4 1902. Bf6 rg4 1903. Qa4 rf3 1904. Qb4 rd3 1905. Bb2 rf3 1906. Bd4
rfg3 1907. Bb2 qd8 1908. Qf6 qa8 1909. Be5 qd8 1910. Qg5 qc7 1911. Qf6 rh4 1912. Bc3
rhg4 1913. Qb2 rd3 1914. Qb4 rg7 1915. Bb2 rg4 1916. Ne5 re3 1917. Nc6 rh4 1918. Bc1
rg4 1919. Qfb2 re4 1920. Qf6 rg7 1921. Qd4 rg4 1922. Qdc3 re5 1923. Qd4 bg7 1924. Qdb2
bh6 1925. Na5 qd8 1926. Nc6 re6 1927. Qa5 re5 1928. Qa6 nf6 1929. Qa5 re6 1930. Qb3 re5
1931. Qac3 rg8 1932. Qa5 rg5 1933. Qba2 rg8 1934. Qb2 bg7 1935. Qba2 qb6 1936. Q2a4
qd8 1937. Nb4 bh6 1938. Nc6 re6 1939. Qd1 re5 1940. Qc2 rh8 1941. Qd1 nh5 1942. Qb6
nf6 1943. Na5 ng4 1944. Nc6 kf7 1945. Qb8 ke8 1946. Bb2 re4 1947. Ba3 re6 1948. Bc1
kf8 1949. Qe5 ke8 1950. [b6] rg8 1951. Qa4 qc7 1952. Nd4 bf8 1953. Qh8 reg6 1954. Nb5
qd6 1955. Qc2 qd5 1956. Qg7 qc6 1957. Bb2 ne5 1958. Qh6 rg3 1959. Qd1 r3g6 1960. Qh3
rh8 1961. Qb1 bh6 1962. Rg1 kf7 1963. Qf3 qd6 1964. Qh5 qd5 1965. Qc2 rd8 1966. Qa4
ke8 1967. Qc2 qe6 1968. Nd4 bg7 1969. Qf3 qg8 1970. Qfc3 qe6 1971. Qf3 rb8 1972. Qh5
rd8 1973. Nc6 bh6 1974. Nd4 rb8 1975. Nb5 rd8 1976. Qf3 qd5 1977. Qh5 qf7 1978. Qa4
qd5 1979. Qa6 kf7 1980. Qa4 qc6 1981. Qc2 qd5 1982. Qc1 rh8 1983. Qc2 ra8 1984. Qb1
rh8 1985. Ba3 qd6 1986. Bb2 kf6 1987. Qf3 kf7 1988. Nc3 qc6 1989. Nb5 rhg8 1990. Qh3
rh8 1991. Qc3 ke8 1992. Qh3 qe6 1993. Rh1 qc6 1994. Qc2 bf8 1995. Qb1 rhg8 1996. Qd1
rh8 1997. Qh6 rhg8 1998. Qh4 rg3 1999. Qh6 rd3 2000. Qc2 rdg3 2001. Rg1 r3g6 2002. Rh1
qf6 2003. Qg7 qc6 2004. Qa4 ng4 2005. Qc2 qc8 2006. Bc1 qc6 2007. Qh8 qd5 2008. Qg7
qd6 2009. Qh8 bh6 2010. Qa4 bf8 2011. Qd4 qc7 2012. Qh8 kf7 2013. Nd4 ke8 2014. Qb5
re6 2015. Qa4 rc6 2016. Qe5 re6 2017. Nf3 bh6 2018. Nd4 qc8 2019. Nc6 qc7 2020. Bb2
qd8 2021. Bc1 bg5 2022. Qd1 bh6 2023. Nb8 rh8 2024. Nc6 kf8 2025. [b7] nf6 2026. Qd6
ne4 2027. Bb2 qa5 2028. Bc1 rg6 2029. Qf6+ kg8 2030. Qg5 bf8 2031. Qb3 nf6 2032. Qh3
rg7 2033. Qhg3 qc3 2034. Qh3 qe3 2035. Qhg3 qf3 2036. Na5 nh5 2037. Ba3 rg6 2038. Qf6
qe3 2039. Qg4 qe6 2040. Qh3 qb6 2041. Qe3 qa6 2042. Qg5 qc6 2043. Bb4 qd5 2044. Qf6
rg5 2045. Nb3 qe4 2046. Qfc3 qd5 2047. Qf6 bh6 2048. Na5 bf8 2049. Nc6 rg6 2050. Na5
ng3 2051. Qg5 nh5 2052. Qb3 qc6 2053. Qe3 qa6 2054. Ba3 qc6 2055. Qf6 qa6 2056. Qh4
qb6 2057. Qf6 rg5 2058. Qh3 rg6 2059. Bc1 qe6 2060. Ba3 rg5 2061. Qg4 rg6 2062. Nc6
qe3 2063. Na5 bg7 2064. Qg3 bf8 2065. Qd4 qf3 2066. Qf6 qc6 2067. Qfg5 qf3 2068. Rg1
rg7 2069. Rh1 qd3 2070. Bc1 qf3 2071. Q5h4 nf6 2072. Qhg5 qd5 2073. Nc6 qf3 2074. Bb2
qe3 2075. Bc1 qd3 2076. Qh3 qe3 2077. Qf3 qc3 2078. Qh3 ne4 2079. Qhg3 nf6 2080. Nb8
qa5 2081. Nc6 qa3 2082. Qh3 qa5 2083. Qgh4 rg6 2084. Qg5 ng4 2085. Qb3 nf6 2086. Qb2
ne4 2087. Qb3 qa3 2088. Qd1 qa5 2089. Nd8 bh6 2090. Nc6 kf8 2091. Qf6+ kg8 2092. Qd6
kf8 2093. Bb2 re6 2094. Bc1 qd8 2095. Bb2 nf6 2096. Bc1 nh5 2097. Qe5 nf6 2098. Ba3
ng4 2099. Bc1 ke8 2100. [b8=N] rg8 2101. Ba3 qb6 2102. Qd6 rh8 2103. Qe5 rf8 2104.
Qe4 qc7 2105. Qe3 qd6 2106. Nb4 qc7 2107. Nc2 rd6 2108. Qg3 rd4 2109. Qf3 qc8 2110.
Qd3 nf6 2111. Qb1 ng8 2112. Qc1 kf7 2113. Qf3 ke8 2114. Na6 rf7 2115. Ne3 bg7 2116. Qg4
qc6 2117. Nd5 qf6 2118. Rg1 nh6 2119. Bb4 kd8 2120. Qh3 ke8 2121. Qg4 ng8 2122. Ba3
nh6 2123. Rh1 ng8 2124. Qh5 qc6 2125. Qg4 bh8 2126. Ne3 bg7 2127. Bb2 qc8 2128. Ba3
bh8 2129. Qf3 bg7 2130. Qd5 bh6 2131. Qf3 qc7 2132. Nc2 qc8 2133. Qe3 rf8 2134. Qf3 rd3
2135. Nb8 rd4 2136. Nb4 kf7 2137. Nc2 bg7 2138. Qd3 bh6 2139. Qe4 ke8 2140. Qd3 qa6
2141. Qb1 qc8 2142. Qb7 nf6 2143. Qb1 rd5 2144. Qd1 rd4 2145. Qb3 ng4 2146. Qd3 rf7
2147. Qf3 rf8 2148. Qc3 qc7 2149. Qf3 ne5 2150. Qg3 ng4 2151. Nb4 rd6 2152. Nc2 qc6
2153. Qe3 qc7 2154. Nb4 re6 2155. Nc2 qd6 2156. Nb4 rg8 2157. N4c6 rf8 2158. Bb2 qc7
2159. Ba3 rf7 2160. Qe4 rf8 2161. Rg1 qb6 2162. Rh1 qb5 2163. Qe5 qb6 2164. Qc7 rh8
2165. Qe5 bf8 2166. Qd6 bh6 2167. Qb3 rg8 2168. Qd1 qb7 2169. Qe5 qb6 2170. Qa4 qd8
2171. Qd1 rg5 2172. Bc1 rg8 2173. Qd5 rh8 2174. Qe5 bg5 2175. [h3] kf7 2176. Na6 qc7
2177. Rg1 re8 2178. Nab8 rd6 2179. Qh8 rd3 2180. Ba3 rd6 2181. Nb4 rg8 2182. N4a6 rc6
2183. Qe5 rg7 2184. Qc3 qa5 2185. Qdc2 nh2 2186. Nb4 rc8 2187. Q2d3 ng4 2188. Qdd4 ne3
2189. N4a6 rf8 2190. Qe5 kg6 2191. Qb2 kh6 2192. Qb3 nd1 2193. Qb7 qc7 2194. Qbe4 rfg8
2195. Qb7 rf8 2196. Nb4 qa5 2197. N4a6 qd8 2198. Qb3 qa5 2199. Bc1 ne3 2200. Ba3 qb6
2201. Qbb2 qa5 2202. Qb5 kg6 2203. Qbb2 qb5 2204. Qbc3 qa5 2205. Qb3 kf7 2206. Qbc3
rgg8 2207. Qed4 rg7 2208. Qb3 rc8 2209. Qbc3 kf8 2210. Nb4 kf7 2211. Qdd3 ng4 2212.
Qdd4 nh2 2213. Qdd3 bh4 2214. Qdc2 bg5 2215. Qe5 rc6 2216. Qec3 bf6 2217. N4a6 bg5
2218. Qf3 ng4 2219. Qfc3 bh4 2220. Qd1 bg5 2221. Qe3 qc7 2222. Qc3 ne3 2223. Qe5 ng4
2224. Qe4 rg8 2225. Qe5 qb6 2226. Qh8 qc7 2227. Qc3 rd6 2228. Qh8 qb7 2229. Nb4 qc7
2230. N8c6 re8 2231. Nb8 rh6 2232. N4c6 rd6 2233. Qc1 rd3 2234. Qd1 nh6 2235. Bc1 ng4
2236. Qc3 rd6 2237. Qh8 nh6 2238. Qe5 ng4 2239. Qc2 re6 2240. Qd1 rg8 2241. Na6 re8
2242. Qc3 rh8 2243. Qe5 ne3 2244. Rh1 ng4 2245. Qb2 qd8 2246. Qe5 rf8 2247. Nab8 rh8
2248. Qc2 ke8 2249. Qd1 bh4 2250. [hxg4] bg3 2251. Rh2 rg8 2252. Rh4 qb6 2253. Qg7 re3
2254. Ne5 rf3 2255. Qa4 rb3 2256. Qg6+ kd8 2257. Qe6 rg5 2258. Qh6 qd6 2259. Qa6 rb6
2260. Qf8+ kc7 2261. Qf6 qd4 2262. Qg6 rb7 2263. Qaf6 rb5 2264. Rh3 ra5 2265. Rh2 kc8
2266. Rh3 ra6 2267. Qd6 qe4 2268. Qde6 qe3 2269. Qgg8+ kb7 2270. Qg7 kc7 2271. Nf7 ra4
2272. Ne5 ra6 2273. Ba3 kb7 2274. Bc1 kc8 2275. Qgg8+ kb7 2276. Qgg6 kc8 2277. Qef6
qe4 2278. Qe6 ra2 2279. Qd6 ra6 2280. Qge6 qd4 2281. Qg6 rc6 2282. Qdf6 ra6 2283. Qfg7
ra5 2284. Q7f6 rb5 2285. Rh2 ra5 2286. Ba3 kc7 2287. Bc1 qd6 2288. Rh3 qd4 2289. Rh1
rb5 2290. Rh3 bh2 2291. Rh4 bg3 2292. Qff7 rb7 2293. Qff6 rb4 2294. Qa6 rb7 2295. Bb2
rb6 2296. Bc1 re6 2297. Qf6 rb6 2298. Qa4 qd6 2299. Qa6 kd8 2300. Qf8+ kc7 2301. Qh6
kd8 2302. Qg7 rb3 2303. Qh6 ke8 2304. Qa4 kd8 2305. Rh1 qb6 2306. Rh4 qc7 2307. Qe6
qb6 2308. Qb4 rg8 2309. Qa4 ke8 2310. Qg6+ kd8 2311. Qg7 ke8 2312. Rh5 rf3 2313. Rh4
re3 2314. Qd1 rf3 2315. Nec6 re3 2316. Na5 re6 2317. Nac6 rf8 2318. Qe5 rg8 2319. Qc3
qd8 2320. Qe5 kf7 2321. Rh2 ke8 2322. Bb2 rh8 2323. Bc1 bh4 2324. Rh1 bg3 2325. [fxg3]
rg6 2326. Qg7 qb6 2327. Qd4 qa5 2328. Rh4 kf7 2329. Qd6 qc3 2330. Qd3 rf6 2331. Qf3 rd6
2332. Nd8+ kg8 2333. Qa8 kg7 2334. Qc6 qb4 2335. Qca4 qa3 2336. Nb7 rc6 2337. Bb2+
kf7 2338. Qb4 rcc8 2339. Qda4 qe3 2340. Qab3 ke8 2341. Q3a3 rd8 2342. Qa2 rf8 2343.
Qab3 rh8 2344. Rh2 qg1 2345. Qb6 qe3 2346. Q6b4 qe6 2347. Rh4 qe3 2348. Bc3 rf8 2349.
Bb2 qe4 2350. Qa2 qe3 2351. Na5 rh8 2352. Nb7 qd4 2353. Qaa3 qe3 2354. Qaa4 rc8 2355.
Qaa3 qe6 2356. Qab3 qe3 2357. Q4c3 kf7 2358. Qcb4 rhd8 2359. Q3a4 rh8 2360. Na5 qa3
2361. Nb7 kf8 2362. Qd1 kf7 2363. Qc2 rc6 2364. Qd1 kg8 2365. Qba4 kf7 2366. Bc1 kg7
2367. Rh6 rd6 2368. Rh4 rh6 2369. Nd8 rd6 2370. Qc6 qb4 2371. Qca4 qc3 2372. Qc6 rh6
2373. Qa8 rd6 2374. Nb7 kg8 2375. Nd8 qa3 2376. Qf3 qc3 2377. Nb7 kf7 2378. Nd8+ kg6
2379. Ndc6 kf7 2380. Nb4 rf6 2381. N4c6 qb3 2382. Qd3 qc3 2383. Rh5 rg6 2384. Rh4 rd8
2385. Qd6 rh8 2386. Qc2 qa5 2387. Qd1 rf6 2388. Qd4 rg6 2389. Na6 ke8 2390. Nab8 qd8
2391. Rh1 qa5 2392. Nb4 qb6 2393. N4c6 qb5 2394. Qg7 qb6 2395. Qf6 qd8 2396. Qg7 qa5
2397. Qe5 qd8 2398. Bb2 re6 2399. Bc1 qc8 2400. [g5] rh6 2401. Ba3 rh3 2402. Qf6 rg8
2403. Nd4 rf8 2404. Qb3 rh2 2405. Qa2 rg8 2406. Qb3 qb7 2407. Bb4 qc6 2408. Na6 rh8
2409. Qf3 rg8 2410. Nb5 rh8 2411. Qh6 rh5 2412. Qd5 qc7 2413. Qg7 qc8 2414. Na3 rh2
2415. Qa8 qd8 2416. Qg6+ kf8 2417. Qgc6 qe8 2418. Qc7 rh6 2419. Nb8 qc8 2420. Qac6 re6
2421. Qf3 rh6 2422. Qfc6 qd8 2423. Qa8 qc8 2424. Qcc6 qe8 2425. Qc7 rh4 2426. Na6 rh6
2427. Qcd8 rh2 2428. Qc7 rh4 2429. Qcc6 rh2 2430. Qb8 qd8 2431. Qba8 ke8 2432. Qg6+
kf8 2433. Qg7+ ke8 2434. Nc2 qc8 2435. Na3 rf8 2436. Qd5 rh8 2437. Qh6 rh5 2438. Qg7
qc6 2439. Nb5 qc8 2440. Qb2 qc7 2441. Qg7 qa5 2442. Qh6 qc7 2443. Qhe6 qc6 2444. Qh6
qd6 2445. Qf3 qc6 2446. Qg7 rh2 2447. Qh6 rg8 2448. Qf6 rh8 2449. Nd4 rg8 2450. Qb3 rh8
2451. Nb8 rg8 2452. Qc3 qb7 2453. Qb3 rh8 2454. Ba3 rg8 2455. Qc3 qc8 2456. Qb3 qc6
2457. Qa2 qc8 2458. Nb5 rf8 2459. Nd4 qc6 2460. Qb3 qc8 2461. Qh8 rh3 2462. Qf6 qa6
2463. Qd1 qc8 2464. Qh8 rg8 2465. Qf6 qa6 2466. Ndc6 qc8 2467. Bb2 rh8 2468. Ba3 rh6
2469. Qe5 rh3 2470. Bc1 rh6 2471. Qc2 re6 2472. Qd1 qa6 2473. Nb4 kd8 2474. Qe3 kc7
2475. [g6] kd8 2476. Qd3 qa4 2477. Qd4 rb6 2478. N8c6+ kc7 2479. Nd5+ kc8 2480. Ba3
rb2 2481. Qg7 qb3 2482. Na5 re8 2483. Nb7 qb6 2484. Qe5 rf8 2485. Nc3 rf7 2486. Nb1
qa6 2487. Nd8 rg7 2488. Qf6 rf7 2489. Nb7 rb6 2490. Qc1 rf8 2491. Rh5 rb4 2492. Rh6 qd6
2493. Nd8 qd5 2494. Qcc3 re8 2495. Qc1 rf8 2496. Qd4 qd6 2497. Qf6 rb5 2498. Nb7 rb4
2499. Rh3 qa6 2500. Rh6 qe6 2501. Rh5 qa6 2502. Qh4 rb6 2503. Qf6 qb5 2504. Rh1 qa6
2505. Bb2 rf7 2506. Ba3 qa4 2507. Qd1 qa6 2508. Qd6 rb2 2509. Qf6 kc7 2510. Nd8 kc8
2511. Qh4 rg7 2512. Qf6 qe6 2513. Qe5 qa6 2514. Qd4 rf7 2515. Qe5 rf6 2516. Nb7 rf7
2517. Qd4 qb6 2518. Qe5 qf6 2519. Nc3 qb6 2520. Nb5 rf8 2521. Nc3 rb1 2522. Nd5 rb2
2523. Rh5 re8 2524. Rh1 qb3 2525. Qg7 qb6 2526. Na5 qb3 2527. Rg1 rh8 2528. Rh1 qb5
2529. Nc6 qb3 2530. Nf6 qa4 2531. Nd5 kb7 2532. Qd4 kc8 2533. Nc7 rb6 2534. Nd5 rb5
2535. Bc1 rb6 2536. Nc3 kc7 2537. Nd5+ kb7 2538. Ndb4 kc7 2539. Na5 kd8 2540. Nac6+
kc8 2541. Nb8 kd8 2542. Qc3 re6 2543. Qd4 kc8 2544. Qd3 kd8 2545. Qc3 qa6 2546. Qd3
qd6 2547. Qe3 qa6 2548. Rg1 kc7 2549. Rh1 kb6 2550. [g7] qa5 2551. Qf2 re4 2552. Rh6+

26

kc7 2553. Ba3 rf8 2554. Rh3 re8 2555. N8a6+ kb6 2556. Na2 re3 2557. Rh5 rc8 2558. Nc1
rf3 2559. Qg1 ra8 2560. Nb8 rf2 2561. Rh1 kb5 2562. Rh4 kb6 2563. Nd3 qa6 2564. Rh6+
kb5 2565. Rh2 qa5 2566. Nb2 qa6 2567. Qh1 qb7 2568. Rh5 rf3 2569. Qb3+ ka5 2570. Qa2
qc7 2571. Na4 qb7 2572. Nb2 kb5 2573. Qb3+ ka5 2574. Qd1 kb5 2575. Na6 rf2 2576. Nb8
qe4 2577. Rh2 qb7 2578. Nd3 qa6 2579. Nb2 qc8 2580. Qg1 qa6 2581. Qc2 qa5 2582. Qd1
qa4 2583. Nd3 qa5 2584. Rh4 qa6 2585. Rh2 kb6 2586. Rh6+ kb5 2587. Rh4 kb6 2588. Bc1
qa5 2589. Ba3 qb5 2590. Nc1 qa5 2591. Qh2 kb5 2592. Qg1 qa6 2593. Rh1 qa5 2594. Na2
kb6 2595. Nc1 qb4 2596. Rh5 qa5 2597. Qh1 rf3 2598. Qg1 rb3 2599. Na6 rf3 2600. Nb3
rc8 2601. Nc1 rd8 2602. Qf2 rc8 2603. Rh4 re3 2604. Rh5 re8 2605. Na2 rc8 2606. Rh3 re8
2607. Bc1 re4 2608. Ba3 kb7 2609. N2b4 kb6 2610. Nb8 kc7 2611. Qd4 rf8 2612. Qf2 rc8
2613. Rh6 rf8 2614. Nd3 rh8 2615. Nb4 rd8 2616. Bc1 rh8 2617. N4c6 kb6 2618. Nb4+ kb5
2619. Rh1 kb6 2620. Ba3 re6 2621. Bc1 rd8 2622. Qe3 rh8 2623. Rh2 qa6 2624. Rh1 rf6
2625. [g8=Q] qa5 2626. Qge6+ kc7 2627. Qa3 kb7 2628. N8a6 qb6 2629. Qd6 rc8 2630.
Qe3 qd8 2631. Rh4 re6 2632. Nc7 rg6 2633. Qee5 rg5 2634. Na2 rg8 2635. Rh5 rg7 2636. Na8
qc7 2637. Nb4 qc6 2638. Qh6 qf6 2639. Nd3 rb8 2640. Qg6 qb6 2641. Qg5 qc6 2642. Nc7
rd8 2643. Qgf6 qf3 2644. Qfd6 rc8 2645. Ne8 qe3 2646. Nc7 qf3 2647. Rh4 rd8 2648. Rh5
rg6 2649. Qdf6 rg7 2650. Qc2 qc6 2651. Qd1 rg6 2652. Qg5 rg7 2653. Na8 rb8 2654. Nc7
qb6 2655. Na8 qf6 2656. Qg6 qb6 2657. Qh6 qf6 2658. Nc7 rc8 2659. Na8 qf8 2660. Nb4 qf6
2661. Rh1 qc6 2662. Rh5 qa6 2663. Qhd6 qc6 2664. Qg6 qc7 2665. Qgd6 rgg8 2666. Na2 rg7
2667. Qef6 qd8 2668. Qfe5 rg8 2669. Nc7 rg7 2670. Rh4 rg8 2671. Ne8 rg5 2672. Nc7 qg8
2673. Nb4 qd8 2674. Qg7 rg6 2675. Qge5 qh8 2676. Qe3 qd8 2677. Rh1 re6 2678. Rh4 rb8
2679. Nca6 rc8 2680. Nd3 rf6 2681. Ndb4 qf8 2682. Rh1 qd8 2683. Qed3 qb6 2684. Qe3 qa5
2685. Qa3 qb6 2686. Qdb3 rh8 2687. Qd1 qc6 2688. Qe6 qb6 2689. Qee3 qa5 2690. Qe6 rf7
2691. Nb8 rf6 2692. Qg8 kc7 2693. Qe6 qb6 2694. Qae3 qa5 2695. Q6e4 kb6 2696. Qe6+ kb7
2697. Qg8 kb6 2698. Qg1 qa6 2699. Qe3 re6 2700. [g4] kb5 2701. Qe4 qa3 2702. N8c6 qf3
2703. Nd4+ kb6 2704. Ndc2 rd6 2705. Qed5 qe4 2706. Nd3 qe5 2707. Qga8 kb5 2708. Rh6
qg7 2709. Re6 rf8 2710. Na3+ ka5 2711. Nb1+ kb6 2712. Qd4 qh6 2713. Bb2 qg5 2714. Qa4
qh5 2715. Rh6 qh2 2716. Qg1 rg8 2717. Qd5 rh8 2718. Qb3+ kc7 2719. Qd1 rf6 2720. Nc3
rg8 2721. Qe4 rh8 2722. Qd5 qh1 2723. Nb1 qh2 2724. Rg6 rd6 2725. Rh6 kb6 2726. Qb3+
kc7 2727. Qa4 kb6 2728. Qa2 rg8 2729. Qa4 qh1 2730. Qa8 qh2 2731. Qd4 rf8 2732. Qg1
qh5 2733. Qd4 rf7 2734. Re6 rf8 2735. Bc1 qg5 2736. Bb2 rh8 2737. Qd1 rf8 2738. Na3 qh6
2739. Nb1 qg6 2740. Bc1 qh6 2741. Re5 qg7 2742. Re6 re8 2743. Qdd5 rf8 2744. Na3 ka5
2745. Nb1+ kb5 2746. Na3+ ka6 2747. Nc2+ kb5 2748. Qe8 rh8 2749. Qea8 rd8 2750. Rh6
rh8 2751. Rh2 qe5 2752. Rh6 re6 2753. Rh1 rd6 2754. Qf8 kb6 2755. Qfa8 qb2 2756. Qag8
qe5 2757. Bb2 qe4 2758. Bc1 re6 2759. Ndb4 rd6 2760. Na6 qf3 2761. Nab4 rg6 2762. Qe4
rd6 2763. Ba3 re6 2764. Bc1 kb5 2765. Nd4+ kb6 2766. Ndc6 kb5 2767. Qe8 qa3 2768. Qg8
qh3 2769. Nb8 qa3 2770. Nc2 qa6 2771. Nb4 qb6 2772. Qe3 qa6 2773. Qg6 kb6 2774. Qg8
rf6 2775. [g5] rh6 2776. Qh3 rh4 2777. Qf3 kc7 2778. Qfd5 qc8 2779. Qde6 rg4 2780. Qgg6
qd8 2781. Rh3 kc8 2782. Rg3 qa5 2783. Qd6 qa6 2784. Re3 qa4 2785. Qf7 rg3 2786. Re4 re3
2787. Ba3 rh3 2788. Qa6+ kc7 2789. Qa5+ kd6 2790. Nd5 rd3 2791. Bb2 qc2 2792. Qc3 qb3
2793. Qg8 qb6 2794. Ba3 qc7 2795. Bb2 qb6 2796. Qa5 qb3 2797. Qc3 qa2 2798. Qf7 qb3
2799. Qcg7 qc2 2800. Qc3 re3 2801. Qa5 rd3 2802. Nb4 qa4 2803. Nd5 rf3 2804. Ba3 rd3
2805. Bc1 rh3 2806. Ba3 re3 2807. Nb4 rh3 2808. Qb5 kc7 2809. Qa5+ kc8 2810. Qa6+ kc7
2811. Qd6+ kc8 2812. Qdg6 re3 2813. Qd6 qc6 2814. Bc1 qa4 2815. Rd4 rg3 2816. Re4 qa3
2817. Re3 qa4 2818. Nc2 rg4 2819. Nb4 rd8 2820. Qfg6 rh8 2821. Ba3 qa6 2822. Bc1 rh4
2823. Rg3 rg4 2824. Qg7 qa5 2825. Qgg6 qb6 2826. Qde6 qa5 2827. Qe5 qd8 2828. Qee6 qf8
2829. Rh3 qd8 2830. Qef6 kc7 2831. Qe6 re8 2832. Rh1 rh8 2833. Qe8 qc8 2834. Q8g6 qb7
2835. Qgg8 qc8 2836. Qb3 rh4 2837. Qd1 rh2 2838. Qd5 rh4 2839. Rg1 qa6 2840. Rh1 qb5
2841. Qf3 qa6 2842. N8c6 kb6 2843. Nb8 qc8 2844. Qh3 qa6 2845. Nc2 rh6 2846. Nb4 qa2
2847. Qe3 qa6 2848. Qd5 rf6 2849. Qg8 re6 2850. [g6] re4 2851. Qc3 qa5 2852. Qf8 rd4
2853. Qdc2 kc7 2854. Q2d3 rd6 2855. Qdd4 rb6 2856. Qg8 rc6 2857. Qe5+ rd6 2858. Rh3
qa4 2859. Bb2 qa2 2860. Qf8 qb1+ 2861. Qc1 qd3 2862. Rg3 qb3 2863. Qef6 ra6 2864. Qcc3
rd6 2865. Rg5 qa3 2866. Qfd4 rf6 2867. Qg3 qd3 2868. Qh3 qc2 2869. Qf2 qe4 2870. Qd4
qc2 2871. Rh5 qd3 2872. Rg5 rg8 2873. Qg3 rh8 2874. Bc1 qa3 2875. Bb2 qa5 2876. Qgc3
qa3 2877. Na2 rd6 2878. Nb4 qa5 2879. Qdf6 qa3 2880. Qcd4 qb3 2881. Qc3 qa4 2882. Rg3
qb3 2883. Q8g7 ra6 2884. Qgf8 ra4 2885. Qc1 ra6 2886. Rg5 rd6 2887. Rg3 re6 2888. Qe5+
rd6 2889. Qd4 qd3 2890. Qe5 qc3 2891. Rh3 qd3 2892. Qcc3 qb1+ 2893. Qc1 qa2 2894.
Qcc3 qb3 2895. Qg8 qa2 2896. Qf7 qa4 2897. Qg8 kb7 2898. Bc1 kc7 2899. Qc2 qa5 2900.
Qcc3 kb6 2901. Rh1 kc7 2902. Qee6 rc6 2903. Qee5+ kb6 2904. Qed4 kc7 2905. Qdd5 rb6
2906. Qdd4 rf6 2907. Qf8 rb6 2908. Qf7 rd6 2909. Qf8 kb6 2910. Qdd3 kc7 2911. Qb1 rd4
2912. Qbd3 kd6 2913. Qdc2 kc7 2914. Q2b3 kb6 2915. Qbc2 qa6 2916. Qd1 qa5 2917. Qa3
re4 2918. Qc3 qa3 2919. Qg8 qa5 2920. Qdc2 qa6 2921. Qd1 qa2 2922. Qe3 qa6 2923. Rh5
re6 2924. Rh1 rf6 2925. [g7] qa4 2926. Nc2 kb5 2927. Qd5 ra6 2928. Qg1 qa5 2929. Qgd4
kb6 2930. Rh6+ kb5 2931. Qe3 qc7 2932. Qd6 ra4 2933. Na3+ kb4 2934. Nb1 qb6 2935.
Qa3+ ka5 2936. Qc3+ qb4 2937. Qh3 ra2 2938. Qg3 qb3 2939. Qc2 qa4 2940. Ba3 rb2 2941.
Qh3 qc6 2942. Nc3 rb6 2943. Qd1 qa4 2944. Qb1 qb5 2945. Qd1 qa4 2946. Qh2 qc6 2947.
Qh3 rf8 2948. Qc2 rh8 2949. Qg3 rb2 2950. Qh3 qc7 2951. Nb1 qc6 2952. Qf6 qa4 2953.
Qd6 rf8 2954. Qg3 rh8 2955. Qe3 ra2 2956. Qg3 qc6 2957. Bc1 qa4 2958. Qdg6 qb3 2959.
Qd6 qd3 2960. Qd1 qb3 2961. Na6 qb4 2962. Nb8 qb2 2963. Qh3 qb4 2964. Qe3 ra4 2965.
Qh3 qb6 2966. Qc3+ qb4 2967. Qa3 qb6 2968. Qh3 kb4 2969. Qa3+ kb5 2970. Qe3 kb4
2971. Qg6 qc7 2972. Qd6 kb5 2973. Na3+ kb4 2974. Nc2+ kb5 2975. Qf6 ra6 2976. Qd6 ra2
2977. Qd5 ra6 2978. Rh5 qa5 2979. Rh6 rf8 2980. Qed4 rh8 2981. Nc6 kb6 2982. Nb8+ kc7
2983. Rh1 kb6 2984. Qf6+ kb5 2985. Qfd4 qb6 2986. Qg1 qa5 2987. Nc6 qa4 2988. Nb8 qa2
2989. Qe3 qa4 2990. Qdd4 rf6 2991. Qd5 rc6 2992. Qg8 rf6 2993. Na3+ kb6 2994. Nc2 qb3
2995. Nb4 qa4 2996. Qg1 qa3 2997. Qf2 qa6 2998. Qe3 re6 2999. Qf8 rf6 3000. [g8=N] rc6
3001. Qa4 rd6 3002. Qa2 qb5 3003. Qc3 rd3 3004. N4c6 re3 3005. Rh6 qb3 3006. Qcf6 qb5
3007. Qb3 rd3 3008. Rh5 rd5 3009. Qe6 rd6 3010. Nd8 rc6 3011. Qb1 qb3 3012. Qee4 qb4
3013. Qed3 kc7 3014. Na6+ kc8 3015. Qh6 rd6 3016. Qdb3 rf6 3017. Qd1 rc6 3018. Nf7 rc7
3019. Qbc2 qa3 3020. Qb1 qb4 3021. Qg6 rc6 3022. Qh6 rd6 3023. Nd8 rc6 3024. Qd3 rf6
3025. Qb1 rg6 3026. Qdb3 rf6 3027. Bb2 rd6 3028. Bc1 rb6 3029. Q3d3 rd6 3030. Qa2 rc6
3031. Qab1 qb5 3032. Qf8 qb4 3033. Nb8 kc7 3034. Rh4 kb6 3035. Rh5 qb3 3036. Qe4 qb4
3037. Qe6 qb3 3038. Rh3 qb5 3039. Rh5 qb4 3040. Qb3 qb5 3041. Qeh6 rd6 3042. Qe6 kc7
3043. Ndc6 kb6 3044. Qff6 rd5 3045. Qf8 qb4 3046. Qef6 qb5 3047. Q8h6 rd3 3048. Qhf8 rf3
3049. Rh6 rd3 3050. Qe5 re3 3051. Qef6 rg3 3052. Qa2 re3 3053. Qa3 qb3 3054. Qa2 kb5
3055. Qc3 kb6 3056. Qfg7 qb5 3057. Qf8 re6 3058. Rh1 re3 3059. Nd8 rd3 3060. Ndc6 rd5
3061. Nb4 rd3 3062. Rh6+ rd6 3063. Rh1 rc6 3064. Qe3 rd6 3065. Qab3 qa6 3066. Qa2 kb7
3067. Qa4 kb6 3068. Nh6 rc6 3069. Ng8 rg6 3070. Qd1 rc6 3071. Qdb3 rf6 3072. Qd1 rd6
3073. Nf6 qb5 3074. Nh5 rg8 3075. [g3] rgg6 3076. Qdb3 kb7 3077. Qbd3 kb6 3078. Qa3
rg5 3079. N8c6 rgg6 3080. Na6 qb4 3081. Rh2 rg5 3082. Nf6 rg7 3083. Qab3 rd5 3084. Ne5
rd6 3085. Qa8 rg4 3086. Rh6 rg6 3087. Qd5 rg7 3088. Qed4 rc6 3089. Rh3 rg8 3090. Bg2 rg5
3091. Rh1 rh5 3092. Rg1 rh2 3093. Qa4 rh6 3094. Ba3 kb7 3095. Qd1 kb6 3096. Qa4 rd6
3097. Bc1 rc6 3098. Rh1 rh2 3099. Rg1 kb7 3100. Qb3 kb6 3101. Qdb2 rh5 3102. Qbd4 kb7
3103. Rh1 kb6 3104. Qbb2 rg5 3105. Qb3 re6 3106. Rh3 rc6 3107. Qbd3 rg8 3108. Qb3 re6
3109. Bf1 rc6 3110. Ba3 rg7 3111. Bc1 rg6 3112. Rh6 rg7 3113. Qdd3 rd6 3114. Q3d4 rg6
3115. Qde3 rg7 3116. Qa8 rg6 3117. Qec3 rg4 3118. Qe3 kb5 3119. Rh2 kb6 3120. Rh4 rg7
3121. Rh2 kb5 3122. Qf8 kb6 3123. Qc2 rd5 3124. Qcb3 rd4 3125. Nc6 rd5 3126. Qf2 rd6
3127. Qfe3 kb5 3128. Qa3 kb6 3129. Nab8 rg5 3130. Na6 rd4 3131. Nh5 rd6 3132. Bh3 rgg6
3133. Bf1 rd3 3134. Rh1 rdd6 3135. Qec3 qb5 3136. Qe3 rg8 3137. Nab4 rgg6 3138. Qf7 rg5
3139. Qf8 qa5 3140. Nb8 qb5 3141. N8a6 rgg6 3142. Nb8 kc7 3143. Qad3 kb6 3144. Rh3 kb7
3145. Rh1 rg7 3146. Qb3 rgg6 3147. Bb2 kb6 3148. Bc1 rg8 3149. Qd1 rgg6 3150. [g4] rde6
3151. Ng7 qa4 3152. Rh2 qb3 3153. Nd3 qb2 3154. Qd4 ref6 3155. Qh8 rf7 3156. Qe8 qb1
3157. Ba3 rh6 3158. Qe3 kb5 3159. Nh5 rg7 3160. Rg2 rgg6 3161. Ng3 re6 3162. Qg1 rc6
3163. Qc1 ka4 3164. Bb4+ kb3 3165. Nf2 rh1 3166. Qg8 re6 3167. Ba5 rd6 3168. Nfe4 qa2
3169. Bc3 rh4 3170. Qd1+ ka3 3171. Qc1+ kb3 3172. Qd4 rh1 3173. Qg1 qb1 3174. Ba5
qa2 3175. Nf2 qb1 3176. Bd8 re6 3177. Ba5 qb2 3178. Bb4 qb1 3179. Rh2 rc6 3180. Rg2 qa2
3181. Qe8 qb1 3182. Bc3 rhh6 3183. Bb4 rhf6 3184. Nd3 rh6 3185. Ba3 ka4 3186. Qd8 kb5
3187. Qe8 ka5 3188. Qd1 kb5 3189. Ne5 rce6 3190. Nd3 rh1 3191. Qe3 rhh6 3192. Qh8 reg6
3193. Qe8 rd6 3194. Nh5 rdg6 3195. Nf2 rg7 3196. Nd3 rhg6 3197. Rh2 rh6 3198. Ng3 rf7
3199. Nh5 ka5 3200. Ng7 kb5 3201. Qg8 kb6 3202. Qe8 qb5 3203. Qd4 qb1 3204. Bb2 rg6
3205. Ba3 qa2 3206. Bc1 qb1 3207. Qh8 qb2 3208. Qe8 rff6 3209. Qh8 qb4 3210. Qf8 qb2
3211. Bh3 re6 3212. Bf1 qb5 3213. Qe3 qb2 3214. Rg2 qb3 3215. Rh2 qb1 3216. Nb4 qb3
3217. Na2 qa4 3218. Nb4 qc2 3219. Rh1 qa4 3220. Bb2 qb5 3221. Bc1 kc7 3222. Nh5 kb6
3223. Qdb3 rd6 3224. Qd1 rge6 3225. [g5] rd3 3226. Qh8 rb3 3227. N8a6 qa5 3228. Qe4 re3
3229. Nd5+ kb5 3230. Nc3+ kb6 3231. Na2 qa4 3232. Qed4 rh6 3233. Nc3 qa2 3234. Ba3
qc2 3235. Qb8+ ka5 3236. Qe8 qb1 3237. Qc8 qc1 3238. Nb8 qc2 3239. Na4 rc6 3240. Qe4
rd6 3241. Qh8 rg3 3242. Qb2 rgd3 3243. Bg2 rb6 3244. Ng3 rh6 3245. Qdc1 rb6 3246. Qd5
rh6 3247. Qe4 rh4 3248. Qd1 rh6 3249. Bh3 rb6 3250. Bg2 ra6 3251. Nh5 rb6 3252. Rh2
rbd6 3253. Rh1 rh3 3254. Bf1 rhd3 3255. Qg7 rg3 3256. Qb2 re6 3257. Qh8 rd6 3258. Qb1
re3 3259. Qd1 rf6 3260. Qc8 rd6 3261. Qg2 rc6 3262. Qe4 rh3 3263. Qd4 re3 3264. Qc1 rh6
3265. Qd1 qc1 3266. Nc3 qc2 3267. Na6 qc1 3268. Rh3 qb1 3269. Rh1 qb8 3270. Qe8 qb1
3271. Ng3 qc2 3272. Nh5 kb6 3273. Qb8+ ka5 3274. Qbh8 kb6 3275. Nf6 qa2 3276. Nh5 rg6
3277. Bc1 rh6 3278. Qa8 qa4 3279. Qah8 rf3 3280. Na2 re3 3281. Ba3 rhe6 3282. Bc1 rc6
3283. Qe4 re6 3284. Ng3 qa5 3285. Nh5 kb5 3286. Nc3+ kb6 3287. Nd5+ kb5 3288. Ndb4
kb6 3289. Qb1 rb3 3290. Qe4 rf6 3291. Qe3 re6 3292. Qe8 qb5 3293. Qh8 rd6 3294. Nb8

re6 3295. Qhd4 rd3 3296. Qh8 rh6 3297. Qf8 re6 3298. Qh3 rdd6 3299. Qe3 rf6 3300. [g6]
rd3 3301. Bh3 rf7 3302. Qb3 qa6 3303. Ng3 kc7 3304. Qh6 qc8 3305. Qe4 rf3 3306. Qeb1
qh8 3307. Ne4 qg7 3308. Nc3 kc8 3309. Ba3 re3 3310. Qh5 qe5 3311. Nc2 rf8 3312. Q3a2
qe6 3313. Nd4 qf6 3314. Nd1 qe6 3315. Rg1 rg8 3316. Qc1 kd8 3317. Qab2 qe5 3318. Bb4
kc8 3319. Rg5 qf6 3320. Bg2 rf3 3321. Bh3 re3 3322. Ndc6 qe5 3323. Nd4 qe4 3324. Rg1
qe5 3325. Bc3 kd8 3326. Bb4 qh8 3327. Ba3 qe5 3328. Rf1 qe6 3329. Rg1 rh8 3330. Qa2
rg8 3331. Qh4 kc8 3332. Qh5 qe5 3333. Qcb1 qe6 3334. Rg4 rf8 3335. Rg1 qb6 3336. Rh1
qe6 3337. Nb3 qf6 3338. Nd4 re6 3339. Nc3 re3 3340. Qbb3 qe6 3341. Qbb1 re4 3342. Nc2
re3 3343. Nd5 qe5 3344. Nc3 rh8 3345. Qab3 rf8 3346. Bf1 rf7 3347. Bh3 rf6 3348. Nb4 rf7
3349. Nd3 qg7 3350. Nb4 re6 3351. Qh6 re3 3352. Qa4 rf3 3353. Qab3 kc7 3354. Bc1 kc8
3355. Ne4 kc7 3356. Rh2 qh8 3357. Rh1 qb2 3358. Ng3 qh8 3359. Qa3 qc8 3360. Qab3 kb6
3361. Qe4 kc7 3362. Rh2 rd3 3363. Rh1 qa6 3364. Qe3 qc8 3365. Qf8 qa6 3366. Qg8 kb6
3367. Qf8 qb7 3368. Nh5 qa6 3369. Ba3 qb5 3370. Bc1 rf6 3371. Qd1 rf7 3372. Bf1 rf6 3373.
Qh8 rdd6 3374. Qf8 rc6 3375. [g7] rc8 3376. Qeb3 rd8 3377. Qh8 rg6 3378. N8c6 rg4 3379.
Ng3 rh4 3380. Na6 rh5 3381. Ba3 rc8 3382. Qa4 rh4 3383. Qb1 rd8 3384. Qe8 rb8 3385. Bc1
qb3 3386. Nd8 rb7 3387. Qd3 qb1 3388. Nc7 qa2 3389. Qd4 qb3 3390. Qe4 rh2 3391. Qc2
qf3 3392. Qca2 qe4 3393. Ba3 rb8 3394. Q4c2 rf2 3395. Qa4 rh2 3396. Bg2 rb7 3397. Bf1
rh5 3398. Bc1 rh2 3399. Rg1 qf3 3400. Rh1 qc6 3401. Q2c2 qf3 3402. Rg1 qb3 3403. Rh1
qa3 3404. Qe4 qb3 3405. Qh5 rh4 3406. Qe8 qa3 3407. Qd4 qb3 3408. Qe5 qa2 3409. Qd4
qa3 3410. Qd3 qa2 3411. Qg8 qb1 3412. Qe8 rh2 3413. Na6 rh4 3414. Nf7 qb3 3415. Nd8
qb5 3416. Qb1 qb3 3417. Qc2 rb8 3418. Qb1 rh6 3419. Nc6 rh4 3420. Rh3 qb5 3421. Rh1
rh6 3422. Ba3 rh4 3423. Qbd1 rd8 3424. Qb1 rh5 3425. Qh8 rh4 3426. Qbd1 rc8 3427. Qb1
rh5 3428. Qbd1 ra8 3429. Qab3 rc8 3430. Qb4 rd8 3431. Qbb3 ra8 3432. Bc1 rd8 3433. Qb4
rh4 3434. Qbb3 rh2 3435. Nab4 rh4 3436. Qb1 rg4 3437. Qbb3 ra8 3438. Nh5 rd8 3439. Bh3
rg6 3440. Bf1 qa6 3441. Nb8 qb5 3442. Bb2 rf6 3443. Bc1 rd6 3444. Qf8 rf6 3445. N8a6 rc8
3446. Nb8 qa5 3447. Qe3 qb5 3448. Qe8 rcc6 3449. Qf8 qa6 3450. [g8=N] qa5 3451. Bg2
qa2 3452. Qd4 rc7 3453. Qa4 rd6 3454. Nh6 rd5 3455. Nd3 qb1 3456. Qa3 rc6 3457. Bf1 kc7
3458. Qb3 re5 3459. Ng4 rf6 3460. Rg1 rfe6 3461. Qdb2 kd6 3462. Q2a3 rg6 3463. Qab4 rf6
3464. Q4c3 rd5 3465. Qa3 qb5 3466. Rg2 rd4 3467. Qab4 qa6 3468. Rg3 qa4 3469. Nb2 rd5
3470. Qc2 re6 3471. Qcc3 rf6 3472. Qh8 rd4 3473. Qf8 rh6 3474. Nd3 rf6 3475. Nge5 qa6
3476. Ng4 re6 3477. Rg2 rf6 3478. Qb1 qb5 3479. Qbb4 qc6 3480. Qba3 qb5 3481. Nc6 rd5
3482. Nb8 qb6 3483. Rg1 qb5 3484. Nh6 qb1 3485. Ng4 qc2 3486. Qab3 qb1 3487. Qbb4 re5
3488. Qbb3 rfe6 3489. Qcb4 rf6 3490. Qb5 rg6 3491. Q5b4 rg7 3492. Q4a3 rg6 3493. Qe8
rge6 3494. Qf8 kd5 3495. Qab2 kd6 3496. Qd1 kc7 3497. Qdb3 rd6 3498. Qd4 rde6 3499.
Rh1 rf6 3500. Rg1 rc6 3501. Rh1 ree6 3502. Nh6 re5 3503. Bg2 rd5 3504. Bf1 qa2 3505. Qa3
qb1 3506. Qdg7 kb6 3507. Qd4 rc7 3508. Bg2 rc6 3509. Qa4 rc7 3510. Ng8 qa2 3511. Nh6
kb7 3512. Nb4 kb6 3513. Rg1 rd6 3514. Rh1 qc2 3515. Ng8 qa2 3516. Bf3 rf6 3517. Bg2 qb1
3518. Qd1 qa2 3519. Qb3 rcc6 3520. Qd1 kb7 3521. Qe3 kb6 3522. Qf2 qa5 3523. Qe3 qa6
3524. Bf1 qa5 3525. [d3] qa2 3526. Bh3 rf7 3527. Bd2 qa5 3528. Qa4 rd6 3529. Qd4 rc6
3530. Bf1 rd6 3531. N8c6 re6 3532. Qe4 rd6 3533. Nhf6 qa6 3534. Qe6 kb7 3535. Qe5 kb6
3536. Nd8 rc6 3537. Qd4 qb5 3538. Qd5 qa6 3539. Qh6 qc8 3540. Qe6 qb8 3541. Be3 qc8
3542. Qe4 rf8 3543. Bd2 qc7 3544. Rg1 qc8 3545. Rg2 re6 3546. Rg5 rc6 3547. Rg2 qb8 3548.
Rg1 qc8 3549. Qg6 qc7 3550. Qh6 rf7 3551. Rh1 rf8 3552. Qh4 qc8 3553. Qh6 qa6 3554. Be3
qc8 3555. Qa2 rf7 3556. Qa4 qb8 3557. Qe6 qc8 3558. Bd2 qb8 3559. Qd5 qc8 3560. Rg1
qa6 3561. Rh1 kc7 3562. Qf8 kb6 3563. Qg2 qb5 3564. Qd5 rd6 3565. Qd4 rc6 3566. Ng4
qa6 3567. N4f6 qa5 3568. Qe5 qa6 3569. Qh6 rd6 3570. Qf8 rd5 3571. Ndc6 rd6 3572. Rh6
kb7 3573. Rh1 qb6 3574. Qe6 qa6 3575. Ng4 kb6 3576. N4f6 rd4 3577. Qe4 rd6 3578. Rg1
qa5 3579. Rh1 re6 3580. Nh5 rd6 3581. Qd4 re6 3582. Nb8 rd6 3583. Qc2 rc6 3584. Qa4
rc8 3585. Bh3 rc6 3586. Qc2 rd6 3587. Qa4 rff6 3588. Qe3 rf7 3589. Bc3 rc6 3590. Bd2 rg6
3591. Qd1 rc6 3592. Nhf6 qa2 3593. Nh5 qa6 3594. Bc1 qa2 3595. N4a6 rff6 3596. Nb4 qa3
3597. Bf1 qa2 3598. Qe5 qa5 3599. Qe3 qa6 3600. [d4] kc7 3601. Qe8 qb5 3602. Bg2 rg6
3603. Ng3 qa5 3604. Ne4 rg4 3605. Nc3 rd6 3606. Qf7 kc8 3607. Qg3 rb6 3608. Bf3 qb5
3609. Qf8+ kc7 3610. Qg7 kc8 3611. Qg5 re6 3612. Rh4 rb6 3613. N4a6 rc6 3614. Qh6 rd6
3615. Qh5 kd8 3616. Na2 rh6 3617. Rh3 rc6 3618. Bd2 rc7 3619. Be4 kc8 3620. N2b4 rh4
3621. Na2 rg4 3622. Qa4 kd8 3623. Qd1 qa5 3624. Bf3 qb5 3625. Bb4 rc6 3626. Bd2 qb1
3627. Bc1 qb5 3628. Rh1 rh6 3629. Rh3 rgg6 3630. Rh4 rg4 3631. Bb7 rd6 3632. Bf3 qa5+
3633. Nc3 qb5 3634. Qc2 kc8 3635. Qd1 rg7 3636. Qh6 rg4 3637. Qb3 rc6 3638. Qd1 rc7
3639. Qg5 rc6 3640. Be3 rb6 3641. Bc1 qa5 3642. Nb4 qb5 3643. Qg7 re6 3644. Qg5 kd8
3645. Rh1 kc8 3646. Nh6 rb6 3647. Ng8 kc7 3648. Qg7 kc8 3649. Qf8+ kc7 3650. Qf7 kc8
3651. N8c6 qa5 3652. Nb8 rh4 3653. Bg2 rg4 3654. Nca2 rd6 3655. Nc3 qa3 3656. Qe3 qa5
3657. Qc2 kc7 3658. Qd1 qa3 3659. Qe8 qa5 3660. Rg1 rc6 3661. Rh1 rgg6 3662. Ne4 rg4
3663. Ng3 rgg6 3664. Qa3 qb5 3665. Qe3 rg4 3666. Nh5 rgg6 3667. Qed2 rgf6 3668. Qe3 qb6
3669. Bf1 qb5 3670. Qa3 qa6 3671. Qe3 qb7 3672. Qf8 qa6 3673. Nh6 kb6 3674. Ng8 qa5
3675. [d5] rh6 3676. Ng3 qa3 3677. Nh5 rcd6 3678. Ng7 kc7 3679. Ne8+ kb7 3680. Nef6
rh4 3681. Qe4 rg4 3682. Bh3 kb6 3683. Qd2 re6 3684. Qg2 re3 3685. Qg1 qb3 3686. Ne8
rh4 3687. Bg4 qa3 3688. Rh3 qd3 3689. Qh1 qb3 3690. Qa2 rf3 3691. N8a6 rd3 3692. Qf3
kb5 3693. Ng7 qb1 3694. Qg3 rh6 3695. Qf3 rh4 3696. Nc2 qb3 3697. Ncb4 re3 3698. Ne8
rd3 3699. Qc2 kb6 3700. Qa2 qc2 3701. Qh1 qb3 3702. Qb2 rf3 3703. Qa2 qb1 3704. Nb8
qb3 3705. Ba3 re3 3706. Bc1 qa3 3707. Qd2 qb3 3708. N8a6 qd3 3709. Nb8 qd4 3710. Qg1
qd3 3711. N4c6 qa3 3712. Nb4 rc3 3713. Rh1 re3 3714. Ng7 qb3 3715. Ne8 qa4 3716. Bh3
qb3 3717. Ba3 rg4 3718. Bc1 re4 3719. Nef6 re3 3720. Qf1 qa3 3721. Qg1 kb5 3722. Qg2
kb6 3723. N8c6 re6 3724. Nb8 rh4 3725. Qe4 rg4 3726. Na2 rd6 3727. Nb4 qa2 3728. Qd1
qa3 3729. Qe6 kb7 3730. Qe4 ra6 3731. Bf1 rd6 3732. N4a6 rh4 3733. Nb4 ra6 3734. Qe3
rd6 3735. Qe8 rh6 3736. Qf8 kc7 3737. Ne8+ kb7 3738. Ng7 kc7 3739. Qed3 kb6 3740. Qe3
qa6 3741. Nh5 qa3 3742. Bg2 rc6 3743. Bf1 qb3 3744. Ng3 qa3 3745. Qeb3 qa5 3746. Qe3
rhg6 3747. Nh5 rh6 3748. Qeb3 rhf6 3749. Qe3 qa6 3750. [dxc6] kc7 3751. Qd5 rf7 3752.
Qdd3 kb6 3753. Qf3 qb7 3754. Qf2 rf6 3755. Qh4 kb5 3756. Qhh3 qa8 3757. Qc3 rh6 3758.
Qf7 re6 3759. Nd3 re5 3760. Qd2 kb6 3761. Qb2+ kc7 3762. Qa2 kc8 3763. Bg2 re3 3764.
Ng7 kd8 3765. Qfh5 kc8 3766. Nb2 rc3 3767. Rg1 re3 3768. Qf7 re5 3769. Qg3 kc7 3770.
Bd2 kc8 3771. Bc1 kc7 3772. Qh3 kc8 3773. Qf6 re3 3774. Qf7 rd3 3775. Qfh5 re3 3776. Bf3
rc3 3777. Bg2 rf3 3778. Rh1 rc3 3779. Q3h4 re3 3780. Qh3 re6 3781. Nd3 re3 3782. Bd2 kd8
3783. Bc1 qb7 3784. Qf7 qa8 3785. Qb2 kc8 3786. Qa2 rf3 3787. Nh5 re3 3788. Nhf6 re5
3789. Nh5 rd5 3790. Bf1 re5 3791. Qh4 kc7 3792. Qh3 kb6 3793. Qb2+ kc7 3794. Qd2 kb6
3795. Qc2 kb5 3796. Qd2 re3 3797. Qc3 re5 3798. Qh4 re6 3799. Qh3 rh6 3800. Nb4 re6
3801. Qf8 rh6 3802. Qa3 rf6 3803. Qac3 rg6 3804. Qcd3 rf6 3805. Qc2 qb7 3806. Qcd3 qc8
3807. Qh4 qb7 3808. Qd1 kb6 3809. Qd3 rd6 3810. Qf2 rf6 3811. Ng7 rf7 3812. Nh5 qc7
3813. Qff3 qb7 3814. Ng7 qa6 3815. Nh5 kb5 3816. Qfe3 kb6 3817. Ng3 kc7 3818. Nh5 qc8
3819. Qd5 qa6 3820. Qg2 rf6 3821. Qd5 rd6 3822. Qd1 rf6 3823. Qe4 kb6 3824. Qe3 qa5
3825. [c7] rd6 3826. Rh3 qa6 3827. Qf6 qa5 3828. Bb2 qa4 3829. Qdb3 qa6 3830. Rh2 re6
3831. Qfd4 qa5 3832. Qc2 qa2 3833. N4c6 qb1+ 3834. Qcc1 rd6 3835. Bc3 rh6 3836. Qdd1
qb4 3837. Qa3 kb5 3838. Qdd3 qb1+ 3839. Qd1 kb6 3840. Na6 kb5 3841. Rf2 rf6 3842. Ne5
rf7 3843. Ng4 qb4 3844. N8f6 qb3 3845. Nd5 qb4 3846. Ndf6 rg7 3847. Ng8 rf7 3848. Ne5
qb1 3849. Ng4 rf6 3850. Ne5 kb6 3851. Nc6 kb5 3852. Bg2 rh6 3853. Bf1 re6 3854. Rh2 rh6
3855. Nab4 kb6 3856. Na6 qd3 3857. Nab8 qb1 3858. Rf2 kb5 3859. Rh2 qc2 3860. Qdd3
qb1+ 3861. Qec1 qb4 3862. Qce3 rf6 3863. Qd1 rh6 3864. Qdd2 kb6 3865. Qd1 qa5 3866.
Qac1 qb4 3867. Qe5 qb1 3868. Qee3 qe4 3869. Qdd4 qb1 3870. Qed3 rd6 3871. Q3e3 qc2
3872. Bb2 qb1 3873. Qf2 re6 3874. Qfe3 qa2 3875. Qc2 re4 3876. Nb4 re6 3877. Qh8 qa5
3878. Qhd4 re4 3879. Qcb3 re6 3880. Bc1 qa6 3881. Bb2 qb7 3882. Qf6 qa6 3883. Qc1 rd6
3884. Qce3 qb7 3885. Rh3 qa6 3886. Qa3 qa4 3887. Qab3 qc6 3888. Qd1 qa4 3889. Qf2 qa5
3890. Qe3 qa2 3891. Bc1 qa5 3892. Qg6 qa6 3893. Qf6 qa3 3894. Qf8 qa6 3895. Qeb3 qa5
3896. Qe3 rd4 3897. Rh1 rd6 3898. Bg2 rf6 3899. Bf1 qa6 3900. [c8=R] qa5 3901. Qd5
qa4 3902. Qf2 rf7 3903. N8a6 rf6 3904. Rc6+ kb7 3905. Qe4 qc2 3906. Qe8 rg6 3907. Qd5
rd6 3908. Nc7 qe4 3909. Rg1 qe5 3910. Rh1 qe6 3911. Nc2 qf6 3912. Qff3 qh4+ 3913. Qg3
rh6 3914. Qd1 rg6 3915. Ne3 qg4 3916. Bh3 qg5 3917. Na8 qh6 3918. Qg1 rg5 3919. Qg3
rg6 3920. Qd6 qg5 3921. Qd1 qf6 3922. Nc7 qg5 3923. Qc2 qg4 3924. Qd1 rg5 3925. Bf1
rg6 3926. Qd2 qh4 3927. Qd1 re6 3928. Nc2 rg6 3929. Na8 rh6 3930. Nc7 qh3 3931. Qd5
qh4 3932. Qef7 rd6 3933. Qe8 qg4 3934. Qgf3 qh4+ 3935. Ng3 qf6 3936. Nh5 re6 3937. Qf2
rd6 3938. Qdd4 qe6 3939. Qd5 qe3 3940. Nb4 qe6 3941. Nhf6 qe5 3942. Nh5 qd4 3943. Rg1
qe5 3944. Qef7 qe4 3945. Qe8 qe3 3946. Rh1 qe4 3947. Bh3 qc2 3948. Bf1 rh6 3949. Nca6
rd6 3950. Rg1 rg6 3951. Rh1 qb2 3952. Qe4 qc2 3953. Qh4 rf6 3954. Qf2 rf7 3955. Qf8 rf6
3956. Qe6 qa4 3957. Qe4 qa3 3958. Qd5 qa4 3959. Rc7+ kb6 3960. Rc6+ kb5 3961. Rc8
kb6 3962. Qd3 rf7 3963. Qd5 qa5 3964. Nb8 qa4 3965. Rc7 rf6 3966. Rc8 rd6 3967. Qe3 rf6
3968. Qde4 qa5 3969. Qd5 rg6 3970. Qd1 rf6 3971. Qdd2 qa3 3972. Qdd4 qa6 3973. Qd1
qa4 3974. Qh3 qa6 3975. [e3] qa3 3976. Nd5+ ka5 3977. Qh4 rb6 3978. Qd2+ rb4 3979.
Qc3 qa2 3980. Bg2 qa3 3981. Qg5 qa2 3982. Nhf6 ka4 3983. Qh5 rb1 3984. Qfe8 rb7 3985.
Qe2 kb5 3986. Qf1 qb1 3987. Nc6 qb2 3988. Ne5 rb6 3989. Bf3 qb1 3990. Bg2 qa2 3991. Qf3
rc6 3992. Qd1 qb3 3993. Qe2 qb1 3994. Rd8 qb3 3995. Rc8 rd6 3996. Qd1 rc6 3997. Bd2
qa2 3998. Bc1 rd6 3999. Qf3 rc6 4000. Rh3 rb6 4001. Rh1 qb2 4002. Qf1 qa2 4003. Rb8 qb1
4004. Rc8 qd3 4005. Bf3 qb1 4006. Qf2 qb2 4007. Qf1 qa3 4008. Bg2 qb2 4009. Rc7 rb7
4010. Rc8 rc7 4011. Nc6 rb7 4012. Qc2 qb1 4013. Qc3 rc7 4014. Nb8 rb7 4015. Qf3 qa2
4016. Qf1 qa6 4017. Qe2 qa2 4018. Qed2 ka4 4019. Qe2 rb6 4020. Q2h5 rb7 4021. Nc7 rb1
4022. Ncd5 kb5 4023. Qf8 ka4 4024. Qd2 rb4 4025. Qc3 rb2 4026. Qg5 rb4 4027. Na6 ka5
4028. Nb8 kb5 4029. Nh5 ka5 4030. Qc2 qa3 4031. Qc3 kb5 4032. Qh4 ka5 4033. Nhf6 qa2
4034. Nh5 qa4 4035. Bf1 qa2 4036. Qfg7 qa3 4037. Qf8 qa4 4038. Qd2 qa3 4039. Qd3 rb6
4040. Qd2+ kb5 4041. Qd1 ka5 4042. Nc7 rf6 4043. Nd5 rg6 4044. Qh3 rf6 4045. Nc7 kb6

27

4046. Nd5+ kb5 4047. Nb4 kb6 4048. Bd2 qa6 4049. Bc1 qa5 4050. [e4] rg6 4051. Qf6+
kb7 4052. Qh8 rg4 4053. Bb2 qa2 4054. Qf6 rg1 4055. Nc2 rg6 4056. Qe6 rg5 4057. Rh2
rg6 4058. Qdg4 rf6 4059. Qg5 rf7 4060. Qd5+ kb6 4061. Qd2 qa3 4062. Re2 rf8 4063. Rg2
qg3+ 4064. Qf2 qf3 4065. Nc6 qe3+ 4066. Be2 qg3 4067. Re8 rf6 4068. Ne3 rh6 4069. Qg7
qg4 4070. Qfh4 re6 4071. Bc1 qg5 4072. Bb2 qg4 4073. Rg1 rh6 4074. Rg2 kc7 4075. Qf2
kb6 4076. Qhh4 qg3 4077. Qh3 kb5 4078. Qg5 kb6 4079. Nf1 rf6 4080. Ne3 qf3 4081. Nc2
qg3 4082. Qg7 rf8 4083. Qg5 kc7 4084. Rc8+ kb6 4085. Bf1 qe3+ 4086. Be2 qf3 4087. Bf1
qb3 4088. Nb8 qf3 4089. Qd2 qg3+ 4090. Qf2 qa3 4091. Qd2 rd8 4092. Re2 rf8 4093. Qd5
rf7 4094. Qd2 qa4 4095. Rh2 qa3 4096. Bd4 qa2 4097. Bb2 kb7 4098. Qd5+ kb6 4099. Qe6+
kb7 4100. Ng7 rf6 4101. Nh5 qa3 4102. Qgg4 qa2 4103. Qe2 rg6 4104. Qeg4 qb3 4105. Qd1
qa2 4106. Ngf6 rg5 4107. Ng8 rg7 4108. Rh1 rg5 4109. Nb4 rg6 4110. Nc2 rg4 4111. Qf6 rg6
4112. Nb4 rg1 4113. Nc2 rg4 4114. Nb4 rg3 4115. Qh8 rg4 4116. Qd2 qa5 4117. Qd1 qb6
4118. Bc1 qa5 4119. Qa3 rg6 4120. Qh3 kb6 4121. Qf6+ kb7 4122. Qf8 kb6 4123. Qhf3 rf6
4124. Qh3 qa6 4125. [e5] rf7 4126. Nhf6 rg7 4127. Rc7 rg4 4128. Na2 qb7 4129. Nh6 rg7
4130. Nfg4 rg5 4131. Qc8 rg6 4132. Be3 rg7 4133. Nf7 qa8 4134. Qhf3 qb7 4135. Bd4 qd5
4136. Bg1 rg6 4137. Be2 rg5 4138. Qb7+ ka5 4139. Qa8 qe4 4140. Nfh6 rh5 4141. Qa3+
kb5 4142. Qab3+ ka5 4143. Qf3 kb5 4144. Qf1 qd3 4145. Nf7 qd6 4146. Rh3 qd4 4147. Rh1
qd6 4148. Na6 qd3 4149. Nb8 rh3 4150. Nfh6 rh5 4151. Nh2 qe4 4152. N2g4 ka5 4153. Qf3
kb5 4154. Qfb3+ ka5 4155. Qa3+ kb5 4156. Qf3 ka5 4157. Nf6 rg5 4158. Nfg4 kb6 4159.
Nf7 ka5 4160. Qb1 qd5 4161. Qd1 kb6 4162. Qb7+ ka5 4163. Qc8 kb6 4164. Bf2 rg6 4165.
Bg1 kb5 4166. Bf1 kb6 4167. Bd3 rg7 4168. Bf1 rg8 4169. Bd4 rg7 4170. Nb4 qb7 4171. Na2
qc6 4172. Be3 qb7 4173. Nc1 qa8 4174. Na2 kb5 4175. Qh3 kb6 4176. Nb4 qb7 4177. Na2
qc6 4178. Nfh6 qb7 4179. Qh2 rg6 4180. Qh3 rg5 4181. Bc1 rg6 4182. Qf8 rg5 4183. Na6
rg7 4184. Nb8 rg6 4185. Nf6 rg7 4186. Na6 rg4 4187. Nb8 qc8 4188. Nhg8 qb7 4189. Nh5
qa6 4190. Nhf6 qa4 4191. Nb4 qa6 4192. Qd2 rg7 4193. Qd1 rg6 4194. Rc8 rg7 4195. Be2
rf7 4196. Bf1 qb5 4197. Nh5 qa6 4198. Ng3 rf6 4199. Nh5 qa5 4200. [exf6] qa6 4201. Be2
qa5 4202. Qg3 qa4 4203. Be3 qa5 4204. Qh3 kb5 4205. Qg3 qd8 4206. Qg5 qe8 4207. Bg4
kb6 4208. Ng7 qh5 4209. Bd2 kb7 4210. Qe2 ka8 4211. Be3 kb7 4212. Qg6 qh4+ 4213. Qf2
qg3 4214. Nh5 qh3 4215. N4c6 kb6 4216. Qgg7 qh2 4217. Qf3 kb7 4218. Rd8 qh3 4219. Qff7
kb6 4220. Qff8 kb7 4221. Qh8 qh2 4222. Qhg7 kb6 4223. Rc8 kb7 4224. Qf2 kb6 4225. Qf1
qh3 4226. Qf2 kb5 4227. Qg6 kb6 4228. Qg5 kb7 4229. Qg6 qh2 4230. Nb4 qh3 4231. Qh2
qg3+ 4232. Qf2 ka8 4233. Ng7 kb7 4234. Qfe8 qh4 4235. Qf8 qh2 4236. Qe2 qh4+ 4237. Bf2
qh5 4238. Be3 qh6 4239. Qg5 qh5 4240. Qa2 ka8 4241. Qe2 qf7 4242. Bd2 qh5 4243. Qe8
kb7 4244. Qf8 qf7 4245. Qd1 qh5 4246. Rg1 kb6 4247. Rh1 qh6 4248. Be3 qh5 4249. Qb3
qe8 4250. Qd1 qf7 4251. Nh5 qe8 4252. Qfg7 kb5 4253. Qf8 qg6 4254. Be2 qe8 4255. Qg6
qd8 4256. Qg5 qc7 4257. Qg3 qd8 4258. Bd2 qa5 4259. Be3 qa2 4260. Qh3 qa5 4261. Qd4
kb6 4262. Qd1 qa6 4263. Qg3 qa5 4264. Re8 qa4 4265. Rc8 ka5 4266. Bc1 kb6 4267. Rh4
qa5 4268. Rh1 qb5 4269. Qh3 qa5 4270. Qh4 qa6 4271. Qh3 qa2 4272. Bd3 qa5 4273. Bf1
qa6 4274. Qg7 qa5 4275. [f7] qa3 4276. Qh4 qc3+ 4277. Qd2 qb2 4278. Qd3 qe5+ 4279.
Qe4 qd5 4280. Qd3 qe6+ 4281. Qe4 qd6 4282. Rc7 qf6 4283. Qeg2 qe6+ 4284. Qe4 qd5
4285. Qb2 qb7 4286. N8a6 kb5 4287. Qh8 qc6 4288. Bb2 qf6 4289. Ba3 qh6 4290. Qg5 kb6
4291. Rg1 qf6 4292. Qc6+ ka5 4293. Qh1 qb6 4294. Qgg2 qb8 4295. Qg4 qc8 4296. Qf6 qb7
4297. Qh8 qc8 4298. Nd5 qb8 4299. Ndb4 qb7 4300. Qgg2 qb8 4301. Bd3 qb6 4302. Bf1 qc6
4303. Qg5 qb6 4304. Nh6 qf6 4305. Ng8 kb6 4306. Qc6+ ka5 4307. Qe4 kb6 4308. Na2 qh6
4309. N2b4 qe6 4310. Rh1 qh6 4311. Qhe5 kb5 4312. Qh8 qb6 4313. Qh4 qh6 4314. Qe2 qf6
4315. Qe4 qd4 4316. Bb2 qf6 4317. Bh3 qc6 4318. Bf1 qg6 4319. Bc1 qc6 4320. Rg1 qb7
4321. Rh1 qb8 4322. Qb2 qb7 4323. Qg4 kb6 4324. Qh4 qa8 4325. Nb8 qb7 4326. Ng3 qd5
4327. Nh5 qc6 4328. Qg7 qd5 4329. Bg2 qe6 4330. Bf1 qc6 4331. Qeg2 qe6+ 4332. Be2 qf6
4333. Bf1 qd6 4334. Qe4 qf6 4335. Rc8 qd6 4336. Qd3 qe6+ 4337. Qe4 qd5 4338. Qd3 qe5+
4339. Qe4 qb2 4340. Qd3 qc2 4341. Qd2 qb2 4342. Qd1 qc3+ 4343. Qd2 qa3 4344. Qd1 qd3
4345. Qh3 qa3 4346. Qc2 qa6 4347. Qe2 qa5 4348. Qd1 qa6 4349. Re8 qa5 4350. [f8=R]
kb7 4351. Bb2 qc7 4352. Bc1 qd6 4353. Qh8 kb6 4354. Rg1 qf6 4355. Qc3 qg5 4356. Qhe5
kb7 4357. Qh8 qh6 4358. Qdd2 qf6 4359. Nh6 kb6 4360. Qh2 qg6 4361. Bg2 qc6 4362. Qhg7
qc8 4363. Bh3 qb7 4364. Qf7 qc7 4365. Qcd2 kb5 4366. N8a6 qc8 4367. Rh1 qc7 4368. Rg1
qd6 4369. Nb8 qc7 4370. Qd3 kb6 4371. Qdd2 qd8 4372. Qc3 qc7 4373. Be3 qb7 4374. Bc1
qd5 4375. Qfg7 qb7 4376. Na2 qc8 4377. Nb4 kb7 4378. Bg2+ kb6 4379. Rf1 qc6 4380. Rg1
qb7 4381. Qh8 qc6 4382. Qch3 qg6 4383. Q3c3 qf7 4384. Bf1 qg6 4385. Qh1 qf6 4386. Qh2
qg5 4387. Qhd2 qf6 4388. Qb3 kb7 4389. Qbc3 qb6 4390. Ng8 qf6 4391. Na2 qh6 4392. Nb4
qg5 4393. Qd1 qh6 4394. Qhe5 qg5 4395. N4c6 kb6 4396. Nb4 qg2 4397. Qh8 qg5 4398. Ba3
qf6 4399. Bc1 qd6 4400. Qh3 qf6 4401. Rh1 qd6 4402. Rd8 kb7 4403. Rde8 qd4 4404. Qg7
qd6 4405. Rh2 qc7 4406. Rh1 qc6 4407. Bb2 qc7 4408. Nd3 qa5+ 4409. Nb4 qb5 4410. Bc1
qa5 4411. Bd2 kb6 4412. Bc1 qa6 4413. Qdg4 qa5 4414. Nc6 qa6 4415. Ne5 qa5 4416. Nf3
qa6 4417. Nd3 qa5+ 4418. Ke2 qa6 4419. Nf2 qa5 4420. Qh8 qa3 4421. Nd1 qa2+ 4422.
Nd2 qa3 4423. Kf2 qa2 4424. Kg1 qa3 4425. [Rxa3] kc7 4426. Ng3 kb6 4427. Ra6+ kc7
4428. Qd4 kb7 4429. Qgh4 kc7 4430. Qc3 kb7 4431. Rb6+ kc7 4432. Rc6+ kb7 4433. Rh6
kc7 4434. Ne3 kb7 4435. Qhf6 kc7 4436. Qfe5+ kb7 4437. Qh8 kc7 4438. Ba3 kb7 4439.
Nb3 kc7 4440. Ne2 kb7 4441. Ng2 kc7 4442. Na1 kb7 4443. Qcf3+ kc7 4444. Qfg3 kb7 4445.
Qgh2 kc7 4446. Rh4 kc6 4447. Kf2 kb5 4448. Ra8 ka5 4449. Qf3 kb5 4450. Ng3 ka5 4451.
Rfe8 kb5 4452. Qd4 ka6 4453. Rh6+ kb5 4454. Qde4 ka5 4455. Ra6+ kb5 4456. Rb6+ ka5
4457. Ra6+ kb5 4458. Rh6 ka5 4459. Qd4 kb5 4460. Rh5 ka6 4461. Rh6+ ka5 4462. Rh4 ka6
4463. Qg4 kb5 4464. Qf3 ka5 4465. Qh8 kb5 4466. Rf8 ka5 4467. Ne2 kb5 4468. Qfh3 ka5
4469. Rae8 kb5 4470. Kg1 kc6 4471. Kf2 kc7 4472. Kg1 kb7 4473. Rh6 kc7 4474. Q2g3 kb7
4475. Qf3+ kc7 4476. Qfc3 kb7 4477. Nb3 kc7 4478. Ne3 kb7 4479. Ng3 kc7 4480. Nd2 kb7
4481. Bc1 kc7 4482. Qhe5+ kb7 4483. Qf6 kc7 4484. Qfh4 kb7 4485. Nd1 kc7 4486. Rc6+
kb7 4487. Rb6+ kc7 4488. Ra6 kb7 4489. Qd4 kc7 4490. Q4g4 kb7 4491. Qh8 kc7 4492. Ra5
kb6 4493. Ra6+ kb7 4494. Ra3 kb6 4495. Nf2 kc7 4496. Nd1 kb7 4497. Nh5 kc7 4498. Ra4
kb6 4499. Ra3 [f3] 4500. Qg5 kc7 4501. Rb8 kd6 4502. Ng7 kc7 4503. Rbb3 kc6 4504. Rb4
kc7 4505. Ne8+ kc8 4506. Nd6+ kc7 4507. Qgg3 kc6 4508. Qhg2 kd5 4509. Rd3+ ke6 4510.
Rdb3 kd5 4511. Q2h3 ke6 4512. Ra8 kd5 4513. Qhg7 kc6 4514. Qf4 kd5 4515. Rb7 ke6 4516.
Nf7 kd5 4517. Ba3 ke6 4518. Nd8+ kd5 4519. Nc6 ke6 4520. Rc7 kd5 4521. Qgg5 ke6 4522.
Qfg4 kd5 4523. Rb1 ke6 4524. Nb2 kd6 4525. Nb3 kd5 4526. Kf2 ke6 4527. Bd3 kf7 4528.
Nf6 ke6 4529. Ng8 kd6 4530. Bf1 ke6 4531. Ke3 kd5 4532. Kf2 kd6 4533. Kg1 kd5 4534.
Nd2 kd6 4535. Rd1 ke6 4536. Rb1 kd5 4537. Nd1 ke6 4538. Rb3 kd5 4539. Q4f4 ke6 4540.
Qg7 kd5 4541. Rcb7 ke6 4542. Nd8+ kd5 4543. Nf7 ke6 4544. Bc1 kd5 4545. Nd6 ke6 4546.
R7b4 kd5 4547. Qhh4 kc6 4548. Qh3 kc7 4549. Qfg3 kc6 4550. Qhg2 kd5 4551. Q2h3 ke6
4552. Qh8 kd5 4553. Rf8 ke6 4554. Qhg2 kd5 4555. Rd3+ ke6 4556. Ra3 kd5 4557. Ra2 kc6
4558. Ra3 kc7 4559. Q2h3 kc6 4560. Qg5 kc7 4561. Ne8+ kc8 4562. Ng7+ kc7 4563. Kf2 kc6
4564. Kg1 kd5 4565. Rbb3 kc6 4566. Rc3 kc7 4567. Rcb3 kd6 4568. Rbb8 kc7 4569. Nh5 kd6
4570. Rbe8 kc7 4571. Qa1 kb6 4572. Qh8 kb7 4573. Qgg4 kb6 4574. Kh2 [f2] 4575. Ne3 kb5
4576. Qhf3 kb6 4577. Qd1 kc6 4578. Bg2+ kb5 4579. Rc3 ka6 4580. Rf1 kb6 4581. Qhd4 ka6
4582. Qd3 kb6 4583. Kh1 kb5 4584. Rd8 kb4 4585. Q1e2 ka5 4586. Qc2 kb5 4587. Be4 ka6
4588. Rd1 kb6 4589. Qb2+ ka5 4590. Qh3 ka6 4591. Qa3+ kb6 4592. Rd3 kb5 4593. Qa1
kb4 4594. Qhf3 kb5 4595. Rd4 kb6 4596. Re1 kb5 4597. Rd1 kb4 4598. Rd3 kb5 4599. Qh3
kb4 4600. Qa3+ kb5 4601. Nc2 kb6 4602. Ne3 kc7 4603. Rc3 kb6 4604. Qb3+ ka6 4605.
Qa3+ kb5 4606. Qb2+ ka6 4607. Nef1 ka5 4608. Ne3 ka4 4609. Qhg4 ka5 4610. Nb3+ kb6
4611. Nd2+ kc7 4612. Qc2 kb6 4613. Qb3+ ka6 4614. Qc2 kb5 4615. Rf1 ka6 4616. Bg2 kb5
4617. Rfe8 ka5 4618. Rf8 kb4 4619. Qcd3 ka5 4620. Qd1 kb4 4621. Qh4 kb5 4622. Qhg4 ka5
4623. Rde8 kb5 4624. Nc2 kb6 4625. Ne3 ka5 4626. Kh2 kb6 4627. Qd5 ka6 4628. Qd3 ka5
4629. Qdd4 ka6 4630. Be4 kb6 4631. Bg2 kc7 4632. Qh8 kb6 4633. Nd5+ ka6 4634. Ne3 kb5
4635. Rh1 ka6 4636. Ra3+ kb5 4637. Nf3 kc6 4638. Nd2+ kd6 4639. Bf1 kc6 4640. Qf4 kb6
4641. Qfg4 kb7 4642. Qdf3+ kb6 4643. Nh6 kb5 4644. Ng8 kb4 4645. Qfh3 kb5 4646. Ne4
kb6 4647. Nd2 kb7 4648. Nd1 kb6 4649. Bd3 [f1=R] 4650. Rc3 kc6 4651. Qe3 rf2+ 4652.
Kh3 rf4 4653. Rc8+ kb7 4654. Ra8 ka6 4655. Rae8 kb5 4656. Nf1 kc6 4657. Qeg3 kb6 4658.
Nfe3 kc6 4659. Bc2 kd6 4660. Bd3 ke6 4661. Rf6+ ke5 4662. Rf7+ kd6 4663. Rg7 kc6 4664.
Rf1 rf2 4665. Kh4 re2 4666. Rc8+ kb6 4667. Ra8 rd2 4668. Kg5 rh2 4669. Bd2 ka6 4670. Qf2
kb7 4671. Qfg3 ka6 4672. Rc1 kb6 4673. Rc3 rh3 4674. Bc1 rh2 4675. Q3f3 rd2 4676. Qfg3
rc2 4677. Kh4 rd2 4678. Q3h3 re2 4679. Qhg3 kc6 4680. Rc8+ kb6 4681. Re8 kc6 4682. Bb2
rf2 4683. Bc1 rb2 4684. Kh3 rf2 4685. Rc2 rf4 4686. Rc3 kc7 4687. Rh1 kc6 4688. Qg5 kd6
4689. Q5g4 kc7 4690. Rf7 kd6 4691. Ng7 ke5 4692. Nh5+ ke6 4693. Rf6+ ke5 4694. Rff8+
ke6 4695. Qgg7 kd6 4696. Q7g4 kc7 4697. Bc2 kd6 4698. Rf7 kc6 4699. Rff8 kb5 4700. Bd3
kc6 4701. Qg5 kb6 4702. Q5g4 rd4 4703. Nf1 rf4 4704. Rb3+ kc6 4705. Rc3 kd6 4706. Qe3
kc6 4707. Rc2 kb5 4708. Rc3 ka5 4709. Nd2 kb5 4710. Qef3 ka6 4711. Qe3 rd4 4712. Ra8
rf4 4713. Ng7 kb7 4714. Nh5 kc6 4715. Rac8+ kb7 4716. Rce8 kc6 4717. Kh2 rf2+ 4718.
Kh3 rf1 4719. Kh2 re1 4720. Qeh3 rf1 4721. Qe5 kb6 4722. Qh8 rg1 4723. Ra3 rf1 4724. Qf4
[rxf4] 4725. Ne4 rg4 4726. Nec3 rh4 4727. Ra4 kc7 4728. Ra1 re4 4729. Nb5+ kc6 4730.
Ra6+ kb7 4731. Ba3 rh4 4732. Rh6 rd4 4733. Nb2 rd5 4734. Nc7 rd6 4735. Rg6 ra6 4736.
Rg4 rc6 4737. Rg7 kb6 4738. Ne6 kb5 4739. Qg2 rd6 4740. Rg6 ra6 4741. Rgf6 rc6 4742.
Nhg7 ra6 4743. Rg1 ka5 4744. Re1 kb6 4745. Kg1 ka5 4746. Kh2 rc6 4747. Rg1 ra6 4748.
Rd8 kb5 4749. Rde8 rc6 4750. Rh1 ra6 4751. Nh5 rc6 4752. Rg6 ra6 4753. Rff6 rd6 4754. Rff8
rd5 4755. Rg7 rd6 4756. Rd1 rc6 4757. Rh1 rc7 4758. Qh3 rc6 4759. Rff7 kb6 4760. Rff8 ka5

4761. Nc7 kb6 4762. Rc1 kb7 4763. Rh1 rf6 4764. Rg4 rc6 4765. Bc2 ra6 4766. Bd3 ra4 4767.
Rg6 ra6 4768. Rgf6 rd6 4769. Rg6 kb6 4770. Rh6 kb7 4771. Nd1 rd5 4772. Nb2 re5 4773.
Nb5 rd5 4774. Bf1 rd4 4775. Bd3 re4 4776. Nd1 rd4 4777. Ng3 rh4 4778. Nh5 rf4 4779. Ra6
rh4 4780. Rd8 re4 4781. Rde8 re5 4782. Bc1 re4 4783. Ra4 kc6 4784. Ra6+ kd5 4785. Ra1
kc6 4786. Na3 kc7 4787. Nb5+ kb7 4788. Nbc3 kc7 4789. Qg7 rh4 4790. Qh8 kb7 4791. Ra4
kc7 4792. Bc2 kb6 4793. Bd3 rd4 4794. Ra3 rh4 4795. Ra8 rg4 4796. Rae8 rf4 4797. Ne4 rg4
4798. Nd2 rf4 4799. Ng7 [h6] 4800. Nb1 kc7 4801. Rb8 rh4 4802. Rbe8 kc6 4803. Re1 kb6
4804. Rd8 rd4 4805. Kg3 rg4+ 4806. Kh2 kb5 4807. Qh5 rg6 4808. Rc3 ka5 4809. Nf2 rc6
4810. Re2 rf6 4811. Re3 kb5 4812. Be2 rb6 4813. Rg3 ka6 4814. Kh3 ka5 4815. Qg4 rf6 4816.
Rde8 kb5 4817. Qf3 ka6 4818. Ne6 kb6 4819. Ng4 kb5 4820. Rc8 kb6 4821. Rce8 ka6 4822.
Nf2 kb6 4823. Ng7 ka6 4824. Rg4 kb5 4825. Rg3 re6 4826. Qg4 rf6 4827. Bf3 ka5 4828. Be2
ka6 4829. Rd8 ka5 4830. Rgd3 rb6 4831. Rg3 rb3 4832. Qh5 rb6 4833. Qe8 ka6 4834. Qh5
rd6 4835. Kh2 rb6 4836. Qf7 kb5 4837. Qh5 rf6 4838. Rge3 rb6 4839. Bd3 rf6 4840. Kg3
ka5 4841. Kh2 ka4 4842. Re2 ka5 4843. Re5 rc6 4844. Re2 kb5 4845. Re1 ka5 4846. Qg4 rg6
4847. Qh5 rb6 4848. Nd1 rg6 4849. Bf1 kb5 4850. Bd3 rg3 4851. Ra3 rg6 4852. Qh7 rg4
4853. Qh8 kc6 4854. Qh3 kb5 4855. Rc8 kb6 4856. Rcd8 re4 4857. Kg3 rg4+ 4858. Kf3 rd4
4859. Kg3 re4 4860. Kh2 rd4 4861. Ra4 rh4 4862. Ra3 kc6 4863. Rde8 kb6 4864. Rh1 kc6
4865. Bd2 kc7 4866. Bc1 kb6 4867. Rb8+ kc7 4868. Rb2 rf4 4869. Rbb8 rg4 4870. Rbe8 rf4
4871. Rf1 kb6 4872. Rh1 rd4 4873. Nd2 rf4 4874. Be4 [rxe4] 4875. Qe3 rd4 4876. Qd3 rg4
4877. Qd4 kc7 4878. Qf4+ kb6 4879. Rb3+ kc6 4880. Rg3 rg5 4881. Rg4 kb6 4882. Nf6 rg6
4883. Rhg1 ka5 4884. Qf3 ka4 4885. R1g3 ka5 4886. Qc6 kb4 4887. Qd5 ka4 4888. Qe5 kb5
4889. Ne6 kb4 4890. Qa1 kb5 4891. Nde4 rg7 4892. Nf4 kc6 4893. Qa5 rg8 4894. Ng2 rg7
4895. Rc3 rg8 4896. Rcg3 rg5 4897. Nf4 rg8 4898. N6h5 rg7 4899. Nhf6 rf7 4900. Qa1 rg7
4901. Ng8 kb5 4902. Ngf6 rg8 4903. Ne6 rg7 4904. Ne3 rg6 4905. Nd1 kb6 4906. Nd2 kb5
4907. Rg1 kb4 4908. R1g3 rg7 4909. Qe5 rg6 4910. Qg8 kb5 4911. Qh8 ka6 4912. Ng7 kb5
4913. Ngh5 ka4 4914. Ng7 kb4 4915. Qd5 ka4 4916. Qc6+ kb4 4917. Ba3+ ka5 4918. Bc1
rg5 4919. Qf3 rg6 4920. Ra8 ka4 4921. Rae8 rg5 4922. Rg1 rg6 4923. Nf2 ka5 4924. Nd1 rg5
4925. Qf4 rg6 4926. Rc8 kb6 4927. Rce8 rg5 4928. Rh1 rg6 4929. Ng8 rg5 4930. Nf3 kc6
4931. Nd2 rg6 4932. Rg3 rg5 4933. Rf1 rg4 4934. Rh1 kb6 4935. Rb3+ kc6 4936. Ra3 kb6
4937. Qg5 kc7 4938. Qf4+ kb7 4939. Qd4 kc7 4940. Ne3 kb6 4941. Nd1 re4 4942. Qd3 rg4
4943. Rf7 rd4 4944. Rff8 kc7 4945. Qe3 kb6 4946. Nb3 re4 4947. Nd2 rf4 4948. Qh3 re4
4949. Ne6 [f4] 4950. Rc3 re3 4951. Kg1 re5 4952. Nf2 ka6 4953. Nd1 kb7 4954. Rb3+ kc6
4955. Qf6 kd6 4956. Qd3+ kc6 4957. Qc3 rh5 4958. Qb2 rh4 4959. Qg5 rh5 4960. Qb1 rh2
4961. Qf6 rh5 4962. Rhh3 kd5 4963. Rbc3 rh4 4964. Kh1 kc6 4965. Qa2 kd5 4966. Nb2 rh5
4967. Nd3 rg5 4968. Rb3 rg4 4969. Re3 rg7 4970. Qfa1 kd6 4971. Rd8 rg5 4972. Rb7 rg7
4973. Rb3 rg4 4974. Rde8 rg7 4975. Kh2 kd5 4976. Kh1 rg4 4977. Qf6 rg7 4978. Rh3 rg4
4979. Rc3 rg5 4980. Rb3 rh5 4981. Rc3 kc6 4982. Nb2 kd5 4983. Rh2 rh4 4984. Rhh3 kd6
4985. Nd1 kd5 4986. Re3 kc6 4987. Rc3 rg4 4988. Qb1 rh4 4989. Rc8+ kd5 4990. Rce8 kd6
4991. Kg1 kd5 4992. Kf1 rh5 4993. Kg1 re5 4994. Rb3 rh5 4995. Qh7 kc6 4996. Qb1 kd6
4997. Rh1 kc6 4998. Qba1 rh2 4999. Qb1 re2 5000. Qg5 rh2 5001. Kf1 rh5 5002. Kg1 rh4
5003. Qb2 rh5 5004. Qgf6 rh4 5005. Qbc3 rh5 5006. Ba3 re5 5007. Bc1 kd6 5008. Qd3+ kc6
5009. Qh3 kd6 5010. Qh8 kc6 5011. Rf3 kb7 5012. Rb3+ ka6 5013. Rc3 kb7 5014. Nf2 ka6
5015. Qh4 kb6 5016. Qh3 ka5 5017. Nd1 kb6 5018. Rh2 re3 5019. Rh1 rg3+ 5020. Kh2 re3
5021. Rc2 re4 5022. Rc3 ka5 5023. Ra3+ kb6 5024. Nf6 [f3] 5025. Ra6+ kb5 5026. Ra5+ kc6
5027. Ng7 kb7 5028. Ng4 re3 5029. Ra8 re5 5030. Kg1 re1+ 5031. Nf1 re6 5032. Ra1 kc7
5033. Ngh2 rg6+ 5034. Ng4 kc6 5035. Rad8 rd6 5036. Ngh2 kb5 5037. Rf6 re6 5038. Ra3 re4
5039. Qe8 rh4 5040. Re6 kb4 5041. Rc8 rf4 5042. Kf2 kb5 5043. Kg1 rf7 5044. Rf6 kb4 5045.
Rd8 kb5 5046. Rc8 rf8 5047. Re6 rf7 5048. Rec6 rf4 5049. Re6 rf8 5050. Kf2 rf4 5051. Ng3
kb4 5052. Ngf1 rd4 5053. Kg1 rf4 5054. Ra8 rh4 5055. Rc8 rd4 5056. Rd8 rh4 5057. Qg8 kb5
5058. Qe8 rh5 5059. Rf6 rh4 5060. Be3 re4 5061. Bc1 re6 5062. Qh8 re4 5063. Ra1 re6 5064.
Qe8 rd6 5065. Qh8 rb6 5066. Rff8 rd6 5067. Ng3 kc6 5068. Ngf1 kb7 5069. Ng4 kc6 5070.
Be3 rg6 5071. Bc1 re6 5072. Ra8 rg6 5073. Qh7 kc7 5074. Qh8 ra6 5075. Ngh2 rg6+ 5076.
Kf2 re6 5077. Kg1 re2 5078. Ng4 re6 5079. Nd2 kb7 5080. Nf1 re2 5081. Ra5 re6 5082. Nd2
re1+ 5083. Nf1 re5 5084. Nd2 kc7 5085. Kh2 kb7 5086. Qh7 re3 5087. Qh8 rc3 5088. Rae8
re3 5089. Nf1 re4 5090. Nd2 re1 5091. Nf6 re4 5092. Rb8+ kc6 5093. Rbe8 kc7 5094. Ne6+
kc6 5095. Ra2 kb5 5096. Ra5+ kb6 5097. Ra6+ kb5 5098. Ra3 kb6 5099. Ng8 [f2] 5100. Ra1
kb5 5101. Qh5 rg4 5102. Kh3 rh4+ 5103. Kg3 rd4 5104. Nf3 rd2 5105. Ng7 ra2 5106. Nc3+
kb4 5107. Nd5+ ka4 5108. Nd4 ka5 5109. Rd1 ka4 5110. Ne2 ka5 5111. Qh7 ra3+ 5112. Kf4
ra4 5113. Ngf6 ra3 5114. Q7f5 kb5 5115. Rb1+ rb3 5116. Rd2 rb4 5117. Ra2 rb2 5118. Qh2
rb4 5119. Qg6 rb2 5120. Qhh5 rb3 5121. Qh2 rb2 5122. Nfh5 rb4 5123. Nhf6 rb3 5124. Qf5
rb4 5125. Qe5 rb2 5126. Qf5 rb3 5127. Qhh5 rb2 5128. Rba1 rb4 5129. Rb1 kc6 5130. Rd2
kb5 5131. Rd3 rb3 5132. Rd2 ka6 5133. Rd1 kb5 5134. Bb2 ra3 5135. Bc1+ ka6 5136. Ra1
kb5 5137. Qhg4 ka5 5138. Qgh5 ra4 5139. Qh7 ra3 5140. Ng8 ra4 5141. Kg3 ra3+ 5142. Kg2
ra2 5143. Kg3 ka6 5144. Qh8 ka5 5145. Rh1 ka4 5146. Rd1 kb3 5147. Nd4+ ka4 5148. Ndf6
ka5 5149. Nd5 ra4 5150. Rh1 ra2 5151. Rh2 ka4 5152. Rh1 ra3+ 5153. Nf3 ra2 5154. Nc3+
kb4 5155. Nd5+ kb5 5156. Nc3+ ka5 5157. Nd1 kb5 5158. Qg5 rd2 5159. Qh5 kb4 5160.
Ne6 kb5 5161. Nh2 rd4 5162. Nf3 kb4 5163. Nd2 kb5 5164. Kh3 rh4+ 5165. Kg3 rg4+ 5166.
Kh3 rd4 5167. Kh2 rg4 5168. Ng7 re4 5169. Ne6 re3 5170. Qh3 re4 5171. Nc7+ kb6 5172.
Ne6 re2 5173. Ra3 re4 5174. Nf6 [f1=B] 5175. Re3 rd4 5176. Nf3 rf4 5177. Kg1 bd3 5178.
Nb2 bf5 5179. Ne4 bg6 5180. Nfd4 rg4+ 5181. Qg3 ka6 5182. Nd1 rh4 5183. Qh2 bf7 5184.
Rc3 ka5 5185. Q2e5 bg8 5186. Rch3 kb4 5187. Rb3+ ka4 5188. Rbf3 rh2 5189. Qf4 rb2 5190.
Qb8 ka5 5191. Ng3 ka4 5192. Ng5 rc2 5193. Bb2 rc1 5194. Kg2 ra1 5195. Kg1 rc1 5196. R8f6
rc2 5197. Rff8 bd5 5198. Bc1 bg8 5199. Ndf5 rb2 5200. Nd4 rb4 5201. Nge6 rb2 5202. Rh5
ka5 5203. Rh1 rb3 5204. Ne4 rb2 5205. Rc3 ka4 5206. Rcf3 rb1 5207. Qf4 rb2 5208. Ng7 rh2
5209. Nge6 rf2 5210. Qfe5 rh2 5211. Qh5 rh4 5212. Q5e5 kb4 5213. Rb3+ ka4 5214. Rbh3
kb4 5215. Nef2 ka5 5216. Ne4 bf7 5217. Rc3 bg8 5218. Qh2 bf7 5219. Rc2 ka6 5220. Rc3 bg8
5221. Re3 bf7 5222. Nec3 bg6 5223. Ne4 rh3 5224. Qg3 rh4 5225. Rb8 rg4 5226. Rbe8 bh5
5227. Nb2 bg6 5228. Nf6 kb6 5229. Ne4 rh4 5230. Qh3 rg4+ 5231. Kh2 rf4 5232. Kg1 bh7
5233. Nf3 bg6 5234. Qf1 bf5 5235. Qh3 kb5 5236. Nf6 kb6 5237. Qh2 bd3 5238. Qh3 bb1
5239. Nd1 bd3 5240. Ra8 bf1 5241. Rae8 bg2 5242. Kh2 bf1 5243. Qh7 rd4 5244. Qh8 kb5
5245. Nd2 kb6 5246. Bb2 re4 5247. Bc1 ka5 5248. Ra3+ kb6 5249. Rd3 [bxd3] 5250. Re1
be2 5251. Rh1 bh5 5252. Nf1 ka6 5253. Qg2 be2 5254. Ng5 re3 5255. Rb8 rc3 5256. Bd2
ka5 5257. Nde3 bd1 5258. Ngh7 bg4 5259. Be1 bf5 5260. Ng3 bg6 5261. Qg7 be4 5262. Rb1
bd5 5263. Qb2 be6 5264. Kg1 bf7 5265. Nef5 ka6 5266. Qe2 re3 5267. Qb2 bg6 5268. Qg2
rc3 5269. Qh3 re3 5270. Qg2 rc3 5271. Qb2 re3 5272. Nfe4 bf7 5273. Nef6 be8 5274. Qe2
bf7 5275. Re8 rc3 5276. Rf8 rb3 5277. Qb2 rc3 5278. Rh8 ka5 5279. Rf8 ka4 5280. Ne3 ka5
5281. Qg4 be6 5282. Qg7 ka6 5283. Kh2 ka5 5284. Rc1 bd5 5285. Rb1 ka6 5286. Qg2 ka5
5287. Qc2 be4 5288. Qg2 bc2 5289. Rbb8 be4 5290. Ngh5 bg6 5291. Ng3 ka6 5292. Qh8 ka5
5293. Nfe4 bf5 5294. Nef6 bb1 5295. Ngf1 bf5 5296. Rbc8 bg4 5297. Rb8 bh5 5298. Bd2 bg4
5299. Nfd5 bd1 5300. Ndf6 bc2 5301. Ng5 bd1 5302. Rg8 be2 5303. Rgf8 bh5 5304. Nd1 be2
5305. Rfc8 ka6 5306. Rf8 bh5 5307. Bc1 be2 5308. Rb5 re3 5309. Rbb8 re4 5310. Rbe8 re3
5311. Ne6 re4 5312. Nd4 bh5 5313. Ne6 rd4 5314. Qh3 re4 5315. Qc3 kb6 5316. Qh3 kc6
5317. Nd2 kb6 5318. Qh7 be2 5319. Qh8 bf3 5320. Re1 be2 5321. Qh7 bd3 5322. Qh8 kc6
5323. Rh1 kb6 5324. Bb2 [a6] 5325. Nf1 ka5 5326. Ng4 bc2 5327. Q3c3+ kb6 5328. Ne5 re1
5329. Rf4 kb7 5330. Rf6 re4 5331. Qh7 rd4 5332. Nfe3 bf5 5333. Nf2 bc2 5334. Rg8 bb1 5335.
Rf1 rg4 5336. Nh1 re4 5337. Qb4+ ka7 5338. Qd2 kb6 5339. Rgg6 rh4+ 5340. Kg3 bf5 5341.
Nc7+ ka7 5342. Rf7 rf4 5343. Rgf6 bg6 5344. Qd3 kb8 5345. Qg8+ ka7 5346. Qh7 kb8 5347.
Qd2 ka7 5348. N5g4 bf5 5349. Ne5 re4 5350. Rg6 rf4 5351. Nf2 rh4 5352. Nh1 kb8 5353.
Rff6 ka7 5354. Nc6+ kb6 5355. Ne5+ kb7 5356. Ne6 kb6 5357. N5g4 bb1 5358. Ne5 rf4
5359. Kh2 rh4+ 5360. Kg1 re4 5361. Kh2 rd4 5362. Rg8 re4 5363. Qd3 ka7 5364. Qd2 kb7
5365. Qb4+ ka7 5366. Qc3 kb7 5367. R6f3 rg4 5368. Rf6 rg7 5369. Nf2 rg4 5370. Rf7 rd4
5371. Rf6 bc2 5372. Rh1 bb1 5373. Re8 bc2 5374. Rc1 bf5 5375. Rh1 bh3 5376. Nfd1 bf5 5377.
Kg1 bc2 5378. Kh2 rg4 5379. Nf1 rd4 5380. Nf3 re4 5381. Ne5 ka7 5382. Qh8 kb7 5383. Nd2
re1 5384. Nf1 ka7 5385. Rf4 kb7 5386. Rff8 kb6 5387. Rf4 re4 5388. Rff8 bd3 5389. Ng4 bc2
5390. Qg3 ka5 5391. Qgc3+ ka4 5392. Qh3 ka5 5393. Ndf2 bd3 5394. Nd1 rd4 5395. Nf6 re4
5396. Rd8 kb6 5397. Rde8 kb7 5398. Nd2 kb6 5399. Bc3 [a5] 5400. Qh4 kb5 5401. Qg4 rd4
5402. Qg3 kc6 5403. Nh5 kb6 5404. Rd8 be4 5405. Rg8 bg2 5406. Qc7+ ka6 5407. Qc8+ kb5
5408. Rg7 bh3 5409. Qg8 bf1 5410. Qe8 be2 5411. Qf8 kb6 5412. Kh3 kb5 5413. Rg4 rd5
5414. Nb1 re5 5415. Qa8 bd3 5416. Ne3 re4 5417. Nf1 be2 5418. Qe8 rf4 5419. Nhg3 rf7
5420. Bh8 rf4 5421. Bc3 bf3 5422. Nh5 be2 5423. Bd2 re4 5424. Bc3 bf3 5425. Qf8 be2 5426.
Nh2 bd3 5427. Nf1 rf4 5428. Ne3 re4 5429. Rg3 re5 5430. Rg4 rf5 5431. Nd1 re5 5432. Neg7
be2 5433. Ne6 rg5 5434. Qc8 re5 5435. Rg2 rd5 5436. Rg4 rd4 5437. Nd2 rd5 5438. Rg7 rd4
5439. Qa8 kb6 5440. Qc8 bf1+ 5441. Kh2 be2 5442. Rf1 kb5 5443. Rh1 rd3 5444. Qe8 rd4
5445. Nb3 bf1 5446. Nd2 rd5 5447. Qg8 rd4 5448. Rhg1 bh3 5449. Rh1 bg2 5450. Qh8 bh3
5451. Rgg8 bg2 5452. Qb8+ ka6 5453. Qc8+ kb6 5454. Qc7+ ka6 5455. Qg3 kb6 5456. Qh4
be4 5457. Qg3 bf3 5458. Rgf8 be4 5459. Nb3 bd3 5460. Nd2 bh7 5461. Rde8 bd3 5462. Qh7
kc6 5463. Qh8 bh7 5464. Nf6 bd3 5465. Qg2+ kb5 5466. Qg3 ka4 5467. Qg4 kb5 5468. Kg1
re4 5469. Kh2 bc2 5470. Qh4 bd3 5471. Bb2 kb6 5472. Bc3 rg4 5473. Qh3 re4 5474. Bb2
[a4] 5475. Qf1 rd4 5476. Nf2 kb5 5477. Nh3 be2 5478. Qf4 bg4 5479. Rd8 rd5 5480. Ba1 rh5
5481. Rc8 bf3 5482. Qe3 rf5 5483. Qe4 kb4 5484. Nd5+ ka3 5485. Ndf4 rh5 5486. Qg6 bd1

28

5487. Qg2 rf5 5488. Bf6 rd5 5489. Re1 rd4 5490. Qg6 bf3 5491. Re4 bh1 5492. Nc7 bf3 5493.
Rce8 ka2 5494. Qg3 rd5 5495. Qg6 rd4 5496. Nfd5 ka3 5497. Ndf4 bh5 5498. Rc8 bf3 5499.
Qg5 bh1 5500. Qg6 rd5 5501. Nce6 rd4 5502. Rc7 bf3 5503. Rcc8 rd6 5504. Re1 rd4 5505.
Qgg7 bd1 5506. Qg6 rd6 5507. Qg2 rd4 5508. Rce8 rd5 5509. Rc8 be2 5510. Rh1 bd1 5511.
Qe2 rf5 5512. Qg2 bh5 5513. Ba1 bd1 5514. Qh7 rh5 5515. Qh8 bc2 5516. Qg6 bd1 5517.
Nd3 bf3 5518. Ndf4 rd5 5519. Qe4 rh5 5520. Rf7 rf5 5521. Rff8 kb4 5522. Nd5+ ka3 5523.
Nf6 kb4 5524. Nh7 kb5 5525. Nf6 bh5 5526. Qe3 bf3 5527. Nfe4 rh5 5528. Nf6 bg4 5529. Qf4
bf3 5530. Rcd8 bg4 5531. Bd4 rd5 5532. Ba1 ka5 5533. Bb2 kb5 5534. Rd1 rd4 5535. Rh1
ka6 5536. Rde8 kb5 5537. Qg7 be2 5538. Qh8 ka6 5539. Qf1 kb5 5540. Qf2 bd3 5541. Qf1
rf4 5542. Nf2 rd4 5543. Kg1 kb6 5544. Kh2 rf4 5545. Nd1 rd4 5546. Ng8 re4 5547. Nf6 ka6
5548. Qh3 kb6 5549. Bc3 [a3] 5550. Qf1 re1 5551. Qg1 re4 5552. Qh7 bb1 5553. Qg3 kb7
5554. Ra8 re2+ 5555. Kg1 re4 5556. Qh4 bc2 5557. Qg7 re1+ 5558. Kf2 bb3 5559. Rac8 kb6
5560. Qg2 ba4 5561. Qd4 re2+ 5562. Kf1 re4 5563. Rf7 kb5 5564. Ne8 re1+ 5565. Kf2 re2+
5566. Kg1 re5 5567. Bb4 bb3 5568. Qg6 ba2 5569. Nd8 re4 5570. Qh7 re5 5571. Qg6 rd5
5572. Ne6 re5 5573. Rf8 bb3 5574. Rf7 rd5 5575. Qg2 re5 5576. Nf1 ba4 5577. Nd2 re2 5578.
Bc3 re5 5579. Kf2 re2+ 5580. Kf1 re1+ 5581. Kf2 re4 5582. Kf1 rg4 5583. Nf6 re4 5584. Rd8
kb6 5585. Rc8 rg4 5586. Rff8 re4 5587. Kf2 re2+ 5588. Kf1 re1+ 5589. Kf2 ka7 5590. Qh4
kb6 5591. Bd4 bb3 5592. Bc3 ba2 5593. Qg7 bb3 5594. Nb1 kb7 5595. Nd2 rg1 5596. Ra8
re1 5597. Qh7 bc2 5598. Qg7 re2+ 5599. Kg1 re1+ 5600. Kh2 re4 5601. Kg1 re5 5602. Qh7
re4 5603. Ne3 bb1 5604. Nd1 kb6 5605. Qg3 kb7 5606. Qf7 re2 5607. Qh7 re5 5608. Kh2
re2+ 5609. Qf2 re4 5610. Qg3 re5 5611. Rae8 re4 5612. Ng4 kb6 5613. Nf6 kc6 5614. Qg1
kb6 5615. Nh5 bd3 5616. Nf6 ka7 5617. Qh8 kb6 5618. Qhg7 re1 5619. Qh8 bh7 5620. Qf1
bd3 5621. Nf4 re4 5622. Ne6 ka6 5623. Qh3 kb6 5624. Bb2 [a2] 5625. Nd8 re1 5626. Qf5
re6 5627. Qf1 bc2 5628. Nc6 bf5 5629. Rf7 re1 5630. Ba3 bb1 5631. Nf2 re3 5632. Qg1 re2
5633. Nb4 kb7 5634. Nbd5 bd3 5635. Rb8+ ka7 5636. Rbf8 ka6 5637. Qhg8 re4 5638. Q8g6
rg4 5639. Re8 re4 5640. Qb1 re2 5641. Qe4 bc2 5642. Qh4 bb3 5643. Nde4 kb5 5644. Nb6
ka5 5645. Nfd5 kb5 5646. Ndf6 ka5 5647. Nbd5 kb5 5648. Qg1 ka6 5649. Qb1 bd1 5650.
Nd2 bb3 5651. Nfe4 bc2 5652. Nef6 ka5 5653. Qe4 ka6 5654. Qd4 bd3 5655. Qe4 re1 5656.
Qg6 re2 5657. Rg7 re4 5658. Rf7 re5 5659. Qbg1 re4 5660. Rg7 rg4 5661. Rf7 bc2 5662. Ref8
bd3 5663. Bb2 re4 5664. Ba3 re1 5665. Qg8 re4 5666. Qb1 re2 5667. Qbg1 re5 5668. Qh8
re2 5669. Rg8 ka7 5670. Rgf8 kb7 5671. Rb8+ ka7 5672. Re8 kb7 5673. Qf1 bb1 5674. Qg1
be4 5675. Nb4 bb1 5676. Nc6 kb6 5677. Nb4 re3 5678. Nc6 be4 5679. Qf1 bb1 5680. Nde4
re1 5681. Nd2 bf5 5682. Nd1 bb1 5683. Bb2 bf5 5684. Bc1 re6 5685. Bb2 bb1 5686. Rff8 bf5
5687. Ba1 bc2 5688. Bb2 bb1 5689. Nd8 bc2 5690. Bc3 bd3 5691. Bb2 kc7 5692. Qf5 kb6
5693. Qfh7 re1 5694. Qf5 rf1 5695. Qh3 re1 5696. Kg3 re4 5697. Kh2 rd4 5698. Ne6 re4
5699. Bc3 [a1=R] 5700. Nb2 rb1 5701. Qh5 rd4 5702. Rc1 kc6 5703. Qf5 rd5 5704. Na4 bf1
5705. Nb3 rd1 5706. Ned4+ kc7 5707. Qg8 bh3 5708. Nc6 bf1 5709. Qgg5 rb2+ 5710. Kh1
rg2 5711. Ng8 rg3 5712. Qf2 re1 5713. Qgd2 re4 5714. Bg7 rd3 5715. Nf6 rg4 5716. Rd1
rgd4 5717. Re1 bh3 5718. Nba5 bf5 5719. Nb3 bh3 5720. Ne4 bf1 5721. Nf6 rd6 5722. Rd1
r6d4 5723. Ra1 rg4 5724. Rd1 kb7 5725. Rc1 kc7 5726. Na1 re4 5727. Nb3 kb7 5728. Ng8
kc7 5729. Qg1 rg3 5730. Qgf2 re2 5731. Bc3 re4 5732. Nca5 re1 5733. Nc6 re5 5734. Qg5 re1
5735. Nca5 rd1 5736. Nc6 rd4 5737. Qff5 rd1 5738. Rf6 rg2 5739. Rff8 rdd2 5740. Nf6 rd1
5741. Bb4 rb2 5742. Bc3 bg2+ 5743. Kh2 bf1+ 5744. Qg2 rb1 5745. Qgg5 rd4 5746. Qg8 rd1
5747. Ba1 bh3 5748. Bc3 kb7 5749. Ncd4 kc7 5750. Kg3 bf1 5751. Kh2 bd3 5752. Qh8 bf1
5753. Nc2 kc6 5754. Ncd4+ kd6 5755. Ne6 kc6 5756. Qg4 rd5 5757. Qf5 rd4 5758. Nd2 rd5
5759. Ng8 bd3 5760. Nf6 rd6 5761. Nb2 rd5 5762. Qg7 rd4 5763. Qh8 kb5 5764. Qh5 kc6
5765. Ng8 kb6 5766. Nf6 bf5 5767. Rh1 bd3 5768. Nf4 re4 5769. Ne6 bf1 5770. Qh3 bd3 5771.
Qf1 ra1 5772. Qh3 rd4 5773. Nd1 re4 5774. Ba5+ [rxa5] 5775. Nb2 kb5 5776. Rb1 ka6
5777. Nh7 rf4 5778. Qh4 ka7 5779. Ra1 kb7 5780. Rf5 rf3 5781. Qf4 rf1 5782. Rg8 ra4 5783.
Qfd4 kb6 5784. Rg1 kb7 5785. Rf2 rd1 5786. Qd8 ka6 5787. Kh3 rb1 5788. Qb8 be4 5789.
Qd3 bf3 5790. Rg7 rf1 5791. Qa8+ kb6 5792. Qf8 ba8 5793. Nf6 ka7 5794. Nh7 kb6 5795.
Rg5 bf3 5796. Rg7 ra2 5797. Qa8 ra4 5798. Qg8 ka6 5799. Qa8+ kb5 5800. Qb8+ ka6 5801.
Qe4 rb1 5802. Qd3 bd5 5803. Rg1 bf3 5804. Qc7 be4 5805. Qb8 rf1 5806. Qd4 rb1 5807. Re1
bd3 5808. Rg1 rc1 5809. Qd8 rb1 5810. Qh4 rd1 5811. Qd4 kb5 5812. Kh2 ka6 5813. Rb1
kb7 5814. Ra1 rb1 5815. Q8h8 rd1 5816. Kg2 rf1 5817. Kh2 be2 5818. Rf5 bd3 5819. Qd5+
kb6 5820. Qdd4 rb1 5821. Rg8 rf1 5822. Rg4 kb7 5823. Rg8 rc1 5824. Qf4 rf1 5825. Rfg5 ra5
5826. Rf5 re1 5827. Re8 rf1 5828. Nb3 rf3 5829. Nd2 bb1 5830. Qh4 bd3 5831. Rd1 rf4 5832.
Ra1 rb5 5833. Rff8 ra5 5834. Qg5 ka7 5835. Qh4 bf5 5836. Rb1 bd3 5837. Rf7 ka6 5838. Rff8
kb7 5839. Qh3 ka6 5840. Rf1 re4 5841. Rb1 rg4 5842. Nf6 re4 5843. Kh1 kb5 5844. Kh2 ra6
5845. Rh1 ra5 5846. Ng8 kb6 5847. Nf6 rf4 5848. Nd1 re4 5849. Nf4 [c3] 5850. Qf5 be2
5851. Ng2 kb7 5852. Nh7 rb4 5853. Nc4 rbb5 5854. Qf1 ra6 5855. Rf5 rb4 5856. Rf6 raa4
5857. Qf5 ra2 5858. Qh5 rb5 5859. Rc6 rab2 5860. Qg8 r5b4 5861. Rf1 rb1 5862. Rcc8 bd3
5863. Kh1 ka6 5864. Nd6 rc4 5865. Rff8 bf5 5866. Nb5 bg4 5867. Rf2 rd4 5868. Kg1 bf3
5869. Kh1 bg4 5870. Qh2 rc4 5871. Qh5 kb6 5872. Rff8 ka6 5873. Nf4 bf5 5874. Ng2 rg4
5875. Nd6 rc4 5876. Kg1 bd3 5877. Kh1 rb5 5878. Rf1 rb1 5879. Nf6 rcb4 5880. Nh7 r1b3
5881. Nc4 rb1 5882. Nf2 kb7 5883. Nd1 rc1 5884. Kh2 rcb1 5885. Qgg4 be2 5886. Qg8 rb5
5887. Rc6 r5b4 5888. Nf4 r1b2 5889. Ng2 r2b3 5890. Rh1 rb2 5891. Qgg5 rb5 5892. Qg8
ka7 5893. Qh8 kb7 5894. Q8e5 ra2 5895. Qh8 bf3 5896. Rf6 be2 5897. Nde3 rb4 5898. Nd1
rba4 5899. Qf5 rb4 5900. Rf7 raa4 5901. Rf6 rb5 5902. Qf1 rbb4 5903. Qf8 ra6 5904. Qh8
rd6 5905. Rf5 ra6 5906. Nb6 rb5 5907. Nc4 bf3 5908. Rff8 be2 5909. Rf7 raa5 5910. Rff8
bg4 5911. Qf5 be2 5912. Qf4 rb4 5913. Qf5 raa4 5914. Nd2 ra5 5915. Kg3 re4 5916. Kh2
ba6 5917. Nf6 be2 5918. Ng4 kb6 5919. Nf6 kc6 5920. Nf4 kb6 5921. Kg2 bd3 5922. Kh2
re3 5923. Qh3 re4 5924. Ne2 [c2] 5925. Qh4 ba6 5926. Qg8 ka7 5927. Qh3 kb7 5928. Nb3
ra2 5929. Nbd4 rb2 5930. Qgg2 rb4 5931. Kg3 kb6 5932. Qf5 re6 5933. Qfe4 kc7 5934. Rf7
rd6 5935. Ndc3 re6 5936. Qa8 rb3 5937. Rf1 rb5 5938. Qd8+ kd6 5939. Nfd5 ke5 5940. Rb1
rb3 5941. Qf2 bb5 5942. Nf3+ kd6 5943. Ng5 bd3 5944. Kh3 re5 5945. Rd1 bh7 5946. Rb1
bd3 5947. Kg2 re6 5948. Kh3 bf5+ 5949. Kg3 bd3 5950. Qc8 bb5 5951. Qd8 ke5 5952. Nf3+
kd6 5953. Nfd4 ke5 5954. Qg2 ba6 5955. Qf2 rb5 5956. Qg2 rbb6 5957. Rbf1 rb5 5958. Nb1
kd6 5959. Nbc3 rb3 5960. Nf6 rb5 5961. Qda8 kc7 5962. Rf4 rb3 5963. Rf1 re5 5964. Rh1
re6 5965. Nh7 rb4 5966. Nf6 bb5 5967. Qae4 ba6 5968. Qh4 rd6 5969. Qhe4 rdb6 5970. Nd1
rd6 5971. Qf1 re6 5972. Qfg2 kd6 5973. Rff8 kc7 5974. Nf2 kb6 5975. Nd1 rb1 5976. Qf5
rb4 5977. Rf1 re4 5978. Rh1 kb7 5979. Qfh3 kb6 5980. Kh2 kb7 5981. Qg6 rb2 5982. Qgg2
bc4 5983. Qg8 ba6 5984. Qf1 ra2 5985. Qh3 ra4 5986. Nb3 ra2 5987. Ne3 ra5 5988. Nd1 re3
5989. Nd2 re4 5990. Rd8 ka7 5991. Rde8 re6 5992. Qh4 re4 5993. Nf3 kb6 5994. Nd2 rea4
5995. Qh8 re4 5996. Rd8 bd3 5997. Rde8 rb4 5998. Qh3 re4 5999. Nf4 [c1=N] 6000. Qg7
re2+ 6001. Kg1 re5 6002. Qf7 re1+ 6003. Nf1 be4 6004. Qh7 re2 6005. N6h5 ka6 6006. Nf2
kb7 6007. Qf3 nb3 6008. Qd3 nd4 6009. Nd1 ra8 6010. Qb3+ ka6 6011. Qe6+ ka5 6012. Rh4
nc2 6013. Qc4 bf3 6014. Rh2 ra7 6015. Qa6+ kb4 6016. Qd6 rc7 6017. Qa6 re3 6018. Qa7
ne1 6019. Nhg3 nd3 6020. Ngh5 ne1 6021. Qf5 nc2 6022. Qh7 kc4 6023. Qa6+ kb4 6024.
Qf5 re2 6025. Qh7 be4 6026. Qd6 bf3 6027. Ne6 ra7 6028. Nef4 ka5 6029. Qa6+ kb4 6030.
Qc4+ ka5 6031. Qhe4 ra8 6032. Qh7 rb8 6033. Rh4 ra8 6034. Nfg3 be4 6035. Nf1 rf2 6036.
Qe6 re2 6037. Nb2 nd4 6038. Nd1 ka4 6039. Rh1 ka5 6040. Qef5 ka6 6041. Qe6+ kb7 6042.
Qb3+ ka6 6043. Qd3+ kb7 6044. Qh8 ra5 6045. Qh7 bd5 6046. Nf2 be4 6047. Ne3 nb3 6048.
Nf1 re1 6049. Qf3 re2 6050. Nd2 nc1 6051. Nf1 bc6 6052. Qh3 be4 6053. Qg3 ka6 6054. Qh3
kb5 6055. Nd1 ka6 6056. Q7f5 kb6 6057. Qh7 rb2 6058. Nf6 re2 6059. Qh5 re1 6060. Qh3
kb5 6061. Qf7 kb6 6062. Nh7 bd3 6063. Nf6 re4 6064. Nd2 re1+ 6065. Kf2 re5 6066. Kg1
bc2 6067. Qg7 bd3 6068. Nb2 re2 6069. Nd1 re6 6070. Kh2 re2+ 6071. Nf2 re4 6072. Nd1
re1 6073. Qh8 re4 6074. Ne2 [c4] 6075. Rd8 nb3 6076. Qe6+ ka7 6077. Qd6 bb1 6078. Nh5
ree5 6079. Nf6 rad5 6080. Rg1 rg5 6081. Qa3+ ra5 6082. Ra8+ kb7 6083. Nde4 rac5 6084.
Rad8 rg7 6085. Nd4 rd5 6086. Rg2 rg4 6087. Qa1 rg8 6088. Rg3 bd3 6089. Qa2 rdg5 6090.
Nb5 bc2 6091. Ra8 rf5 6092. Ra6 nd2 6093. Qb3 rf1 6094. Rg2 rf5 6095. Rg3 rf3 6096. Qa2
rf5 6097. Ra4 nb3 6098. Ra6 rf3 6099. Raa8 rf5 6100. Kh3 rfg5 6101. Kh2 nd4 6102. Rad8
nb3 6103. Nef2 bd3 6104. N2e4 bb1 6105. Nd4 bd3 6106. Rg1 rd5 6107. Rg3 rg7 6108. Qa1
rg8 6109. Qa5 bb1 6110. Qa1 ra5 6111. Rg2 rd5 6112. Rc2 rg4 6113. Rg2 rg6 6114. Qa3 rg4
6115. Ndf2 rg7 6116. Nd1 rc5 6117. Rg1 rd5 6118. Ne2 rc5 6119. Rg2 rgg5 6120. Rg1 nc1
6121. Ra8 nb3 6122. Nd4 ra5 6123. Ne2 rae5 6124. Nd2 ra5 6125. Rae8 ka7 6126. Ra8+ kb6
6127. Rad8 ka7 6128. Qb2 rad5 6129. Qa3+ kb6 6130. Qd6+ ka7 6131. Nf4 rge5 6132. Ne2
bc2 6133. Rh1 bb1 6134. Nfe4 ra5 6135. Nf6 rec5 6136. Nh5 re5 6137. Rfe8 re4 6138. Rf8 rb5
6139. Nf6 ra5 6140. Nf3 bd3 6141. Nd2 ra4 6142. Qe6 ra5 6143. Qe5 kb6 6144. Qe6+ kb7
6145. Qh3 kb6 6146. Qh5 nc1 6147. Qh3 rd4 6148. Rde8 re4 6149. Ng4 [nxe2] 6150. Nb1
rd5 6151. Qe5 nc1 6152. Nb2 rb5 6153. Qc3 ra5 6154. Rf2 ne2 6155. Qe3+ kb7 6156. Rf4
ra1 6157. Qf3 ka7 6158. Rb8 ka6 6159. Rd8 ra2 6160. Qf2 bc2 6161. Qce1 ra3 6162. Rff8 kb5
6163. Qfg1 nf4 6164. Qeg3 re2+ 6165. Nf2 be4 6166. Rg8 kc5 6167. Q1g2 re1 6168. Nfd1
ra5 6169. Qc2 ra3 6170. Qcg2 ra6 6171. Nf2 ra3 6172. Qg1 re2 6173. Q1g2 kb5 6174. Qg1
rae3 6175. Rgf8 ra3 6176. Na4 bc2 6177. Nb2 ng2 6178. Ng4 nf4+ 6179. Q1f2 re4 6180. Qfg1
ra5 6181. Q3e1 ra3 6182. Qef2 ne2 6183. Qfe1 rf4 6184. Qgf2 re4 6185. Ne3 ka6 6186. Ng4
bd1 6187. Rf4 bc2 6188. Qd2 ra2 6189. Qde1 bd1 6190. Qc3 bc2 6191. Rc8 bd3 6192. Rd8
kb7 6193. Qf3 ka6 6194. Qg3 ra1 6195. Qf3 re6 6196. Rb8 re4 6197. Qg3 ka7 6198. Qf3
ra2 6199. Re8 ra1 6200. Rh8 kb7 6201. Re8 nd4 6202. Qe3 ne2 6203. Nf2 ra5 6204. Ng4 nc1
6205. Rf2 ne2 6206. Qed2 kb6 6207. Qe3+ rc5 6208. Qh3 ra5 6209. Qf3 nc1 6210. Qh3 nb3
6211. Rff8 nc1 6212. Qg7 rb5 6213. Qc3 nb3 6214. Qe5 nc1 6215. Qh8 rd5 6216. Qe5 rd6
6217. Nd1 rd5 6218. Qg5 ne2 6219. Qe5 kc6 6220. Qh8 kb6 6221. Qh5 ra5 6222. Qh3 ree5

6223. Nd2 re4 6224. Nge3 [c3] 6225. Rg1 bc4 6226. Nd5+ kc5 6227. Nf6 ba2 6228. Rgg8
rea4 6229. Qh4 rb5 6230. Nfe4+ kb4 6231. Ng3+ ka3 6232. Nf3 bb3 6233. Kh1 re5 6234.
Rg6 rd4 6235. Rgf6 ra5 6236. Ra8 re4 6237. Rf4 bd5 6238. Rfc8 nc1 6239. Nd2 bc4 6240. Rg4
bb3 6241. Nh5 ba4 6242. Qg7 rc4 6243. Rf8 rd5 6244. Rg2 re4 6245. Nf4 rc4 6246. Nh5 ra5
6247. Rg4 rd5 6248. Rfc8 ra5 6249. Nf4 re4 6250. Nh5 rd5 6251. Qh8 ra5 6252. Qg5 bb3
6253. Qh4 rf4 6254. Ng3 re4 6255. Nf5 bc4 6256. Ng3 ra6 6257. Rf4 ra5 6258. Nf2 bd5 6259.
Nd1 ka2 6260. Nf3 ka3 6261. Qg8 ne2 6262. Qh8 ng1 6263. Rcf8 ne2 6264. Rg4 bb3 6265.
Rgf4 kb4 6266. R4f6 ka3 6267. Ne1 rd4 6268. Nf3 bg8 6269. Rae8 bb3 6270. R8f7 re5 6271.
Rff8 rc4 6272. Rg6 rd4 6273. Qg4 ra4 6274. Qh4 rg4 6275. Rgg8 ra4 6276. Qc4 rb5 6277.
Qh4 ra8 6278. Kh2 ra4 6279. Ra8 ba2 6280. Rae8 ra7 6281. Nd2 ra4 6282. Q4f6 kb4 6283.
Qh4+ kc5 6284. Nge4+ kb4 6285. Nf6+ kc5 6286. Ne3 rba5 6287. Nd1 bf7 6288. Qh3 ba2
6289. Qe6 re4 6290. Qh3 kb6 6291. Rg1 kc5 6292. Nd5 bc4 6293. Nf6 kb6 6294. Nd5+ kc6
6295. N5e3 kb6 6296. Qh7 bd3 6297. Qh8 ng3 6298. Rh1 ne2 6299. Nf5 [c2] 6300. Qd4+ kb7
6301. Rg1 kc6 6302. Qa7 kd5 6303. Re1 rd4 6304. Kh1 rb5 6305. Ng3 bc4 6306. Qf5+ kd6
6307. Rg8 rbd5 6308. Qg6+ ke5 6309. Qh7 bb5 6310. Nb2 rh4+ 6311. Kg2 rg4 6312. Rg7
rg5 6313. Qa5 bd3 6314. Qd8 rg4 6315. Rf1 ng1 6316. Rh8 rb5 6317. Rf6 rb6 6318. Rfg6 re4
6319. Ndc4+ kd5 6320. Na5 ke5 6321. Nac4+ kf4 6322. Nd2 ke5 6323. Rf6 rg4 6324. Rfg6
rb5 6325. Rf6 rg5 6326. Rf1 rg4 6327. Rd1 rd5 6328. Rf1 rgd4 6329. Re8 rg4 6330. Rf6 ne2
6331. Rf1 nc3 6332. Re1+ ne2 6333. Nb3 rg5 6334. Nd2 kd4 6335. Qa5 ke5 6336. Rg6 bb5
6337. Rg7 kf4 6338. Qa7 ke5 6339. Kh3 rg4 6340. Kg2 bc6 6341. Rgg8 bb5 6342. Kh1 rh4+
6343. Kg2 rhd4 6344. Kh1 rd6 6345. Nd1 r6d5 6346. Qa1 bc4 6347. Qa7 kd6 6348. Qg6+
ke5 6349. Qf5+ kd6 6350. Qa4 rb5 6351. Qa7 rb7 6352. Rgf8 rb5 6353. Qf1 kd5 6354. Qf5+
kc6 6355. Qh3 kd5 6356. Rh8 bd3 6357. Rhf8 nc3 6358. Nf5 ne2 6359. Qh2 ra5 6360. Qh3
ra1 6361. Kh2 ra5 6362. Nc4 re4 6363. Nd2 bc4 6364. Rg1 bd3 6365. Nf1 kc6 6366. Nd2 bc4
6367. Qd4 bd3 6368. Rf1 kb7 6369. Rg1 rc5 6370. Rh1 ra5 6371. Qc3 kb6 6372. Qd4+ ka6
6373. Qh8 kb6 6374. Nfe3 [c1=R] 6375. Qh5 raa1 6376. Nef1 rc8 6377. Rd8 rf4 6378. Qg5
be4 6379. Qf5 rg4 6380. Qf7 kb7 6381. Nb3 raa8 6382. Qf2 rcb8 6383. Qa1 rc8 6384. Nde3
nc3 6385. Rf3 nd1 6386. Nc5+ kb6 6387. Rf6+ rc6 6388. Qe5 nb2 6389. Qb8+ ka5 6390.
Qbg3 bg6 6391. Rc8 re4 6392. Rg1 rb6 6393. Qff4 rb7 6394. Rh8 rc4 6395. Rc8 re4 6396. Qh3
rb6 6397. Qhg3 bh5 6398. Qff2 bg6 6399. Kg2 rc6 6400. Kh2 ra4 6401. Rh1 re4 6402. Rf3
rg4 6403. Rf6 rb8 6404. Rd8 ra8 6405. Ne6 be4 6406. Nc5 kb6 6407. Qb8+ ka5 6408. Qe5
kb6 6409. Qc3 nd1 6410. Qe5 ra2 6411. Qa1 ra8 6412. Rf7 rcc8 6413. Rf6+ bc6 6414. Rf3
be4 6415. Ne6 kb7 6416. Nc5+ kc6 6417. Nb3 kb7 6418. Rg1 nc3 6419. Rh1 ne2 6420. Rff8
nc3 6421. Nd1 ne2 6422. Qf4 rcb8 6423. Qf2 bf3 6424. Qh8 be4 6425. Rf6 rc8 6426. Rff8
ra4 6427. Qf7 raa8 6428. Qb2 ra1 6429. Qh8 kc7 6430. Nbd2 kb7 6431. Qc3 kb6 6432. Qh8
kc5 6433. Qf5+ kb6 6434. Nb1 rf4 6435. Nbd2 rc7 6436. Qg5 rc8 6437. Nc3 bd3 6438. Nd1
rg4 6439. Qh5 rf4 6440. Nfe3 re4 6441. Nef1 kb7 6442. Rde8 kb6 6443. Qg4 rcc1 6444. Qh5
rcb1 6445. Nfe3 rc1 6446. Qh7 ra5 6447. Qh8 ng1 6448. Qh3 ne2 6449. Re1 [h5] 6450. Nb1
rea4 6451. Qf1 bf5 6452. Ndc3 rc4 6453. Qg1 nd4 6454. Na2 rd5 6455. Kh1 ka7 6456. Qgg8
rc7 6457. Rc8 r7c2 6458. Rg1 rd6 6459. Rce8 bh7 6460. Nd5 rc7 6461. Nf6 r7c3 6462. Qgg7
ka6 6463. Ra8+ kb5 6464. Rfc8 nc6 6465. Rg8 re6 6466. Rgc8 bc2 6467. Nd2 rd6 6468. Rd8
nb8 6469. Rc8 nc6 6470. Ra3 re6 6471. Raa8 bb3 6472. Nb1 bc2 6473. Qgf8 bh7 6474. Qfg7
nb4 6475. Rg8 nc6 6476. Nb4 rd6 6477. Na2 rd4 6478. Rgc8 rd6 6479. Ng4 nd4 6480. Nf6
re1 6481. Rf8 rec1 6482. Rad8 ka6 6483. Ra8+ kb6 6484. Rae8 ka6 6485. Rc8 ka7 6486. Rce8
ne2 6487. Qgg8 nd4 6488. Re1 rc7 6489. Rg1 r1c4 6490. Nd5 rc1 6491. Na3 r7c2 6492. Nb1
rc4 6493. Ne3 r4c2 6494. Qd5 bf5 6495. Qdg8 re2 6496. Rc8 rec2 6497. Nc4 rd5 6498. Ne3
bg4 6499. Re1 bf5 6500. Rf7 rc7 6501. Rff8 nc2 6502. Rce8 nd4 6503. Rf6 r7c4 6504. Rff8 bh3
6505. Qg1 bf5 6506. Qhg8 kb6 6507. Qh8 nc2 6508. Kh2 nd4 6509. Kg3 ra5 6510. Kh2 rca4
6511. Nac3 rc4 6512. Qh1 ne2 6513. Qg1 rcc5 6514. Qf1 rc4 6515. Ng2 rca4 6516. Ne3 rd4
6517. Ncd1 rda4 6518. Nf2 bd3 6519. Nfd1 rg4 6520. Qh3 rga4 6521. Qg8 re4 6522. Qh8 re6
6523. Nd2 re4 6524. Qf3 [h4] 6525. Qd4+ kb7 6526. Nd5 ra4 6527. Qff6 rac4 6528. Qf7 rc6
6529. Nf4 re3 6530. Qg8 rf3 6531. Qb4+ bb5 6532. Qg3 r1c3 6533. Qg2 kc7 6534. Ne3 kb7
6535. Qg1 rd6 6536. Ng6 kc6 6537. Qh1 nd4 6538. Nf4 rc4 6539. Kg1 rf6 6540. Qc3 kb6
6541. Ned5+ kb7 6542. Re2 bc6 6543. Qg2 rf1+ 6544. Kh2 rf3 6545. Kg1 nf5 6546. Qh1 nd4
6547. Rf2 bb5 6548. Re2 rd3 6549. Re1 rf3 6550. Nc7 kb6 6551. Ncd5+ ka6 6552. Ne3 kb6
6553. Ne6 kc6 6554. Nf4 ne6 6555. Qb4 nd4 6556. Rb8 rd6 6557. Rbe8 kb7 6558. Kh2 kc6
6559. Rf1 rc3 6560. Re1 kb6 6561. Ng6 kc6 6562. Qf1 ne2 6563. Qh1 kb7 6564. Qg1 kc6
6565. Nf4 kb7 6566. Rf7 rdc6 6567. Rff8 ra3 6568. Qg2 rac3 6569. Qe4 kc7 6570. Qb4 ng3
6571. Nd1 ne2 6572. Qa3 kb7 6573. Qb4 ra3 6574. Qg3 rac3 6575. Qb3 rc1 6576. Qb4 r6c3
6577. Qg8 rc6 6578. Qa5 bd3 6579. Qb4+ ka6 6580. Qd4 kb7 6581. Nc3 re3 6582. Nd1 r6c4
6583. Qf7 rc6 6584. Qg6 re4 6585. Qf7 r6c2 6586. Nd5 rc6 6587. Qc3 rcc4 6588. Qd4 nc3
6589. Qff6 ne2 6590. Rc8 ra4 6591. Rce8 rc2 6592. Qf3 rc1 6593. Qh1 ra5 6594. Qf3 rc7
6595. N5e3 rc1 6596. Qd5+ kb6 6597. Qd4+ rcc5 6598. Qh8 rc1 6599. Kg2 [h3+] 6600. Kf2
ng3 6601. Kg1 rd5 6602. Rf1 ree5 6603. Nc2 nh1 6604. Nc3 rd6 6605. Nde4 kb7 6606. Nb4
kc7 6607. Ne2 ra1 6608. Ng5 bh7 6609. Nf7 rde6 6610. Nd3 re4 6611. Qe5+ kb7 6612. Qg5
bf5 6613. Ndc1 kc6 6614. Qgg4 ra8 6615. Nd4+ kb6 6616. Ndb3 kb7 6617. Rf2 re5 6618. Qc3
rb5 6619. Nd4 rbe5 6620. Qd1 rb5 6621. Qg4 re2 6622. Ndb3 re4 6623. Qd4 rbe5 6624. Qc3
ra7 6625. Qcf3 ra8 6626. Qg5 re6 6627. Qgg4 bg6 6628. Rf1 bf5 6629. Rd8 kb6 6630. Rde8
kc6 6631. Nd4+ kb6 6632. Nde2 kc6 6633. Qf2 ra1 6634. Qff3 kd5 6635. Qg5 kc6 6636. Nh8
kb7 6637. Nf7 rc6 6638. Nd3 rce6 6639. Re1 bh7 6640. Rf1 kc7 6641. Qe5+ kb7 6642. Qh8
kc7 6643. Nh6 r4e5 6644. Nf7 rf5 6645. Nb4 rfe5 6646. Qd5 rd6 6647. Qf3 rc6 6648. Ng5
rd6 6649. Rd1 bd3 6650. Rf1 kb6 6651. Ne4 kc7 6652. N4c3 rc1 6653. Ne4 rh5 6654. N2c3
re5 6655. Rd1 kb7 6656. Rf1 ree6 6657. Nc2 re5 6658. Qh7 kb6 6659. Qh8 ka5 6660. Nd2
kb6 6661. Qe4 rdd5 6662. Qf3 bf5 6663. Nd1 bd3 6664. Qe2 ng3 6665. Qf3 ka6 6666. Nce3
kb6 6667. Nb2 re4 6668. Nbd1 bc2 6669. Re1 bd3 6670. Qc3 ra5 6671. Qh8 rc6 6672. Kf2
rc1 6673. Rf1 ne2 6674. Re1 [h2] 6675. Rd8 kb7 6676. Rde8 ng1 6677. Kg2 rcc5 6678. Qa1
rc2 6679. Qff6 rea4 6680. Qfe5 rcc5 6681. Qf6 rc6 6682. Nf5 ra3 6683. Qfc3 bc4 6684. Qb4+
ka7 6685. Rd8 ba2 6686. Qh8 r3a4 6687. Qc4 ne2 6688. Qhd4+ rac5 6689. Rde8 ng1 6690.
Nb1 kb6 6691. Qe6 kb7 6692. Re3 ka6 6693. Rh3 rc2+ 6694. Kh1 kb5 6695. Qd2 ka6 6696.
Qd4 rcc4 6697. Kg2 rc2+ 6698. Qd2 rc5 6699. Qd4 re5 6700. Re3 rc5 6701. Nh4 kb7 6702.
Nf5 kb6 6703. Re1 kb7 6704. Qec4 kb6 6705. Rf6 ka7 6706. Rff8 ne2 6707. Nd2 ng1 6708.
Rd8 ne2 6709. Qh4 rca5 6710. Qhd4+ rb6 6711. Qh8 rc6 6712. Rc8 ng1 6713. Rcd8 rb6 6714.
Qb4 rc6 6715. Qf4 ra3 6716. Qb4 rc8 6717. Qa1 rc6 6718. Nd4 bc4 6719. Nf5 rb6 6720. Rde8
rc6 6721. Qc5+ kb7 6722. Qb4+ rb6 6723. Qbc3 rc6 6724. Rd8 bd3 6725. Rde8 r3a4 6726.
Qf6 ra3 6727. Nfe3 r3a4 6728. Qf4 rcc5 6729. Qff6 kc7 6730. Qfe5+ kb7 6731. Nf5 rc2 6732.
Nfe3 rc1 6733. Qf6 rc2 6734. Kh1 re4 6735. Kg2 rb2 6736. Qf3 rc2 6737. Qc1 rcc5 6738. Qa1
bb1 6739. Qh8 bd3 6740. Qg4 rc1 6741. Qf3 rd5 6742. Kf2 ra5 6743. Qfh3 ne2 6744. Qf3 rc7
6745. Rd8 rc1 6746. Qh7 kb6 6747. Qh8 ra8 6748. Rde8 ra5 6749. Qfh5 [h1=R] 6750. Nc2
ka7 6751. Qa1 rb4 6752. Na3 ng3 6753. Nb3 rbh4 6754. Nc4 ka6 6755. Nc5+ ka7 6756. Na3
ra6 6757. Qg4 r1h3 6758. Re5 rc3 6759. Rf7 be2 6760. Rg5 nh1+ 6761. Kg2 rb3 6762. Na4
re6 6763. Rgg7 rb1 6764. Qf3 rb2 6765. Rgg8 rhe4 6766. Rgf8 reb6 6767. Nc5 nf2 6768. Qf5
rf6 6769. Qe5 rfb6 6770. Qf5 rc6 6771. Qf3 rcb6 6772. Qc3 nh1 6773. Qf3 r6b3 6774. Na4
rb6 6775. Qf5 rbe6 6776. Qf3 rbb6 6777. Rg8 rb2 6778. Qc3 reh4 6779. Qf3 rf6 6780. Rgg7
re6 6781. Rf4 rb1 6782. Rff7 rg6+ 6783. Qg4 re6 6784. Qb4 rbb3 6785. Qg4 re5 6786. Rg5
re6 6787. Reg8 ra6 6788. Re8 rh7 6789. Nc5 r7h4 6790. Rf2 rbc3 6791. Rf7 ng3 6792. Kf2
rh1 6793. Re5 r1h3 6794. Rb8 bd3 6795. Re8 bb5 6796. Rff8 bd3 6797. Qb1 rc1 6798. Qa1
bf5 6799. Re1 bd3 6800. Qg8 rh1 6801. Qg4 nf1 6802. Qh5 ng3 6803. Nc3 ra5 6804. Nd1 nf5
6805. Nc4 ng3 6806. Ne6 ka6 6807. Nc5+ kb5 6808. Nb3+ ka6 6809. Qc3 ka7 6810. Qa1 ra6
6811. Na3 ra5 6812. Rf7 rb4 6813. Rff8 rg4 6814. Nd2 rb4 6815. Qb5 ne2 6816. Qh5 nf4
6817. Nc2 ne2 6818. Nb2 re4 6819. Nd1 ka6 6820. Qah8 ka7 6821. Nf3 kb6 6822. Nd2 rca1
6823. Nce3 rc1 6824. Ng4 [rxg4] 6825. Rd8 rag5 6826. Qb2+ rb5 6827. Qhe5 rg7 6828. Kf3
rc3 6829. Kf2 rg6 6830. Ne3 rg7 6831. Qc1 rbb3 6832. Rf1 rg2+ 6833. Kf3 rh4 6834. Qh5
kc7 6835. Rf7 ra4 6836. Rdf8 kb6 6837. Qe5 bg6 6838. Qe6+ ka5 6839. Rf4 rb2 6840. Qc4
raa3 6841. Rf2 rh2 6842. Qd4 rh8 6843. Qa4+ kb6 6844. Qd4+ ka5 6845. Rg8 rh2 6846.
Rgf8 rb1 6847. Qc4 rb2 6848. Rh4 rg2 6849. Rhf4 rba2 6850. Rf1 rb2 6851. Re8 ra4 6852.
Ref8 bf7 6853. Qe6 bg6 6854. Qf7 rbb3 6855. Qe6 raa3 6856. R4f7 ra4 6857. Qc4 kb6 6858.
Qe6+ ka7 6859. Qe5 kb6 6860. Re8 bd3 6861. Ref8 bc2 6862. Qh5 bd3 6863. Qb2 kc7 6864.
Qc1 ba6 6865. Rd8 bd3 6866. Rf2 rh4 6867. Rf1 bh7 6868. Rff8 bd3 6869. Nc2 kb6 6870. Ne3
bh7 6871. Qe5 bd3 6872. Qe1 rh1 6873. Qc1 rg3+ 6874. Kf2 rg2+ 6875. Ke1 rg7 6876. Kf2
rg3 6877. Re1 rg7 6878. Nb1 rb5 6879. Nd2 rh5 6880. Qb2 rh1 6881. Qa2 rg6 6882. Qb2
rgh6 6883. Nd1 rg6 6884. Nf3 rg7 6885. Nd2 rc7 6886. Kf3 rc3 6887. Rg8 rc1 6888. Rgf8 rc8
6889. Kf2 rc1 6890. Qa3 rg4 6891. Qab2 ra4 6892. Qh5 rg4 6893. Qa3 rbg5 6894. Qb2+ kc5
6895. Qbh8 kb6 6896. Rc8 ra5 6897. Rcd8 rgc4 6898. Rde8 rg4 6899. Nc4+ [bxc4] 6900.
Q8h7 ka6 6901. Qg7 bd3 6902. Qb2 rcc5 6903. Qg7 rcc4 6904. Qd4 rag5 6905. Qd6+ kb5
6906. Qb8+ ka4 6907. Qb6 rc3 6908. Qbh6 ra3 6909. Rd8 rc5 6910. Qg7 rc8 6911. Rfe8 bg6
6912. Qf8 rc7 6913. Qhh6 rc5 6914. Qc1 bd3 6915. Qg7 rb5 6916. Qc7 rbb4 6917. Rb8 nc3
6918. Qf4 rh8 6919. Qc7 nb5 6920. Qf4 nc3 6921. Qg8 rh1 6922. Qg7 bc4 6923. Qc7 bd3
6924. Qd8 ne2 6925. Qc7 rge4 6926. Rbd8 rg4 6927. Qc5 rb5 6928. Qc7 rgb4 6929. Qc1 rg4
6930. Qcb2 rc5 6931. Qc1 bg6 6932. Qf8 bd3 6933. Qch6 bg6 6934. Qf5 rc7 6935. Qff8 kb5
6936. Qh5+ ka4 6937. Qhf5 rc8 6938. Qh5 rhh3 6939. Qg7 rh1 6940. Qh2 bd3 6941. Qh5
ng3 6942. Rf8 ne2 6943. Qhg6 rc5 6944. Qh5 bc2 6945. Qgh6 bd3 6946. Rde8 rcg5 6947. Rd8
rc3 6948. Rde8 kb3 6949. Qb6+ ka4 6950. Rf4+ rc4 6951. Rff8 rgh4 6952. Qb8 rhg4 6953.

29

Qc7 kb5 6954. Qb8+ kc6 6955. Qd6+ kb5 6956. Qb4+ ka6 6957. Qd6+ kb7 6958. Qd4 ka6
6959. Qh2 ra5 6960. Qh5 rac5 6961. Qg7 ra5 6962. Qa1 rc5 6963. Qg7 re4 6964. Qb2 rg4
6965. Rf5 rc1 6966. Rff8 bh7 6967. Qg7 bd3 6968. Qhh8 bc4 6969. Qh5 rc5 6970. Qgh7 ra5
6971. Kf3 kb6 6972. Kf2 be6 6973. Qh8 bc4 6974. Nb2 [d6] 6975. Ra8 rhg1 6976. Rae8 rg8
6977. Qb5+ kc7 6978. Rb8 bb3 6979. Qa4 r1g4 6980. Qb4 rcc4 6981. Qh3 rf5+ 6982. Qf3
rc3 6983. Nd3 r8g7 6984. Rbc8+ kd7 6985. Ra8 r4g5 6986. Qc5 re5 6987. Rh8 rh5 6988. Ra4
rg4 6989. Qh3 rc1 6990. Rb4 rb1 6991. Qg2 rgg5 6992. Qa5 bc2 6993. Rc4 bb3 6994. Rc5 rh1
6995. Rc4 rhh5 6996. Ra4 bc2 6997. Rc4 rc5 6998. Rb4 rce5 6999. Qad5 bb3 7000. Qa5 rg8
7001. Qc5 rgg5 7002. Rb5 rg4 7003. Rb4 rh6 7004. Qh3 rhh5 7005. Qf1 rc1 7006. Qh3 reg5
7007. Ra4 re5 7008. Re4 rc3 7009. Ra4 ke6 7010. Qf3 kd7 7011. Rha8 rg7 7012. Rh8 bg8
7013. Ra8 bb3 7014. Nb4 rhg5 7015. Nd3 ng3 7016. Rhf8 ne2 7017. Rc1 ref5 7018. Re1 rgf7
7019. Qb4 rfg7 7020. Rg8 rg4 7021. Rgf8 kc7 7022. Rac8+ kd7 7023. Rb8 kc7 7024. Ne5 rg8
7025. Nd3 rg1 7026. Nb2 r1g4 7027. Rg1 rcc4 7028. Re1 rh5 7029. Qh3 rf5+ 7030. Ke3 ra5
7031. Kf2 rce4 7032. Qh8 rc4 7033. Rfe8 rc1 7034. Rf8 rg1 7035. Qa4 r1g4 7036. Qb5 rg1
7037. Qb4 bc4 7038. Qb5 r8g3 7039. Rbe8 rg8 7040. Qa4 kb6 7041. Qb5+ ka7 7042. Qbh5
kb6 7043. Rd1 r8g4 7044. Re1 re4 7045. Ra8 reg4 7046. Rf7 rh1 7047. Rff8 ba2 7048. Rae8
bc4 7049. Rg1 [d5] 7050. Q8e5 rhh4 7051. Rf1 rg6 7052. Rh1 kc5 7053. Rf7 rcg1 7054. Kf3
re6 7055. Qg3 ra8 7056. Rh8 raa1 7057. Nd3+ kb5 7058. Nc1 kb4 7059. Qe1+ kc5 7060. Qg6
ra5 7061. Na2 nc3 7062. Rh5 ree4 7063. Qg5 rb5 7064. Qge3+ rd4 7065. Qf4 nd1 7066. Qee3
rf1+ 7067. Kg3 ra5 7068. Qc7+ kb5 7069. Qd8 kc5 7070. Qc7+ kb5 7071. Qcf4 kc5 7072.
Rg5 rb5 7073. Rh5 nf2 7074. Kf3 nd1+ 7075. Kg2 rg1+ 7076. Kf3 bb3 7077. Qe1 bc4 7078.
Re5 nc3 7079. Rh5 re4 7080. Qfe3+ rd4 7081. Qg5 rde4 7082. Rfh7 ra5 7083. Rf7 nb5 7084.
Qg6 nc3 7085. Qf1 re6 7086. Qe1 rg5 7087. Rh8 rg1 7088. Qf1 ne2 7089. Qe1 rgg4 7090. Nc1
rg1 7091. Qb1 ra1 7092. Qg6 rb6 7093. Qh5 re6 7094. Qf2+ kb4 7095. Qe1+ ka4 7096. Qg3
kb4 7097. Qgg5 kb5 7098. Qg3 rb6 7099. Nd3 re6 7100. Ne1 kc5 7101. Nd3+ kc6 7102. Nb2
kc5 7103. Rh2 ra8 7104. Rh1 bd3 7105. Re8 bc4 7106. Rd8 ra5 7107. Re8 re1 7108. Qge5 rg1
7109. Qb8 reg6 7110. Qbe5 rb1 7111. Kf2 rbg1 7112. Qc3 rc1 7113. Qce5 ng3 7114. Rff8 ne2
7115. Ra8 kb6 7116. Rae8 ra6 7117. Rf1 ra5 7118. Qe4 rgg4 7119. Qee5 rg7 7120. Rg1 rgg4
7121. Qeg5 rh1 7122. Qe5 kb5 7123. Qeh8 kb6 7124. Rg2 [d4] 7125. Qc5+ kb7 7126. Rf7
ra3 7127. Rg7 rhh3 7128. Qh6 rcc3 7129. Qhd6 rgg3 7130. Reg8 be6 7131. Nd1 rg6 7132. Rg5
rh8 7133. Ra8 rcb3 7134. Rf5 bc8 7135. Qc3 rf6 7136. Rf7 ra4 7137. Qd7+ kb6 7138. Qe8
rc4 7139. Rf3 re6 7140. Qd3 re5 7141. Qf8 rcc5 7142. Qd8+ rc7 7143. R7f6+ kc5 7144. Ra4
bb7 7145. Qa6 bc8 7146. Qd3 rh7 7147. Ra8 rh8 7148. Rf8 kb6 7149. R8f6+ re6 7150. Rf7
re5 7151. Qe8 rcc5 7152. Qd8+ kc6 7153. Qf8 kb6 7154. Qe8 rc4 7155. Qf8 re6 7156. Qe8
rh5 7157. Qc3 rh8 7158. Ra3 rf6 7159. Ra8 rb5 7160. Rf5 rb3 7161. Qh3 ra4 7162. Qc3 kb7
7163. Qd7+ kb6 7164. Qd6+ kb7 7165. Ra5 raa3 7166. Ra8 rh5 7167. Rg7 rh8 7168. Ra6 rg6
7169. Ra8 ra4 7170. Qcc5 raa3 7171. Rg5 be6 7172. Rf5 rc3 7173. Rg5 ng3 7174. Rag8 ne2
7175. Qf5 rhh3 7176. Qfc5 rh7 7177. Rg2 rhh3 7178. Re8 rgg3 7179. Reg8 bf7 7180. Nb2 be6
7181. Rd8 bc4 7182. Rdg8 rh1 7183. Re8 rhh3 7184. Rh8 rg4 7185. Re8 rhh4 7186. Qh6 rhh3
7187. Qhh5 rc1 7188. Qh6 rac3 7189. Qh8 ra3 7190. Qe5 rhh1 7191. Qc5 bd5 7192. Rf7 bc4
7193. Qh4 ra5 7194. Qh8 rg8 7195. Rff8 rg4 7196. Qd5+ kb6 7197. Qc5+ ka6 7198. Qch5
kb6 7199. Kf3 [d3] 7200. Kf2 ra7 7201. Ke3 nc3 7202. Q8h6+ kb7 7203. Qa6+ kc7 7204.
Qa2 rgh4 7205. Rgf2 rcf1 7206. Na4 ra5 7207. Qg5 re4+ 7208. Kf3 rf4+ 7209. Ke3 rd4 7210.
Rg8 rf4 7211. Rf3 rc5 7212. Qf5 rfg1 7213. Rg4 ba6 7214. Rg7 re5+ 7215. Kd2 rb5 7216. Re3
re4 7217. Rgg8 rbb1 7218. Qc5+ kd7 7219. Qh5 rb6 7220. Qe2 nd5 7221. Qh5 kd6 7222. Rc8
kd7 7223. Rce8 rb3 7224. Qe2 rb6 7225. Qa3 nc3 7226. Qa2 rc6 7227. Qh5 rb6 7228. Qe5
rbb1 7229. Qh5 ra1 7230. Qc5 rab1 7231. Qb4 kc7 7232. Qc5+ kb7 7233. Qf5 kc7 7234. Re1
rb5 7235. Re3 rf4 7236. Rg7 re4 7237. Rf3 rf4 7238. Ke3 re5+ 7239. Kd2 rc5 7240. Ke3 rh5
7241. Rg4 rhh1 7242. Rh4 bc4 7243. Rg4 rh8 7244. Rgg8 rhh1 7245. Qg4 rf1 7246. Qf5 kb7
7247. Qg5 kc7 7248. Rf2 ra5 7249. Rf3 rd4 7250. Rf2 rd8 7251. Rgf8 rd4 7252. Kf3 rf4+
7253. Ke3 re4+ 7254. Kf3 reh4 7255. Ke3 bg8 7256. Qh5 bc4 7257. Kd2 ra7 7258. Ke3 bb5
7259. Nb2 bc4 7260. R8f6 rc1 7261. Rff8 ra6 7262. Rg2 ra7 7263. Qb1 rg4 7264. Qa2 kb7
7265. Qa6+ kc7 7266. Qah6 kb7 7267. Qh7 kb6 7268. Q7h6+ rg6 7269. Qh8 rg4 7270. Rf4
ne2 7271. Rff8 be6 7272. Kf2 bc4 7273. Kf3 ra5 7274. Kf2 [d2] 7275. Qf7 rh3 7276. Ra8 bd3
7277. Qff6+ rc6 7278. Rg8 kc5 7279. Rg1 nf4 7280. Qhh6 be2 7281. Rg7 rh2+ 7282. Rg2
rhh4 7283. Rh8 rh1 7284. Na4+ kd5 7285. Rg6 rgh4 7286. Re8 bh5 7287. Qd6+ ke4 7288.
Qa3 rc4 7289. Rg7 bg4 7290. Rb8 bc8 7291. Qb2 rb1 7292. Rg8 bb7 7293. Qa1 re5 7294. Rbe8
ra5 7295. Rb8 ne2 7296. Qb2 nf4 7297. Qhf6 bc8 7298. Qh6 rah5 7299. R8g7 ra5 7300. Rf7
rbh1 7301. Rfg7 rg5 7302. Qa3 ra5 7303. R2g3 bg4 7304. Rg2 rb4 7305. Re8 rc4 7306. Rh2
bh5 7307. Rhg2 rc8 7308. R7g6 rc4 7309. Rb8 rc6 7310. Re8 r4h2 7311. Qd6 rh4 7312. Qc7
kd5 7313. Qd6+ kc4 7314. Qf6 kd5 7315. Qhg7 be2 7316. Qh6 rb5 7317. Rh8 ra5 7318. Nb2
rg4 7319. Na4 rb6 7320. Rg7 rc6 7321. Nc3+ kc5 7322. Na4+ kb5 7323. Nb2 kc5 7324. Re8
rhh4 7325. Rh8 rh2 7326. Ra8 rhh4 7327. Rg1 rh2+ 7328. Ke3 rh3+ 7329. Kf2 bd1 7330.
Rgg8 be2 7331. Ra6 bd3 7332. Raa8 rgh4 7333. Qhh8 rg4 7334. Qf8 ne2 7335. Qff6 rh7 7336.
Rg2 rh3 7337. Rg7 kb6 7338. Rgg8 rh6 7339. Rgf8 rh3 7340. Qf4 rc1 7341. Qff6+ kc7 7342.
Qf7 kb6 7343. Rg3 bc4 7344. Rg2 kc7 7345. Rae8 kb6 7346. Rc8 rhh1 7347. Rce8 rgg5 7348.
Qfh5 rg4 7349. Qh2 [d1=R] 7350. Rg3 rhg1 7351. Rg2 kc5 7352. Rf4 rd8 7353. Q8h4 kd5
7354. Rd4+ kc5 7355. Nd1 rg5 7356. Rd6 ra3 7357. Rh8 re8 7358. Qh1 rb8 7359. Rg8 re3
7360. Rh8 nc3 7361. Rhh6 rb3 7362. Q1h3 rb8 7363. Rd2 be2 7364. Ra2 rgg3 7365. Q4g4 kd5
7366. Qh2 ke5 7367. Rc6 rd8 7368. Ra5+ nd5 7369. Ra2 nc3 7370. Qd7 rb8 7371. Qg4 bc4
7372. Rh6 be2 7373. Rb6 kd5 7374. Rh6 bf3 7375. Qhh3 be2 7376. Ra4 kc5 7377. Ra2 rb2
7378. Qgh4 rb8 7379. Rg6 rg5 7380. Rh6 reg3 7381. Rd2 re3 7382. Q3g4 bc4 7383. Qgh3 rb7
7384. Rdd6 rb8 7385. Qe4 rb3 7386. Qeh4 be2 7387. Qh1 bc4 7388. Qg4 rb8 7389. Qgh4
ree5 7390. Rh8 re3 7391. Qe4 ne2 7392. Qeh4 reb3 7393. Rg8 re3 7394. Rgg6 ra3 7395. Rg8
bf7 7396. Rh8 bc4 7397. Q4h2 re8 7398. Qh4 rc2 7399. Q1h2 rc1 7400. Rg6 rd8 7401. Rd6
bd3 7402. Re8 bc4 7403. Rd3 ra5 7404. Rd6 re1 7405. Rd4 rg1 7406. Rf4 rg4 7407. Rd4 bg8
7408. Nb2 bc4 7409. Re4 kd5 7410. Rd4+ kc6 7411. Rf4 kd5 7412. Q2h3 kc5 7413. Qh2 rd2
7414. Qh8 rd8 7415. Rff8 rdd1 7416. Rf4 kb6 7417. Rff8 ka6 7418. Rg3 kb6 7419. Q2h7 rh1
7420. Qh2 rg7 7421. Rg2 rg4 7422. Kf3 nc3 7423. Kf2 rg8 7424. Kf3 [e6] 7425. Q8h6 bd5+
7426. Kf2 raa1 7427. Q6h3 ka5 7428. Qc7+ kb5 7429. Rg7 rdg1 7430. Qc8 rg4 7431. Rge7
bf3 7432. Qg3 rhf1+ 7433. Ke3 rfe1+ 7434. Kd2 rd4+ 7435. Nd3 rh4 7436. Qd8 rgg4 7437.
Ne5 na4 7438. Nc4 re2+ 7439. Kd3 re3+ 7440. Kd2 re5 7441. Qd5+ kb4 7442. Qe4 bg2 7443.
Rff7 rab1 7444. Qc3+ kb5 7445. Qed4 ree4 7446. Ra7 rg5 7447. Rae7 rgg4 7448. Qdd3 re5
7449. Qdd4 rh3 7450. Qe4 rhh4 7451. Qh3 kb4 7452. Qc3+ kc5 7453. Qg3 kb4 7454. Rff8
ra1 7455. Rff7 bf3 7456. Rff8 kb5 7457. Qd5+ kb4 7458. Qd8 kb5 7459. Kd3 re3+ 7460.
Kd2 re2+ 7461. Kd3 ree1 7462. Kd2 bc6 7463. Ne5 bf3 7464. Rd7 nc3 7465. Rde7 rhh1 7466.
Nd3 rhh4 7467. Qf2 rg8 7468. Qg3 kc4 7469. Qc8+ kb5 7470. Nb2 rd4+ 7471. Nd3 rdg4
7472. Nb2 red1+ 7473. Ke3 re1+ 7474. Kf2 rf1+ 7475. Ke3 rh1 7476. Kf2 r4g6 7477. Qh3
rg4 7478. Qh5+ bd5 7479. Qh3 bg2 7480. Rg7 bd5 7481. Rh7 rgg1 7482. Rg7 ra7 7483. Qc7
raa1 7484. Qe5 rgd1 7485. Qc7 rhe1 7486. Rg2 rh1 7487. Qc6+ ka5 7488. Qc7+ ka6 7489.
Qch2 ka5 7490. Rg6 kb6 7491. Rg2 rd4 7492. Qh6 rdd1 7493. Qh8 ra5 7494. Q8h6 ba2 7495.
Kf3 bd5+ 7496. Ke3 bc4 7497. Kf3 rg6 7498. Qh8 rg8 7499. Kf4 [e5+] 7500. Kf5 bf7 7501.
Q2h5 rh3 7502. Re6+ kb5 7503. Rf6 bd5 7504. Re6 ne4 7505. Rd6 ra2 7506. Nd3 rf2+ 7507.
Nf4 rfc2 7508. Rb6+ kc4 7509. Rb3 bb7 7510. Qg5 nc3 7511. Qhf6 nb1 7512. Rc8+ bc6 7513.
Rcb8 rf2 7514. Qgh6 rff3 7515. Rh2 rc2 7516. Rc8 rgg2 7517. Rf8 rcc3 7518. Ra8 re2 7519.
Rh8 rdc1 7520. Qe7 rd1 7521. Qef6 rff1 7522. Ra8 rff3 7523. Qfh4 rg2 7524. Q4f6 rgf2 7525.
Rf8 rg2 7526. Rb5 rcc2 7527. Rb3 rdg1 7528. Rc8 rd1 7529. Qhh4 rg8 7530. Qhh6 na3 7531.
Rcb8 nb1 7532. Qhh4 rcc1 7533. Qhh6 rgg3 7534. Rg2 rg8 7535. Rbb2 rf2 7536. R2b3 bb5
7537. Qhg5 bc6 7538. Qd6 rfc2 7539. Qdf6 bb7 7540. Rc8+ bc6 7541. Rf8 bb7 7542. Qg4 nc3
7543. Qgg5 ba6 7544. Qh8 bb7 7545. Qd8 ne4 7546. Qg5 rd6 7547. Qgh5 rdd1 7548. Rb5
bd5 7549. Rb3 rhh1 7550. Rb6 rh3 7551. Re6 kb5 7552. Rb6+ ka5 7553. Rd6 kb5 7554. Nd3
rf2+ 7555. Nf4 ra2 7556. Nd3 rg4 7557. Nb2 rg8 7558. Qh4 ra5 7559. Q4h5 rh2 7560. Re6
rh3 7561. Q8h7 nc3 7562. Qh8 bf3 7563. Ref6 bd5 7564. Rg4 bf7 7565. Rg2 nd5 7566. Re6
nc3 7567. Re7 kb6 7568. Re6+ kb7 7569. Ree8 kb6 7570. Rg3 rhh1 7571. Rg2 rb1 7572. Qh2
rbc1 7573. Q2h7 bc4 7574. Qh2 [e4+] 7575. Q8e5 rdd5 7576. Rc8 rhg1 7577. Rb8+ kc5
7578. Rg3 na2 7579. Ra8 rgf1+ 7580. Rf3 rg5+ 7581. Kf6 rf2 7582. Ke6 rcf1 7583. Rfb8 rd2
7584. Ra6 rh5 7585. Ra3 rf7 7586. Rh3 rb5 7587. Rab6 bd3 7588. Rc6+ kb4 7589. Rd6 rff5
7590. Rbd8 rh7 7591. Qeg3 rd4 7592. Rh6 rc7 7593. R6d7 ra7 7594. Qg8 rc7 7595. Qgg3 bb1
7596. Rd6 bd3 7597. Qe5 rh7 7598. Qeg3 rg5 7599. Rh3 rgf5 7600. Qg5 rdd5 7601. Qgg3
rhf7 7602. Qe5 rh7 7603. Qhf4 rhh5 7604. Qh2 bc2 7605. Rb8 bd3 7606. Re3 rf7 7607. Rh3
kc5 7608. Rc6+ kb4 7609. Rcb6 kc5 7610. Qhf4 bc4 7611. Qh2 rc2 7612. Ra6 rd2 7613. Qg2
ra5 7614. Qgh2 ra7 7615. Ra3 rf7 7616. Rf8 rf1 7617. Rb8 rh7 7618. Rf3 rh5 7619. Rd3 rg5
7620. Rf3 rfg1 7621. Raa8 rf1 7622. Rb6 rdf2 7623. Rbb8 nc3 7624. Rbf8 na2 7625. R3f7 rc1
7626. Rf3 rg8 7627. Kf6 rg5 7628. Qh6 rff1 7629. Qhh2 rg3 7630. Kf5 rg5+ 7631. Ke6 rg8
7632. Kf5 rfe1 7633. Rg3 rf1+ 7634. Ke6 rg1 7635. Kf5 kc6 7636. Rab8 kc5 7637. Kf4 nc3
7638. Kf5 bf1 7639. Rg2 bc4 7640. Ra8 kb6 7641. Rab8+ ka7 7642. Rbc8 kb6 7643. Rg3 rh1
7644. Rg2 rdb5 7645. Rce8 rd5 7646. Rf2 rdd1 7647. Rg2 raa1 7648. Qeh8 ra5+ 7649. Kf6
[e3] 7650. Q8h5 ka7 7651. Qb5 rd3 7652. Qh8 rd2 7653. Re6 ra2 7654. Ke5 be2 7655. Rd8
rhd1 7656. Rc8 re8 7657. Rg6 bg4 7658. Qh1 ne2 7659. Rg8 bf3 7660. Nd3 rdb2 7661. Kd6
nf4 7662. Qh6 ng6 7663. Rc4 ba8 7664. Rce4 rc3 7665. Rf4 ne5 7666. Qa4+ kb6 7667. Rg5
bb7 7668. Rf2 rdd2 7669. Qa3 re7 7670. Qa4 re8 7671. Qc4 rd1 7672. Qa4 re1 7673. Rf4 rd1

7674. Qh7 ba8 7675. Qh6 rh2 7676. Rg8 rhb2 7677. Qb3+ ka7 7678. Qa4+ kb8 7679. Qb5+
ka7 7680. Rf5 ng6 7681. Rf4 rbd2 7682. Rfe4 rdb2 7683. Kd7 rcc1 7684. Kd6 re1 7685. Rc4
red1 7686. Qf8 bf3 7687. Qh6 be4 7688. Rc8 bf3 7689. Qbh5 nf4 7690. Qb5 rca1 7691. Qh1
rac1 7692. Qd5 ne2 7693. Qb5 ra4 7694. Ke5 raa2 7695. Nc5 rbd2 7696. Nd3 rf1 7697. Nb2
rfd1 7698. Qh3 bg4 7699. Qh1 nc3 7700. Rg6 ne2 7701. Qh8 nc3 7702. Rc4 be2 7703. Rc8
nd5 7704. Rg2 nc3 7705. Rb8 rg8 7706. Rc8 bg4 7707. Rd8 be2 7708. Qe8 rh1 7709. Qb5
rhe1 7710. Rf8 rh1 7711. Kf6 bc4 7712. Ke5 ra5 7713. Kf6 rd4 7714. Ree8 rd2 7715. Rh2 rd3
7716. Rg2 rhe1 7717. Qh2 rh1 7718. Qd6 rdd1 7719. Qh2 rdg1 7720. Qbh5 rgd1 7721. Rg7+
kb6 7722. Rg2 rb1 7723. Qh8 rbc1 7724. Rg3 [e2] 7725. Qg2 ba2 7726. Rg5 bb1 7727. Qf3
bg6 7728. Qg2 ra7 7729. Rf7 rb7 7730. Ra5 na4 7731. Rb5+ ka7 7732. Rd5 re7 7733. Qe4
rc2 7734. Qg2 rc6+ 7735. Kg5 rc5 7736. Rf3 rd3 7737. Qf2 rc3 7738. Qhd4 rgg7 7739. Rf7
rh8 7740. Qe1 bf5+ 7741. Kf4 re3 7742. Re5 rg2 7743. Rff8 rhh7 7744. Qec3 rgg7 7745. Nc4
rg2 7746. Nb2 re6 7747. Qe1 ree7 7748. Qd5 rh8 7749. Qd4 rg6 7750. Rf7 rg2 7751. Qd6 rg7
7752. Qd4 ra3 7753. Rd5 re3 7754. Qf6 rec3 7755. Qd4 rgh7 7756. Kg5 rg7+ 7757. Kf6 bg6
7758. Kg5 rh1 7759. Qef2 rh8 7760. Rf3 rh1 7761. Rd8 rg8 7762. Re8 rh4 7763. Qh8 rh1
7764. Qf1 rd3 7765. Qf2 reh7 7766. Qg2 re7 7767. Rd8 rdd1 7768. Re8 rd2 7769. Rf7 rdd1
7770. Kf6 rc6+ 7771. Kg5 rc2 7772. Kf6 rdc1 7773. Qe4 rcd1 7774. Red8 rcc1 7775. Re8 rd3
7776. Qg2 rdd1 7777. Qh7 rb7 7778. Qh8 kb6 7779. Rb5+ ka7 7780. Ra5+ kb6 7781. Ref8
nc3 7782. Re8 rbd7 7783. Rg5 rb7 7784. Qg3 ra7 7785. Qg2 rc2 7786. Rff8 rcc1 7787. Ke6
ra5 7788. Kf6 ra4 7789. Qf3 ra5 7790. Qa8 bb1 7791. Qf3 ra2 7792. Qg2 ra5 7793. Rf5 ba2
7794. Rg5 rd7 7795. Rg3 rdd1 7796. Qf3 bc4 7797. Qg2 rdd5 7798. Qgh2 rdd1 7799. Rg2
[e1=Q] 7800. Q2h5 rd3 7801. Rc8 nd1 7802. Qd5 rd4 7803. Rg3 qe8 7804. Qd6+ kb7 7805.
Qe7+ ka6 7806. Na4 rb1 7807. Rd3 rg3 7808. Rb3 bf1 7809. Qh4 qh5 7810. Nb6 bh3 7811.
Rfe8 qd5 7812. Qhh7 raa4 7813. Rb4 qa8 7814. Ke5 bd7 7815. Rc7 rc3 7816. Rbc4 kb5 7817.
Qh3 rcd3 7818. Qh5 rbb4 7819. Qd6 rb1 7820. Qe7 qd8 7821. Qh3 qa8 7822. Rh8 rc3 7823.
Re8 qc8 7824. Qhh7 qa8 7825. Qg8 ka6 7826. Qgh7 bc8 7827. Rb4 bd7 7828. Rh8 rg3 7829.
Re8 rba1 7830. Rcc8 rb1 7831. Qd6 bh3 7832. Qde7 rh4 7833. Kf6 rd4 7834. Rb2 qd5 7835.
Rb4 raa3 7836. Rb3 raa4 7837. Qe1 ra5 7838. Qee7 rg5 7839. Qh4 rg3 7840. Qd7 qh5 7841.
Qe7 nb2 7842. Rf8 nd1 7843. Rf3 bf1 7844. Rb3 rg7 7845. Na4 rg3 7846. Re3 qe8 7847. Rb3
qb5 7848. Qh8 qe8 7849. Rd8 bc4 7850. Rc8 rh7 7851. Rd3 rh1 7852. Rf3 rg8 7853. Rd3 qd7
7854. Rg3 qe8 7855. Qf7 rc1 7856. Qe7 rgg5 7857. Nb2 rg8 7858. Qe1 kb7 7859. Qe7+ kb6
7860. Qd6+ kb7 7861. Qd5+ kb6 7862. Rg6 qe1 7863. Rg3 raa1 7864. Rg2 ra5 7865. Qe5 rd3
7866. Qd5 ra4 7867. Qdh5 ra5 7868. Rfe8 nc3 7869. Rf8 nb1 7870. Rce8 nc3 7871. Re3 rdd1
7872. Ree8 qd2 7873. Qh2 qe1 7874. Nd3 [rxd3] 7875. Rf2 ra4 7876. Rd2 rhg1 7877. Qb8+
ka5 7878. Qa8+ kb5 7879. Qb7+ kc5 7880. Qe7+ kb5 7881. Rd8 r8g2 7882. Rde8 rb1 7883.
Qd8 ra3 7884. Re3 raa1 7885. Rb2+ ka6 7886. Re6+ ka7 7887. Ree8 rd5 7888. Qh2 ba2
7889. Qh8 rdd1 7890. Qd4+ ka6 7891. Qdh4 qe4 7892. Rc2 nb5 7893. Rb2 kb7 7894. Rc8
bd5 7895. Q8h6 qe2 7896. Rd2 qe4 7897. Rb2 ra4 7898. Qh8 raa1 7899. Rg8 ba2 7900. Rgf8
qh7 7901. Rce8 qe4 7902. Rf2 ka6 7903. Rb2 qe3 7904. Rc2 qe4 7905. Qh1 nc3 7906. Q1h4
rde1 7907. Rb2 red1 7908. Rd2 qe1 7909. Rb2 ka7 7910. Qd4+ ka6 7911. Qd8 ka7 7912. Rg8
rd5 7913. Rgf8 rc5 7914. Qh2 rd5 7915. Qh3 bc4 7916. Qh2 rb5 7917. Qh8 rd5 7918. Rb5
rd3 7919. Rb2 ka6 7920. Re6+ ka7 7921. Re3 ka6 7922. Rbe2 kb5 7923. Rb2+ kc6 7924.
Rd2 kb5 7925. Qa8 ra3 7926. Qd8 ra4 7927. Ree8 ra3 7928. Qe7 ra4 7929. Qe3 rc1 7930.
Qe7 qe4 7931. Rd8 qe1 7932. Qh2 rg8 7933. Qh8 rh3 7934. Rde8 rd3 7935. Qe4 kc5 7936.
Qe7+ kd5 7937. Qb7+ kc5 7938. Qbg7 kb5 7939. Qb7+ ka5 7940. Qa8+ kb5 7941. Qb8+
ka5 7942. Qf4 kb6 7943. Qb8+ kc5 7944. Qbh2 kb6 7945. Ree2 rh1 7946. Ree8 ra5 7947.
Rf2 ra4 7948. Rg2 ra5 7949. Rg1 [rgxg1] 7950. Re4 raa1 7951. Q2h3 rg7 7952. Q8h6 rdd7
7953. Rd4 rgg1 7954. Qh7 ba2 7955. Rc8 ra7 7956. Qf3 rc7 7957. Qd1 kc6 7958. Qh6 qg3
7959. Rd5 rg2 7960. Qhh5 rh4 7961. Qd3 rhh2 7962. Rd7 na4 7963. Qhh7 bb3 7964. Qd6+
kb5 7965. Qf4 rh3 7966. Qff5+ kb4 7967. Re7 rgh2 7968. Qg7 r7c3 7969. Rf8 rf2 7970. Rc8
rfh2 7971. Qfg6 rc7 7972. Qf5 bf7 7973. Qgh7 bb3 7974. Qc2 rg2 7975. Qcf5 ka3 7976. Rd7
kb4 7977. Qe4+ kb5 7978. Qef5+ ka6 7979. Qf4 kb5 7980. Rd2 rhh2 7981. Rd7 kc6 7982.
Qd6+ kb5 7983. Qdd3+ kc6 7984. Rb8 ba2 7985. Rc8 kb6 7986. Qh5 kc6 7987. Qa5 nc3
7988. Qh5 nb5 7989. Rd5 nc3 7990. Qb1 rh4 7991. Qd3 rhh1 7992. Qdd1 rh4 7993. Qh6 rh1
7994. Ra8 rgg1 7995. Rc8 qh3 7996. Rd4 qg3 7997. Rh4 qe1 7998. Rd4 rg3 7999. Qh7 rgg1
8000. Rd2 kb6 8001. Rd4 rg4 8002. Qf3 rgg1 8003. Qd5 ra7 8004. Qf3 rc2 8005. Qfh3 rcc1
8006. Q7h5 rd7 8007. Qh7 bb3 8008. Rf8 ba2 8009. Rg8 bc4 8010. Rf8 bd3 8011. Q7h6 bc4
8012. Rf4 rgg7 8013. Rd4 ra3 8014. Re4 raa1 8015. Re7 rd3 8016. Re4 rc2 8017. Qh8 rcc1
8018. Rf4 rgg1 8019. Re4 nd1 8020. Qh2 nc3 8021. Qg2 ra5 8022. Qgh2 rgg3 8023. Ree8 rgg1
8024. Q8h3 [rxh3] 8025. Rf7 na2 8026. Qd2 rhg3 8027. Rb7+ kc5 8028. Ra7 rgh3 8029.
Rg7 rf3+ 8030. Qf4 rb3 8031. Qh2 qe2 8032. Qh6 qf2+ 8033. Ke7 raa3 8034. Rgg8 qc2 8035.
Qa6 rbb1 8036. Qc8+ kb6 8037. Kd8 qc3 8038. Qc7+ kb5 8039. Re2 qd2+ 8040. Qd6 qb4
8041. Re1 rf3 8042. Re5+ qc5 8043. Qc7 rfb3 8044. Qg7 rd3+ 8045. Rd5 rdb3 8046. Re5 be6
8047. Qc7 bc4 8048. Rg2 rf3 8049. Rg8 rgd1+ 8050. Qd6 rdg1 8051. Re4 qb4 8052. Re5+ ka4
8053. Re1 kb5 8054. Rg7 ra3 8055. Rg8 rgg3 8056. Re2 rgg1 8057. Qc7 qd2+ 8058. Qd6 qc3
8059. Qc7 qc2 8060. Ree8 qc3 8061. Qh7 kb6 8062. Qc7+ ka6 8063. Qc8+ kb6 8064. Rg4
qc2 8065. Rgg8 rb4 8066. Ke7 rbb1 8067. Qb7+ kc5 8068. Qc8+ kb4 8069. Qa6 kc5 8070.
Qf6 rbb3 8071. Qa6 rh2 8072. Qh6 rhh1 8073. Kd8 qf2 8074. Ke7 rgd1 8075. Rg7 rdg1 8076.
Rf7 ra5 8077. Rg7 qe1+ 8078. Kf6 qf2+ 8079. Ke5 qe2+ 8080. Kf6 rc2 8081. Qh2 rcc1 8082.
Rg4 qe1 8083. Rg7 raa3 8084. Qf4 ra5 8085. Qd2 rf3+ 8086. Qf4 rfh3 8087. Qd2 r3h2 8088.
Ra7 rh3 8089. Qb2 rhg3 8090. Qd2 kb6 8091. Rb7+ kc5 8092. Rf7 kb6 8093. Qg5 rgh3 8094.
Qd2 rg7 8095. Qh2 rgg1 8096. Re4 nc3 8097. Re8 rb5 8098. Rff8 ra5 8099. Ra8 [rxa8] 8100.
Qd6+ ka7 8101. Re8 bb5 8102. Re2 rg6+ 8103. Kf5 ba6 8104. Re5 r1h2 8105. Rc5 rcc2 8106.
Qd4 bd3+ 8107. Qe4 rb8 8108. Re5 rbb2 8109. Rd5 rb8 8110. Rd8 rgb6 8111. Ke5 qh1 8112.
Kf5 rc8 8113. Rh8 rc5+ 8114. Kf4 rd5 8115. Rd8 rbb2 8116. Qg6 rcg2 8117. Rd6 rdb5 8118.
Qg4 be4 8119. Qe6 ra2 8120. Rd3 rab2 8121. Rd6 re5 8122. Qg4 reb5 8123. Re6 bd3 8124.
Rd6 rh6 8125. Qg6 r6h3 8126. Qf5 rd5 8127. Qg6 rh6 8128. Rd8 r6h3 8129. Rf8 rgc2 8130.
Rd8 na4 8131. Qe4 nc3 8132. Rg8 rb6 8133. Rd8 rbh6 8134. Rh8 rb6 8135. Rf8 rc5 8136. Rh8
rc7 8137. Kf5 rc5+ 8138. Kg4 rc8 8139. Kf5 rg6 8140. Rd8 rb6 8141. Kg5 rcb8 8142. Kf5 rb1
8143. Ke5 r1b6 8144. Re8 qe1 8145. Rd8 qd2 8146. Kf5 qe1 8147. Rd5 rg6 8148. Rd8 rbb2
8149. Rd5 rh8 8150. Re5 r8h3 8151. Rc5 rb8 8152. Re5 ra8 8153. Rc5 be2 8154. Qd4 bd3+
8155. Kf4 ba6 8156. Kf5 rg1 8157. Qd6 rg6 8158. Re5 rc1 8159. Rc5 rh1 8160. Re5 rag8
8161. Re2 ra8 8162. Rb2 bb5 8163. Re2 rg5+ 8164. Kf6 rg6+ 8165. Ke5 rgg1 8166. Kf6 rc8
8167. Re8 ra8 8168. Qd8 bc4 8169. Qd6 na4 8170. Rf8 nc3 8171. Qe5 kb6 8172. Qd6+ ka5
8173. Qh2 kb6 8174. Rb8+ [rxb8] 8175. Qf2+ ka5 8176. Qe2 nd5+ 8177. Ke5 rbb3 8178.
Ke4 rcb1 8179. Qe3 rb6 8180. Kd4 r1h2 8181. Qe2 ka4 8182. Qa2+ kb5 8183. Qf2 bf1 8184.
Qe3 rbb2 8185. Qc1 rg4+ 8186. Qf4 rg8 8187. Qf6 rc3 8188. Qh6 ne7 8189. Qh5+ kc6 8190.
Qc5+ kd7 8191. Qe5 rhe2 8192. Qb5+ ke6 8193. Qc5 kf6 8194. Qh5 rgb8 8195. Qc5 rg8
8196. Qa5 ke6 8197. Qc5 rh8 8198. Qb5 rg8 8199. Qg5 kd7 8200. Qb5+ kc8 8201. Qe5 kd7
8202. Qh8 rh2 8203. Qe5 rf8 8204. Qc5 rg8 8205. Qa3 kc6 8206. Qc5+ kb7 8207. Qh5 kc6
8208. Qh7 kb5 8209. Qh5+ rc5 8210. Qh6 rc3 8211. Qd2 nd5 8212. Qh6 rh1 8213. Qf6 rhh2
8214. Qf8 rch3 8215. Qf6 qe2 8216. Qf4 qe1 8217. Qc1 rg4+ 8218. Qf4 rg1 8219. Qc1 nf4
8220. Qe3 nd5 8221. Qf4 rb1 8222. Qe3 rhh6 8223. Qf2 r6h3 8224. Qf5 bc4 8225. Qf2 ka4
8226. Qa2+ kb5 8227. Qe2 ka4 8228. Qe5 ka5 8229. Qe2 rf2 8230. Qe3 rfh2 8231. Qg3 rhh1
8232. Qe3 r6b4 8233. Ke4 rb6 8234. Kf5 r6b3 8235. Ke4 ba6 8236. Qe2 bc4 8237. Kf5 rc1
8238. Ke4 rb5 8239. Ke5 rbb3 8240. Qe3 rb8 8241. Qe2 nb4 8242. Kf6 nd5+ 8243. Kf7 nc3+
8244. Kf6 nb1 8245. Qf2 nc3 8246. Qf4 kb6 8247. Qf2+ qe3 8248. Qh2 qe1 8249. [Qxb8+]
kc6 8250. Qe5 rd3 8251. Qe4+ rd5 8252. Qc2 qf2+ 8253. Ke7 rd2 8254. Qa4+ bb5 8255.
Qe4+ kc5 8256. Qe5+ kc6 8257. Qf6+ kb7 8258. Qh4 qd4 8259. Kf8 kc7 8260. Qe7+ kc6
8261. Qe2 qg4 8262. Qe7 rd4 8263. Qb4 rd5 8264. Qa4 rh3 8265. Qc4+ kd7 8266. Qa4 ra1
8267. Qd1 kd8 8268. Qd3 kc8 8269. Kf7 rgc1 8270. Kf8 rg1 8271. Qc2 kd8 8272. Qd3 qd7
8273. Qd1 qg4 8274. Qa4 kd7 8275. Qd1 rc1 8276. Qa4 rg2 8277. Qc4 rgg1 8278. Qa2 kc6
8279. Qc4+ rc5 8280. Qa4 rd5 8281. Qa1 rhh1 8282. Qa4 rhh5 8283. Qb4 rhh1 8284. Ke7
rd4 8285. Kf8 rd3 8286. Qe7 rd4 8287. Ke8 rd2 8288. Kf8 qa4 8289. Qe2 qg4 8290. Qf2 qd4
8291. Qe2 kc7 8292. Qe7+ kc6 8293. Qh4 kc7 8294. Qh3 kb7 8295. Qh4 qf4+ 8296. Ke7 qd4
8297. Qh6 qf2 8298. Qh4 bc4 8299. Qf6 bb5 8300. Ke6 kc6 8301. Ke7+ kc5 8302. Qe5+ kc6
8303. Qe4+ kc5 8304. Qa4 kc6 8305. Qb3 bc4 8306. Qa4+ kd5 8307. Qc2 kc6 8308. Qf5 rd5
8309. Qc2 qa7+ 8310. Kf6 qf2+ 8311. Qf5 qe1 8312. Qc2 rg2 8313. Qe4 rgg1 8314. Qf4 rd3
8315. Qe4+ kb6 8316. Qe5 kc6 8317. Ke7 rdh3 8318. Kf6 bg8 8319. Qb8 bc4 8320. Qd8 kb7
8321. Qc8+ kb6 8322. Qb8+ ka5 8323. Qg3 bf7 8324. [Qxe1] kb6 8325. Qh4 r1h2 8326.
Qf2+ re3 8327. Qf5 rce1 8328. Qf4 nd5+ 8329. Kf5 ne7+ 8330. Kf6 nc8 8331. Qf5 rg4 8332.
Qh5 rf4+ 8333. Kg5 rd4 8334. Kf5 kc5 8335. Kf6+ rd5 8336. Qf5 rg1 8337. Qg4 red3 8338.
Qd7 ra3 8339. Qe8 raa2 8340. Qh8 rd4 8341. Qh7 kb4 8342. Qe4 kc5 8343. Qg2 rhh4 8344.
Qh3 rag2 8345. Qb3 ra2 8346. Qh3 kd6 8347. Qg2 kc5 8348. Qb2 rh2 8349. Qg2 ra3 8350.
Qe4 raa2 8351. Qg6 kb4 8352. Qe4 ka3 8353. Qh7 kb4 8354. Ke5 kc5 8355. Kf6 re4 8356.
Qh8 rd4 8357. Qh3 rd5 8358. Qh8 rc1 8359. Qe8 rg1 8360. Qe4 ra3 8361. Qe8 rga1 8362.
Qd7 rg1 8363. Qh3 rad3 8364. Qd7 ra1 8365. Qg4 rg1 8366. Qg2 re3 8367. Qg4 rd7 8368.
Qf5+ rd5 8369. Qe4 rge1 8370. Qf5 kc4 8371. Qh5 kc5 8372. Qf3 rd4 8373. Qh5+ bd5 8374.
Kf5 bf7 8375. Qh7 kb6 8376. Qh5 re8 8377. Kg5 r8e3 8378. Kf6 rf4+ 8379. Kg5 rg4+ 8380.
Kf6 ra4 8381. Qf5 rg4 8382. Qh7 rgg1 8383. Qf5 rhh3 8384. Qf4 rh2 8385. Kf5 ne7+ 8386.
Kf6 nd5+ 8387. Kf5 nc3 8388. Kf6 rhh1 8389. Qf5 rh2 8390. Qf3 rc1 8391. Qf5 reh3 8392.

30

Qf2+ re3 8393. Qh4 reh3 8394. Qg3 rhh1 8395. Qh4 rg4 8396. Qe1 rgg1 8397. Qe6+ ka5
8398. Qe1 rg5 8399. [Qxc1] rf3+ 8400. Ke7 rfg3 8401. Qd2 rf1 8402. Qf2 ka4 8403. Qf5
rg6 8404. Qd5 nb5 8405. Qa2+ kb4 8406. Qc2 ka5 8407. Qc1 rd1 8408. Qg5 rd8 8409. Qf6
nd6 8410. Qe6 rgg8 8411. Qe4 r8g6 8412. Qe2 rh6 8413. Qe4 ka6 8414. Qg6 re8+ 8415. Kf6
bb3 8416. Kg7 ra8 8417. Qg5 bd1 8418. Qg6 rc8 8419. Qg4 ra8 8420. Qg6 rc3 8421. Qg5 rg3
8422. Qg4 bb3 8423. Qg5 rhh8 8424. Qg6 rh6 8425. Kf6 re8 8426. Kg7 bf7 8427. Kf6 rg8
8428. Ke7 re8+ 8429. Kd7 rd8+ 8430. Ke7 rf8 8431. Qe4 rd8 8432. Qc4+ ka5 8433. Qe4 rh7
8434. Qe2 rh6 8435. Qd1 rhg6 8436. Qe2 kb6 8437. Qe4 ka5 8438. Qg4 rgg8 8439. Qe4 ne8
8440. Qe6 nd6 8441. Qg4 rg6 8442. Qe6 rb3 8443. Qf6 rbg3 8444. Qh8 nb5 8445. Qf6 na7
8446. Qg5+ nb5 8447. Qe3 rd1 8448. Qg5 rf6 8449. Qc1 rfg6 8450. Qc4 rf1 8451. Qc1 nd4
8452. Qc2 nb5 8453. Qd1 kb4 8454. Qc2 rgf3 8455. Qa2 rfg3 8456. Qh2 ka4 8457. Qa2+
ra3 8458. Qd5 rag3 8459. Qe6 nc3 8460. Qd5 rc6 8461. Qf5 rcg6 8462. Qc5 r6g5 8463. Qf5
bh5 8464. Qf2 bf7 8465. Kf6 ka5 8466. Ke7 be8 8467. Qd2 bf7 8468. Kd6 rh1 8469. Ke7 rh8
8470. Qc1 rh1 8471. Kf6 rf3+ 8472. Ke7 rfh3 8473. Kf6 ka6 8474. [Qxc3] bd5 8475. Qb3
be4 8476. Qc2 rb3 8477. Qg2 bb7 8478. Qh2 ra5 8479. Qb8 rf1+ 8480. Ke7 re5+ 8481. Kd8
rb4 8482. Qa7+ kb5 8483. Qf2 rg1 8484. Qd2 rc5 8485. Qh6 rbb1 8486. Qf6 rbc1 8487. Qd6
rg3 8488. Qe7 rg7 8489. Qe3 ka5 8490. Qe5 rc3 8491. Qe3 rg3 8492. Qe4 ba8 8493. Qe7
rcd3+ 8494. Ke8 rdc3 8495. Kd8 bc6 8496. Qe4 ba8 8497. Qh7 bb7 8498. Qe4 rg2 8499. Qe3
rg3 8500. Qc1 rg7 8501. Qe3 kb5 8502. Qe5 ka5 8503. Qf5 rc1 8504. Qe5 re7 8505. Qe3
rg7 8506. Qc3+ kb5 8507. Qe3 ra1 8508. Qe7 rac1 8509. Qf8 rg3 8510. Qe7 rcg1 8511. Qd6
rgc1 8512. Qf8 rgg1 8513. Qd6 kb4 8514. Qf6 kb5 8515. Qf4 rb1 8516. Qf6 rbe1 8517. Qh6
rb1 8518. Qf8 rb4 8519. Qh6 ka4 8520. Qd2 kb5 8521. Qf2 re5 8522. Qd2 rf1 8523. Qf2
ka6 8524. Qa7+ kb5 8525. Qb8 ka6 8526. Kd7 rb3 8527. Kd8 ref5 8528. Ke7 re5+ 8529.
Kd7 ra5 8530. Ke7 bf3 8531. Kf6 bb7+ 8532. Kg6 rh1 8533. Kf6 rbb1 8534. Qh2 rb3 8535.
Qb2 rg5 8536. Qh2 bc8 8537. Qg2 bb7 8538. Kf7 be4 8539. Kf6 rg4 8540. Qc2 rg5 8541. Ke7
rbh3 8542. Kf6 rg4 8543. Qb3 rg5 8544. Qb1 bd5 8545. Qb3 bg8 8546. Qc3 bd5 8547. Qd4

[7] Axel Thue. über unendliche zeichenreihen. Norske Vid Selsk.
Skr. I Mat-Nat Kl.(Christiana), 7:1–22, 1906.
[8] Wikipedia. Fifty-move rule. http://en.wikipedia.org/
wiki/Fifty-move_rule.
[9] Wikipedia. Threefold repetition. http://en.wikipedia.
org/wiki/Threefold_repetition.
[10] Victor Zakharov and Vladimir Makhnichev. Lomonosov
endgame tablebases, 2012. https://chessok.com/?page_
id=27966.

bf7 8548. Qc3 kb6 8549. [Qxh3] ka7 8550. Ke7 ka6 8551. Kd6 rg6+ 8552. Ke5 bd5 8553.
Qf1+ kb7 8554. Kf5 rg7 8555. Qc1 rg8 8556. Qa1 rf8+ 8557. Ke5 ba2 8558. Qb1+ bb3 8559.
Qf1 bc2 8560. Qf4 rc8 8561. Qa4 rc7 8562. Kf6 bh7 8563. Qb3+ kc6 8564. Qc2+ kd7 8565.
Kg7 bg8 8566. Qc6+ ke7 8567. Qc3 rc5 8568. Qc2 bh7 8569. Qc3 bg8 8570. Qa5 rc7 8571.
Qc3 kd7 8572. Qc6+ ke7 8573. Qc2 kd7 8574. Qh2 bh7 8575. Qc2 bf5 8576. Kf6 bh7 8577.
Qe2 kc6 8578. Qc2+ kb6 8579. Qb3+ kc6 8580. Qd1 kb7 8581. Qb3+ ka8 8582. Qa4+ kb7
8583. Ke6 bc2 8584. Kf6 rh2 8585. Ke5 rh1 8586. Ke6 rc8 8587. Ke5 rc3 8588. Qf4 rc8 8589.
Qe3 rf8 8590. Qf4 rfh8 8591. Qf1 rf8 8592. Qc4 bb3 8593. Qf1 kc8 8594. Qb1 kb7 8595. Qd1
ba2 8596. Qb1+ kc8 8597. Qa1 kb7 8598. Kd4 bd5 8599. Ke5 ra8 8600. Kf5 rf8+ 8601. Qf6
rg8 8602. Qa1 ba2 8603. Qc1 bd5 8604. Qc2 rg7 8605. Qc1 bb3 8606. Qf1 bd5 8607. Qg1 rg6
8608. Qf1 rgh6 8609. Ke5 rg6 8610. Qf2 ka6 8611. Qf1+ bc4 8612. Qh3 bd5 8613. Qg4 bf7
8614. Qh3 rg4 8615. Kd6 rg6+ 8616. Qe6 rg5 8617. Qh3 rb1 8618. Ke7 rh1 8619. Qf3 ka7

8620. Qh3 rg7 8621. Kf6 rg5 8622. Qa3+ kb6 8623. Qh3 kc5 8624. [Kxf7] rgg1 8625. Qg3
rh8 8626. Qb3 re1 8627. Qa2 re7+ 8628. Kf6 re3 8629. Kg5 ree8 8630. Qa5+ kc4 8631. Kf6
re6+ 8632. Kg5 rc6 8633. Qe5 rg6+ 8634. Kf5 rgh6 8635. Qe2+ kb3 8636. Ke4 re8+ 8637.
Kd3 ree6 8638. Qf3 re5 8639. Qg2 rhh5 8640. Qf3 ka3 8641. Qh3 rh7 8642. Qg2 rd7+ 8643.
Kc4 rde7 8644. Kd3 rd7+ 8645. Kc3 rh7 8646. Kd3 ra7 8647. Qh3 rh7 8648. Qf5 rh5 8649.
Qh3 rb5 8650. Qf3 rbe5 8651. Qf4 kb3 8652. Qf3 reg5 8653. Qg2 re5 8654. Qe4 rh6 8655.
Qg2 rg5 8656. Qf3 re5 8657. Qc6 ree6 8658. Qf3 ka4 8659. Qe2 kb3 8660. Qe4 re8 8661.
Qe2 rd8+ 8662. Ke4 re8+ 8663. Kf4 reh8 8664. Ke4 rg8 8665. Kf5 rgh8 8666. Qf2 kc4 8667.
Qe2+ kc5 8668. Qe5+ kc4 8669. Qd6 rg6 8670. Qe5 rd6 8671. Kg5 rg6+ 8672. Kf4 rc6 8673.
Kg5 rd8 8674. Qa5 rh8 8675. Kf5 re6 8676. Kg5 re5+ 8677. Kf6 re6+ 8678. Kf5 ree8 8679.
Kf6 kd4 8680. Kg5 kc4 8681. Qe1 kc5 8682. Qa5+ kd4 8683. Qa2 kc5 8684. Qa6 re3 8685.
Qa2 kc6 8686. Kf6 kc5 8687. Kf7 re7+ 8688. Kf6 re1 8689. Kf7 kd6 8690. Qb3 kc5 8691.
Qa4 rg1 8692. Qb3 rc1 8693. Qg3 rg1 8694. Qg6 rhh1 8695. Qg3 re1 8696. Qh3 reg1 8697.
Qh7 rg5 8698. Qh3 kc6 8699. [Qxh1+] kb6 8700. Qg1+ ka5 8701. Qe1+ ka4 8702. Ke8
rg1 8703. Qa1+ kb4 8704. Qe5 rb1 8705. Qc5+ kb3 8706. Qc7 rf1 8707. Qa7 rg1 8708. Qa1
rh1 8709. Qg7 kb4 8710. Qg2 kb3 8711. Qg4 rh4 8712. Qg2 rf4 8713. Qc2+ kb4 8714. Qc7
rf1 8715. Qh2 rf7 8716. Qh1 kc5 8717. Qh6 ra7 8718. Qh1 rf7 8719. Qd5+ kb4 8720. Qh1
ka5 8721. Qh2 kb4 8722. Qg3 rf1 8723. Qh2 kb5 8724. Qc7 kb4 8725. Qd7 rf4 8726. Qc7
ka4 8727. Qc2+ kb4 8728. Qd2+ kb3 8729. Qc2+ ka3 8730. Qg2 kb3 8731. Kd7 rh4 8732.
Ke8 kc3 8733. Qg4 kb3 8734. Qd4 rh1 8735. Qg4 kc2 8736. Qg2+ kb3 8737. Qc6 kb4 8738.
Qg2 rf1 8739. Qg7 rh1 8740. Qg5 kb3 8741. Qg7 rh7 8742. Qa1 rh1 8743. Kf7 rg1 8744. Ke8
rb1 8745. Qa7 rg1 8746. Qa8 rf1 8747. Qa7 rf2 8748. Qc7 rf1 8749. Qe5 rb1 8750. Qc7 rh1
8751. Qc5 rb1 8752. Qc1 kb4 8753. Qc5+ ka4 8754. Qe5 kb4 8755. Kf8 rg1 8756. Ke8 ka4
8757. Qa1+ kb4 8758. Qe1+ ka4 8759. Qc3 rg5 8760. Qe1 rh5 8761. Kf7 rg5 8762. Qd1+
ka5 8763. Qe1+ kb5 8764. Qg1 ka5 8765. Qg2 kb6 8766. Qg1+ kb5 8767. Qh1 kb6 8768.
Qc1 kb5 8769. Qa1 kc6 8770. Qh1+ kd7 8771. Kf6 ke8 8772. Qg2 rg8 8773. Ke6 rg7 8774.

[Qxg7]

kd8 8775. Qh7 kc8 8776. Qg7 kb8 8777. Kf5 ka8 8778. Qc3 kb7 8779. Kg6 ka8
8780. Qd3 kb8 8781. Kf7 kc7 8782. Kf6 kb7 8783. Qd7+ kb8 8784. Qd2 kc7 8785. Qc3+ kb6
8786. Kg6 kb7 8787. Qc2 ka7 8788. Qc7+ ka8 8789. Qg3 kb7 8790. Qg1 kc6 8791. Qf1 kb7
8792. Kg5 ka8 8793. Qf4 ka7 8794. Qc1 ka8 8795. Qc3 ka7 8796. Qc2 ka8 8797. Qc8+ ka7
8798. Qc7+ ka8 8799. Qc8+ ka7 8800. Qc2 ka8 8801. Qc3 ka7 8802. Qc1 ka8 8803. Qf4 ka7
8804. Qf1 ka8 8805. Qd1 kb7 8806. Qf1 kb8 8807. Kg6 kb7 8808. Qh3 kc6 8809. Qf1 kc5
8810. Qg1+ kc6 8811. Qa1 kb7 8812. Qg1 ka8 8813. Qg3 kb7 8814. Qc7+ ka8 8815. Qd8+
ka7 8816. Qc7+ ka6 8817. Qc2 ka7 8818. Qf2+ kb7 8819. Qc2 kb6 8820. Qc3 kb7 8821. Kf6
kb6 8822. Qb2+ kc7 8823. Qc3+ kd6 8824. Qd2+ kc7 8825. Qc2+ kb8 8826. Qd2 ka8 8827.
Qd7 kb8 8828. Qd4 kb7 8829. Qd7+ ka8 8830. Qd3 kb7 8831. Qb5+ kc7 8832. Qd3 kb6
8833. Kf7 kc7 8834. Qh3 kb8 8835. Qd3 ka7 8836. Kg6 kb8 8837. Qe3 ka8 8838. Qd3 kb7
8839. Qc3 ka8 8840. Kf5 kb7 8841. Qg7+ ka8 8842. Qg6 kb8 8843. Qg7 kc8 8844. Ke6 kb8
8845. Qh7 kc8 8846. Qh6 kd8 8847. Qh7 ke8 8848. Qg7 kd8 8849.

[Qd7++]

1-0

References
[1] FIDE handbook – E.I.01. Laws of chess, 2020. handbook.
fide.com/chapter/E012018.
[2] Max Euwe. Mengentheoretische betrachtungen über das
schachspiel. Proc. Konin. Akad. Wetenschappen, 1929.
[3] Saul Kripke. Outline of a theory of truth. The Journal of
Philosophy, 72(19):690–716, 1975.
[4] Tom Murphy, VII. Survival in chessland. In A Record of the
Proceedings of SIGBOVIK 2019. ACH, April 2019.
[5] Tom Murphy, VII, Ben Blum, and Jim McCann. It still
seems black has hope in these extremely unfair variants of
chess. In A Record of the Proceedings of SIGBOVIK 2014,
pages 21–25. ACH, April 2014. sigbovik.org/2014.
[6] N. J. A. Sloane. Thue-Morse sequence. https://oeis.org/
A010060.

31

5

Optimizing the SIGBOVIK 2018 speedrun
leo60228
@leo60228
April 1, 2020
Abstract
SIGBOVIK 2018 [1] is a popular game. However, very little work
has been put into optimizing its speedrun. In this paper, I will demonstrate usage of pathfinding algorithms for the goal of making the optimal
speedrun of SIGBOVIK.

1

Motivation

I was bored.

2

Method

I first looked at implementations of pathfinding algorithms in Rust. I initially
wanted to use A* because it was popular. However, the API looked complicated.
I found an implementation of DFS to use instead. This DFS implementation
required me to define a start point, one or more end points, and a function to
map from point to points. I implemented all three of these, using an enum of
Start, Success, and Page, because I forgot that there was only one success page.
I used a depth first search algorithm in order to create the mapping. To do
this, I clicked as far as possible along the first choices, adding to my mapping (a
match block) as I went, then backtracking once I reached a cycle or the end. I
then manually removed cycles because I wasn’t sure if the DFS implementation
I was using supported them and I didn’t feel like writing my own.
After I had defined these three functions, I simply used the pathfinding [2]
crate’s DFS function. This gave me an optimal path to program into LiveSplit
One [3], a tool for speedrunning that I’ve seen speedrunners use, so I probably
should too if I’m speedrunning. I opened it on my phone and added the order
that my code gave me.
I recorded my phone and computer screen while I began the speedrun. I
then combined the phone and computer screen and attempted to sync them
from memory in a video editor. I then uploaded it to YouTube [4].

32

3

Results

My speedrun was 17.88 seconds, which I’m pretty sure is a world record among
the SIGBOVIK 2018 speedrunning community. It can be viewed at https:
//youtu.be/VPrT8Y-aRRs. My code will be available on GitHub at https:
//github.com/leo60228/sigbovik2018 if I remember to make it non-private
after SIGBOVIK.

4

Appendix: Code

use pathfinding::directed::dfs::dfs;
#[derive(Debug, PartialEq, Eq, Copy, Clone)]
pub enum Page {
Start,
Success,
Page(isize),
}
macro_rules! pagevec {
($($page:expr),*) => {
vec![$(Page::Page($page)),*]
}
}
impl Page {
pub fn successors(&self) -> Vec<Self> {
match self {
Page::Start => pagevec![47, 177, 205],
Page::Page(47) => pagevec![153, 206],
Page::Page(153) => pagevec![17],
Page::Page(17) => pagevec![35, 135],
Page::Page(35) => pagevec![51, 68],
Page::Page(51) => pagevec![68], // cycle
Page::Page(68) => vec![], // cycle
Page::Page(135) => pagevec![124, 183],
Page::Page(124) => pagevec![116, 88],
Page::Page(116) => pagevec![208],
Page::Page(208) => vec![Page::Success],
Page::Page(88) => pagevec![208],
Page::Page(183) => pagevec![207, 208],
Page::Page(207) => vec![],
Page::Page(206) => vec![],
Page::Page(177) => pagevec![130, 117],
Page::Page(130) => pagevec![154, 39],

33

Page::Page(154) => pagevec![17],
Page::Page(39) => vec![],
Page::Page(117) => pagevec![87, 50, 28],
Page::Page(87) => pagevec![69, 40, 28],
Page::Page(69) => vec![], // cycle
Page::Page(28) => vec![], // cycle
Page::Page(50) => vec![], // cycle
Page::Page(40) => pagevec![178], // cycle
Page::Page(178) => pagevec![208], // cycle
Page::Page(205) => vec![],
Page::Success => pagevec![],
_ => unimplemented!("{:?}", self),
}
}
}
fn main() {
println!("{:?}", dfs(Page::Start, Page::successors, |x| x == &Page::Success));
}

5

Appendix 2

I just realized I probably should have used more sections, so here’s one.

References
[1] Association for Computational Heresy. Message from the Organizing Commitee. In Proceedings of SIGBOVIK 2018. ACH, Pittsburgh, PA, USA,
March 29, 2018.
[2] Samuel Tardieu. pathfinding. https://crates.io/crates/pathfinding
[3] LiveSplit. LiveSplit One. https://one.livesplit.org
[4] Google. YouTube. https://youtube.com

34

Retraction of
a boring follow-up paper to
“Which ITG Stepcharts are Turniest?”
titled, “Which ITG Stepcharts are
Crossoveriest and/or Footswitchiest?”

6

Ben Blum

bblum@alumni.cmu.edu

In my 2017 paper, a boring follow-up paper to Which ITG Stepcharts are Turniest? titled, Which ITG
Stepcharts are Crossoveriest and/or Footswitchiest? (Blum 2017), I wrote of maximum T , XO%, FS%, and
JK% values as follows:
A chart could conceivably end right before such a step, sneaking through some small ε extra
turniness (VII 2014) (similar to the case of 270s in (Blum 2016)),
[. . . ]
By the way, the theoretical maxima for XO%, FS%, and JK% are 50-ε, 100-ε, and 100-ε,
respectively (VII 2014).
However, in the experimental results, Tachyon Epsilon (Matt 2013) placed among the lowest-ranking
stepchart packs in every category, yet I neglected to properly cite Dr. VII’s landmark paper from 2014
at that time. Though we may never know know what, if anything, is Epsilon? we now know at least what it
is not: crossovery and/or footswitchy.
In conclusion, please reject my paper. I messed up on it.

References
B. Blum. Which ITG stepcharts are turniest? SIGBOVIK, 2016.
B. Blum. Which ITG stepcharts are crossoveriest and/or footswitchiest? SIGBOVIK, 2017.
M. Matt. Tachyon epsilon, 2013.
T. VII. What, if anything, is epsilon? SIGBOVIK, 2014.

35

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 18: Retraction of a boring follow-up paper to “Which ITG stepcharts are turniest?” titled, “Which ITG stepcharts are crossoveriest
and/or footswitchiest?”
Reviewer: Reviewer 5/7
Rating: 5/7
Confidence: Supercharged Chevy Silverado

We believe the subject of this paper is of utmost importance to the field, and appreciate its timeliness, timeline, timbre, and temerity. That being said, in order to be accepted into this prestigious
venue, the we suggest the following changes:
• The main methodology is seriously flawed and must be addressed.
• The authors used the word “whilst” 11 times. We recommend a minimum “whilst” usage
count of 20 to be seriously considered for publication.
• The English language writing in this paper contains numerous spelling and grammar errors.
Consider kidnapping a native English speaker, chaining them in your basement, and depriving them of food until the draft is written in language that William Strunk would be proud
of.
• Please use raster rather than vector images for all figures, so that the reader may count the
pixels used in the each letter of the text in the axis labels.
• In order to give due credit to all contributors to this work, please make Reviewer 5/7 a coauthor.
• On page 44, line 13, eight word, second character, we found the letter “o.” We like this letter,
please leave it as is.
• Additional changes have been suggested in the attached PDF.

36

Programming Languages
7

Ntinuation copassing style
Cameron Wong, Dez Reed
Keywords: continuation, ntinuation, control flow, programming
languages

8

Formulating the syntactically simplest programming language
Brendon Boldt
Keywords: proramming languages, compilers, formal grammars

9

Type-directed decompilation of shell scripts
Spencer Baugh, Dougal Pugson
Keywords: partial evaluation, type systems, shell scripting

10

Verified proof of P=NP in the Silence theorem prover language
Matias Scharager
Keywords: soundness, P=NP, theorem prover, Silence, programming language theory, type theory

37

7

Ntinuation Copassing Style
Cameron Wong, Dez Reed

Abstract—We applied the categorical technique of “do
that thing, but backwards” to continuation passing style
to produce a novel programming idiom utilizing the COME
FROM control flow operator.

(’a -> ’b) -> ’b, which has arrows. Entering into
the true categorical spirit and flipping the arrows, then,
suggests that “copassing” a ntinuation must be typed
at (’a <- ’b) <- ’b. Unfortunately, this doesn’t
mean anything, but it is fun to write and look at.

I. I NTRODUCTION
Continuations are an important abstraction in the
analysis and description of functional programs. By
manipulating them, a savvy progrmamer can express
complex control flow precisely and unambiguously. Unfortunately, with great power comes great responsibility,
and code written in continuation-passing style is often
obtuse and unreadable to those without such arcane
knowledge. It is even said that CPS is merely an obfuscation technique designed to terrorize novice functional
programmers and has no practical benefit.
Of course, obscure intermediate forms and
programmer-facing implementations thereof need
not be restricted to functional programs. A reasonable
analogue may be the vaunted Static Single Assignment
form, in which the programmer may need to use
mystical ϕ-functions. As it turns out, however, SSA is
actually equivalent to CPS and is therefore cheating. No,
to achieve true parity, a different approach is needed.
Towards this end, we take a page from category theory,
applying the time-worn approach of “flip everything
backwards and see what happens”. Monads embed impurity to a pure language, so too can comonads embed
(enforced) purity into an impure language, and thus
“ntinuation copassing style”∗ is born.
A continuation is a typesafe abstraction of the “return
address”. In other words, invoking a continuation is a
type-safe delimited GOTO. A ntinuation, then, is the
opposite of GOTO. For wisdom as to what, precisely, this
entails, we turn to the programming language INTERCAL. While INTERCAL does have a form of GOTO,
it also implements the COME FROM operator, which is
precisely the opposite of GOTO!
What is copassing? The type of a CPS function
in a traditional functional language typically involves
∗
Categorically-inclined readers may protest that we have not
properly complemented the “style”. The authors believe that code
written using COME FROM is already pretty much the opposite of
“stylish”, and thus is already complementary.

II. N TINUATIONS
In continuation passing style, the core observation
is that, by passing the continuation as an argument,
it allows the callee to manipulate it to construct new
continuations and call them as necessary. In this way, the
state of the overall program after any given subroutine
invocation is neatly encapsulated into the continuation.
To flip this around, then, a ntinuation would be to
increase expressiveness by removing control of not only
the state of the program on subroutine return, but also
the condition on which the return occurs.
A NCS-transformed function may contain no local
variables, but instead relies entirely on its parameters
and global, mutable state. The ntinuation, then, is subject
to an exit condition imposed on the global state of
the program. This process is known as “copassing” or
“catching” the ntinuation. † .
This construction mimics the vaunted COME FROM
control flow operator from the INTERCAL programming
language in that a ntinuation represents a “trapdoor” that
wrests control flow from the encapsulated routine on a
given condition being met. In the true imperative spirit,
ntinuations are given meaning entirely by global state
and the manipulation thereof.
III. E XAMPLE
Here is the factorial function, written in Ntinuation
Copassing Style psuedo-code extended with ntinuations
and subroutines:

ROUTINE fact:
PLEASE NOTE that the input is passed in the glo
1: SET y TO y * x
SET x TO x-1
PLEASE COME FROM (1) WITH y WHEN x EQUALS 0 IN
†
An initial draft of this work was titled “termination catching
style”, which perhaps more neatly encapsulates this idea, but after
much debate, we settled on “ntinuation” because it would be funnier
watching people attempt to pronounce it.

38

In true INTERCAL fashion, the line beginning with
PLEASE NOTE is a comment, as it begins with the text
“PLEASE NOT”‡ .
This is the standard iterative formulation of the factorial function that multiplies the global result variable y
by a loop counter x until x reaches 0. The key difference,
however, is in the wrapped recursive call to fact that
declares a new ntinuation which aborts computation and
returns to that point.
Note that, should multiple exit conditions apply at
once, it is nondeterministic which trapdoor opens, as in
the original formulation in INTERCAL. In this case, all
trapdoors lead to the same place with the same condition,
so they will take control one after the other in some
nondeterministic order, but do nothing before yielding
control to another trapdoor. Finally, after all trapdoors are
opened, the routine halts with the value of y containing
the result of the factorial.
IV. C ONCLUSION
We have presented a novel control flow operator in
the style of continuation passing style by attempting to
“reverse the arrows”. In fact, the idea of conditional
trapdoors (encapsulating COME FROMs) mimic that of
pre-empting interrupts that take control from a process.
In this way, just as CPS allows for precise manipulation
of control flow by manipulating a subroutines’ next steps,
NCS does the reverse by moving all control flow into
global condition triggers, allowing programmers to focus
on the individual steps of a given operation. We expect
this tool to become key in an imperative obfuscator’s
toolbox.
In future work, we hope to explore whether, as CPS
forms a monad, NCS forms a true comonad (in fact, we
believe that NCS also forms a monad, but we did not
confirm this).

‡

The authors originally intended to give this example in an
extended INTERCAL, but gave up after four hours of whiteboarding
a multiplication algorithm.

39

8

F ORMULATING THE S YNTACTICALLY S IMPLEST
P ROGRAMMING L ANGUAGE

Brendon Boldt
Language Technologies Institute
Carnegie Mellon University

A BSTRACT
The syntax of a programming language is one of its most visible characteristics, and thus, heavily
shapes how users both interact with and view the language. Abstruse syntax correlates to lower
usability, and therefore, lower adoption of a programming language. In light of this, we present a
programming language with the simplest possible syntax. In addition to the language specification
we give a basic style guide and reference implementation.
Keywords programming languages · compilers · formal grammars

1

Introduction

As computers become further and further prevalent in modern day society, computer literacy similarly grows in
importance. Taking the analogy further, composition is to literacy as computer programming is to computer literacy. A
novice programmer will often find it difficult to address the syntax of a programming language at first, which hinders
the acquisition and fluency with the semantics of the programming language [6]. In order to lessen this barrier of entry,
we will explore a programming language with the simplest possible syntax.
The complexity of the syntax of a grammar is dependent on a number of well-defined factors: number of rules,
cardinality of unique terminals (tokens), and number of terminals and non-terminals per production. These are all
further characterized by probabilisitc factors, namely their distributions and associated information-theoretic entropies.
Determining the equivalence of two grammars is an undecidable problem, and the computation of the entropy of
grammars and their syntax trees is an open area of research [7]. Thus, these considerations are beyond the scope of this
paper. Regardless of the formal specifications of complexity and entropy, we assert that fewer unique tokens, fewer
rules, and lower branching factors all correspond to simpler syntax.
The core concept of our programming language is that of simplicity. Fittingly, the name we have chose for the
programming language is “SimPL” standing for “Simple Programming Language”.0 We present the formal specification
of SimPL 2 in Section 2.2
1.1

Related Work and Scope

SimPL bears some resemblance to so-called “esoteric” programming languages or “esolangs.” Such programming
languages typically try to maximize goals, such as minimality and creativity, that typical programming languages
typically leave by the wayside. Two well-known esolangs are brainfuck and Unlambda which are a Turing tarpit and
Church quagmire respectively [2]. Such tarpits and quagmires seek to limit the semantics of a programming language
to the bare minimum as a demonstration of how little is required to be computationally universal.
Another direction of programming language minimalism comes in the form of one instruction set computers (OISCs)
[1]. These are specifically machine languages which provide only one instruction (operator) which, in turn, can take
0

This is not to be confused “SymPL” or “Symbol Programming Language” which is the true name of APL.
The astute reader will notice that we have skipped straight to SimPL 2 (typically, 1 comes before 2). For an explanation of this,
see Footnote 1 (although 2 has preceded 1 in this case).
2

40

variable arguments. While OSICs and SimPL both center around the idea of “unity” in terms of representing a program
in a language of some sort, SimPL does this at the level of a programming language while OSICs are, by definition,
concerned with instruction sets.
While the aforementioned languages are mostly interested in testing the limits of minimalism in computability (either
through limited semantics or instruction sets), our work adheres to area of language design purely with respect to syntax
and syntactic usability. Thus, rather than introducing a language with entirely new semantics as well as syntax, we
will recycle familiar semantics packaged in a novel syntactic model. In this sense, our language bears similarities to
transpiled languages such as CoffeeScript or Dart.

2

Language Specification

2.1

Syntax

We begin with the Backus-Naur form of SimPL.
hstarti ::= hexpri
hexpri ::= htoki hexpri
| hemptyi

Note that there is only one lexical token SimPL, which can be represented an arbitrary charcter, emoji, or any other kind
of thing. While we give the BNF of the grammar here. We have artfully crafted the syntax such that it does not need to
be expressed as a context-free grammar (Type-2 on the Chomsky hierarchy). We can, in fact, express the syntax of
SimPL with a finite state automaton (representing a Type-3 grammar). While this specification seems small, one could
imagine it smaller, but we ran into serious problems attempting to use a simpler specification than presented above.1 For
example, a string can be determined to be within the grammar using the following regular expression (Perl compatible):
.*
Figure 1: Perl-compatible regex specification for SimPL.
2.2

Semantics

The semantics of a programming language are concretely expressed in machine code. This transformation is achieved
through the compilation of a programming language. The relationship between the source code of different lagnagues
as well as their machine representation can be represented as a simple subcategory of Set. Namely, our objects will
be the set of source strings for a given programming language with a special object for the set of all machine code
representations. Morphisms from source code objects to machine code objects are realized by the process of compilation.
If we take every programming language to have a canonical compiler implementation, assume that decompilers do not
exist, and consider only one machine architecture, we can view the machine code object as the terminal object of our
category.
We will use SC and SS to refer to the objects of the C and SimPL languages respectively and M to refer to the
terminal object corresponding to machine code representations. kC : SC → M and kS : SS → M are morphisms
that correspond to the compilation of a programming lagnauge. While both GCC and Clang could be seen as different
morphisms from SC to M we would consider the languages to be distinct since the same source strings correspond to
different machine code representations.
In this paper, we focus on simplicity of syntax, and in order to keep other factors constant, we tie the semantics to
a well-established reference point, namely C. To express this more succinctly, we will introduce the term semantic
equivalent in C or SEC. A SimPL source code string corresponds (semantically) to a unique C source code string—this
C source code is the SEC of the SimPL source code. We formally define the semantics of SimPL as kS = kC ◦ rS
where rS is one component of an isomorphism between semantically equivalent C and SimpPL source code. The
components of this isomorphism are rC : SC → SS and rS : SS → SC such that rC ◦ rS = idSS and rS ◦ rC = idSC .
This is the illustrated by the commutative shown in Figure 2.
The expressivity of the syntax of C an SimPL may seem too widely disparate to be practical, yet we can actually define
the isomorphism between C and SimPL source code. In particular, we will call this isomorphism radix representation
1

See Appendix A for specifications that are not 2 SimPL.

41

SC
rS

kC
rC

SS

M

kS =kC ◦rS

Figure 2: Commutative diagram illustrating the category-theoretic definition of SimPL’s semantics.
00000000000000000000000000000000000000000000000000000000000000000

(a)

ROhNGKVUZH0cQvoy5jBG0tFLNkeFPS9cO7oGlL5DOjVlU3l1M2WxQcggM1x3VKv3e

(b)

l\8+mm;/Q9F1e:hs1+SP1xQLW3&[Mh?ohJMX47zaGO˜?3Vp‘maU’onby(i,ˆp_r#,

(c)

Figure 3: Assorted SimPL source strings for the SEC A.
mutation (RRM). As C can be represented as a sequence of ASCII-encoded bytes, We can express any C program as a
radix 128 number (as standard ASCII values can be represented with 7 bits) with each numerical place corresponding
to an ASCII character in the C program. SimPL, on the other hand, is represented by a radix 1 number. Thus, we can
morph C source code into SimPL as follows.
n−1
X
i=0

ci · 128i = N → 00 01 02 . . . 0N −1

(1)

Similarly, given a SimPL program of length N , we can generate the SEC where the zero-indexed ith character
corresponds to:


N
mod 128.
(2)
128i
2.3

Syntax vs. Semantics

It is worth addressing briefly a common objection to our delineation between syntax and semantics. The objection is
that SimPL’s syntax is simply a trivial shell and that the real syntax (that of C) is masquerading as the “semantics” of
SimPL. Hence, the true syntax of SimPL is just as complicated as that of C. One illustration of the objection consists in
the fact that SimPL has no “syntax errors” per se but instead mutates C syntax errors into SimPL “semantic errors.”
In response, we first point out that there are many different levels of errors beyond syntax and semantic ones. In roughly
ascending level of “depth” we have: lexical (errors), syntax, semantic, logical, and design-level errors (though this list
is not exhaustive). Although these levels are not strictly defined, one of the sounder heuristics for determining the level
of an error comes from looking at from which stage of the compiler the error comes. The fact is that syntax errors in the
corresponding SEC would be generated by the compiler backend after parsing and AST generation, and thus would be
out of the scope of true “syntax errors.”
Furthermore, in some cases there is not even a clear distinction syntactical and semantic errors. For example, in Python,
using return or yield outside of a function yields a SyntaxError. Although trying to return or yield from outside
of a function is a semantic error rather than a syntactical one, this difference is not maintained in Python [3]. Further
debate as to the precise distinction between syntax and semantics could provide useful topics for future work but is
beyond the scope of this paper.

3
3.1

Representation
Naïve Representation

The naïve representation of SimPL entails representing each token as a single character. For example, representing
the SEC A in SimPL could take the various forms specified in Figure 3. It becomes evident when we give the naïve
representation of the SEC AB that this mode of representation quickly becomes unwieldy; for formatting purposes, the
SimPL source string has been rendered in Appendix B.
As is the case with all programming languages, syntactically correct does not imply readable or maintainable. Thus,
we present some potential stylistic improvements in order to reduce cognitive load when determining the number of

42

.........|.........|.........|.........|.........|.........|.....

(a)

.........10........20........30........40........50........60....

(b)

.........ten.......twenty....thirty....forty.....fifty.....sixty.

(c)

12.4...8.......16..............32..............................64

(d)

...............................................................65

(e)

Figure 4: More easily readable SimPL representations for the SEC A.
# i n c l u d e < s t d i o . h>
i n t main ( ) {
p r i n t f ("% s \ n " , " H e l l o , a r X i v ! " ) ;
return 0;
}
Figure 5: A basic “Hello, arXiv!” program in C.
characters in Figure 4. The choice of using delimiters every 10 is somewhat arbitrary. This is certainly appropriate for
situations where the program length is not many orders of magnitude greater than 1 but could be counter-productive
otherwise. Thus, we could turn to constant multiplicative spacing between delimiters instead of constant additive
spacing.
While these intermediary delimiters can make it easier track one’s place in the source code, generating such delimiters
goes beyond what is strictly necessary. For example, we can simply express the number of tokens at the end of the
program.
3.2

Practicality

In a more practical example, take the basic SEC of a “Hello, arXiv!” program show in Figure 5. The above program
consists of 86 characters; thus, the naïve representation consists of (27 )86 = 2602 tokens or about 2 tebiyobiyobiyobiyobiyobiyobiyobibytes (if each token is represented by 1 byte). More generally, an n-character SEC requires (27 )n
bytes to represent.
At very large-scale code bases, simply comprehending the scale of the representation (let alone actually representing)
becomes difficult. Take the Linux kernel, it has on the order of 2 × 107 lines of code [5]. If we use the estimate of
an average of 40 characters per line, that gives us 8 × 108 characters. The SimPL representation of this would, then,
8
9
require on the order of (27 )8×10 = 26×10 tokens to represent. Any explicit representation of this number of tokens
surpasses any current or theoretically possible information system. Although this exhibits exponential growth, this
number is not very large in the context of mathematics, being 2 ↑↑ 5 < nLinux  2 ↑↑ 6, but this number is still large
enough to make pursuing more efficient modes of representation prudent. Thus, while this mode of representation
works well for explaining the conceptual grounding of SimPL, a more efficient mode of representation is needed to
effectively store and process SimPL.
3.3

Compressed Representations

Let us revisit Figure 4e, namely the representation which consists of a repeated, non-numeric character followed by the
number of total tokens at the end (expressed as a radix 10 number). By observing this style of representation, we can
see that number at the end by itself (i.e., without the repeated leading characters) could serve as a sort of shorthand
representation of the whole program. This sort of shorthand makes the task of representation far more tractable. For
example, simply using a radix 10 representation for the “Hello, arXiv!” SEC would as presented above would yield
dlog10 2602 e = 182 characters in total.

Using a higher radix could give us an even more compact representation; for example radix 64, common in text-based
data transmission would give us dlog64 2602 e = 101. If we take this even further, we could use the cardinality of Unicode
12.1 characters which stands at 137 994, which leads us to an even more compact representation dlog137,994 2602 e = 36.
Although, this becomes unwieldy not by virtue of its length but on account needing to have complete familiarity with

43

# i n c l u d e < s t d i o . h>
i n t main ( ) {
p r i n t f ("% s \ n " , " H e l l o , a r X i v ! " ) ;
return 0;
}
Figure 6: The Canonical SimPL of the SEC shown in Figure 5.
every Unicode character. Thus, the optimal representation will strike a balance between compactness and recognizability
of the set of characters used.
“Canonical SimPL,” as we call it, uses a radix 128 representation of an SEC where each digit is rendered as its ASCII
equivalent (e.g., 65 as A). An immediate issue seems to arise from the fact that there are 33 unusable values (either
non-printable characters or unmapped values) in the ASCII encoding [4]. Yet due to RMM with C, any semantically
valid Canonical SimPL source code will only ever correspond to printable ASCII characters. In this way, just as RMM
provides us with an isomorphism at the machine-interpretable semantic level, Canonical SimpPL provides us with a sort
of isomorphism at the human-interpretable level. As an illustration, we have shown the same “Hello, arXiv!” program
written in Canoncial SimPL in Figure 6.

4

Reference Implementation

In our reference implementation of SimPL, we limited our scope to compiling Canoncial SimPL source code. This
compilation, in fact, can be done entirely with a typical *nix toolchain. For example, for any given Canonical SimPL
program lorem.spl, the compiled binary can be generated as such:
cp lorem.spl lorem.c && gcc lorem.c -o lorem
This represents the GCC dialect of Canonical SimPL; the Clang dialect would implemented similarly.

Acknowledgements
Special thanks Dr. Bartosz Milewski for informative YouTube lectures on category theory.

References
[1] In Esolang. https://esolangs.org/wiki/OISC
[2] In Wiki. TuringTarpit
[3] Guido van Rossum et al. 7. Simple Statements; 7.6. The return statement In The Python Language Reference
https://docs.python.org/3/reference/simple_stmts.html?highlight=syntaxerror#the-return-statement
[4] W3Schools. HTML ASCII Reference In HTML Charsets https://www.w3schools.com/charsets/ref_html_ascii.asp
June 16, 2019.
[5] Linus Torvalds. Linux kernel. In https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/. commit hash
0ee4e76937d69128a6a66861ba393ebdc2ffc8a2 (from “master” branch on June 5, 2019).
[6] Andreas Stefik and Susanna Siebert. 2013. An Empirical Investigation into Programming Language Syntax. Trans.
Comput. Educ. 13, 4, Article 19 (November 2013), 40 pages. DOI=http://dx.doi.org/10.1145/2534973
[7] Werner Kuich, On the entropy of context-free languages, Information and Control, Volume 16, Issue 2, 1970, Pages
173-200, ISSN 0019-9958 https://doi.org/10.1016/S0019-9958(70)90105-1.

A

Earlier SimPL Iterations

SimPL 0 was described by the empty grammar. Though the syntax was very simple, the semantics of the program were
static, that is, SimPL 0 could describe one and only one program. We found these semantics to be unreasonably limiting.
Thus, we decided that a non-empty grammar would be necessary.

44

hstarti ::= htoki

Simple 1.0 introduced having at least one rule in the grammar. Although it now conceivable that program could be
written according to the grammar (namely a single token), there is still only one possible program. In this way, SimPL
1.0 is equivalent to SimPL 0. We proceeded to Simple 1.1 as such:
hstarti ::= hexpri
hexpri ::= htoki hexpri

Although programs can now consist of multiple tokens, the only valid program is in fact one consisting of an infinite
string of tokens. In this way, SimPL 1.1 is equivalent to SimPL 1.0. Finally, we added the grammatical option to
terminate a multi-token program which we present as SimPL 2 as described in the body of the paper. In short, SimPL 0,
1.0, and 1.1 are too simple but 2 SimPL is not.

B

C Source Equivalent of AB

iwiNLHDAdMgA7hrgfoM6BNyYLXJYBoY916TUl1FdKL2uFRprQRJ7O3M8EbXfw3KK99dZE5Aby3GTQGtvLf6LXDr9O
AbDXjd367WY47Zra42aAvKMOaW3bA7WjyU1vnhV6LdwUmjCNB9dOq4RhfPnJ0KDe9fwMuPg78AMK4UT8VomZGUu9Z
CmeyoWo5OGNRVclcxVwVVhuJdj0lVuJuyAGZPzsayD8g7FhX6Gug2tHYHDblNQaoTCjKYpDa4Hx245qRneGLy7pDE
19rW6gtW1dgMQK3trBU7E69jSs6hXPqKBgMAdDtiaW5i8N5of91JopS7lqEaScxPGrfVxqEdrcfyD8mNyuFicrego
kiTQCVScfQW4OznWZuBbZCnI8kiZS91pWpMGEo6aydmw0A5A8CWRczpFBCjLvSd6NHIFwXoNkplmBI0yzG3d0Girs
2biZjHylBbZacd4BIp4P21zOp6qmt9O9K7T1ovEwfeFiwtPk0lFm5OJJkDCouNgZoBYJL1bEAMuo2C4q4cpwfU5YO
AfxUvv5tzjCPDxuev6jMMlPfdzib0SAs9i9lRInkqen7MCFIXYdtS8MsZvNfW5ZU0tTKrmUaoVlAlo19pQecjPgFP
SZg4W4TWILFAAuckUjOUh7dmcSNFWE6VLaIhbZVPA9iRmIuj03KKG1L8HaF6LtBXu8Et0m3pHPNjQ98u3HemaWwse
VMqKGwBEXwTzksVcBUQRUUZHh9dN9yhs4bF2DCdogb2suzRb5btMBSVltu1T2EO3THzmu6IebEmL7gVvvYmQw0frk
RTreSj5IoHT5gr13XwH8GB73DIh5l14n77CmDzCQCIhQJvdRKA1fR1mDPypEai1hmC0qeHYEoG8CpdT5Rah66DWva
ou6hEqrsxQXvxYZ627Mn1heglNz205BgaJI4b1WWM5gs1VALJ5UZJHmObSEqbizP3Glszq4JDmBIjDdJQfH0cDlLX
LsEzi1s2xMyj8LR6KgkGRlXLrbN2RVSfsADadMqsguERxxMUzlwNALOg6Ev2noQRNJ7pSkQaFVI7yDt3J84dWXY8A
qE9J8jbyVqAbdAeRsz34UIUuukXMAynFP1PB1O9PQ7DMS0gn5VDLoHwfbLvc5ngaD6KiST1MLVYVEHjnuLIo3oFAt
RrqBTFlNDnr2ehsGmZOGQ5OqjniUAG6oeixbn8AF1Pfh4Feg8zNP2MZqOjViEar8ZZaPTil4NuZt5NHWhMzjUdIST
ATiijNXUmXxCMhZrxgHSd4RHtDLKfn3f7oYGRuSNU8NOnR8M0kHLlbjl8IvxTNGmLL8cUovBEquYcDKL5V1vklwQg
aaP5CjKLoqANY1SsPTrLw5DJKXTqJDlC4u8HzNiJTT8tAHyUVM6uSXuUOhTFpFhNgrwNAwROIsgYKNUMG4Ak2dY94
vCuh3kPxTXu4akOMFDTnqfn8zod0Zhj82qiNBcfeq3mskjJTeBrjklRTPo4b1jdznouI6DoFn27CHeWGCuzbvfpuI
Wne4XFCkEk8V0ZXuPh4qSgHLTLBFVMwAU5ATzqO6zYyN6jx62RVmWHlUBIqsHM6cMSojSvT0rP95HTy7d6PdPQrUB
GlM30g4LYqYbYQO1KaDa5vjvBwHIo4SucjbCmbWw4N12DqsxaTuVpVq6MHuHQ7aoJxvBFs176j6D3Hz1Ongr9aHF8
OtJWR11rqNsgvIPRsdfMKtBPKwa5yXE0IYAk9vwhZCws6LL8hZQzEwAlisUm2Lqe4Jj8zAV6ybTa7SKBFcj8vRDjv
ITyciIK79ZX3S32PMS7AJTtonE8iEjcHxuMLCseB8w0Zwwm4FnPqnb6UtO9tPNZhdoyGu1UBzURVnYWu8HLqsT5sw
2h91ugywjhp63sJUcGJh4lXFPQ1AjEnhLF2s2dIdMLEZMYJMDzUDLbewuk4xTxH6Wg31sQCFWUyuKVLpi2Q3EK62x
kQeBMJH3XOFSkDOdvOkS03TZPBmzDrUmmrxmyjro9LoVNGnllXDZXFVmHeK5ux5cXr7VMF4mW1kf9TfXReLf31B5W
LFELxnRSE8ptpb8TtIHJOIVQaaLMNeBUDdqejSPQ0i0BXdYasLabblBm1O16BPJ0nlNO31JLnbe6rHmGtOkvlm1Fl
4rGjI86vdZE58Qr6Ej4edaF6aQ6XjRyOtNfYLDUJ26Gzm1haMDjToAYyyh8ccIJleBTpw9D21EAeYaOK6ERhfxMXi
qzBvyMrYMJben7uUmcGOCbDYIAWHZbmPu5eOrkXOSjc6snOsQPUCALcBnFs3BMZsA40BFXr68Rq3OgRCkN6MXp5HV
9AkIbWcNJGAIDOeUSiNSwTgZwnMVixqbCeaDJcyW0ftereVahD6Qzzicxtba4z6e8iwuoSiWJ0FJ5yuGeEasxYXec
XDMCky7mCLXsajHAyT7RafcB18klOa9AFccQCV9P1AXzGTOpEm7fHC4Jz8AoqDW5I0zAihVcz8vpsT3oijH6UWM9B
3AbSExDFnu8L63dj98d8LxiPgbGoOqEj6sbkkzZfMpE0fbU0HiXDIsgpLhB1MhTPWy8uq0va2JVsCcBGBA661Md8u
BdCwFnPltRlZv1Cydc4BoNEY7mvJwbiRiu6pKcyHi5lBHryhSZGacpa2jMBvoF3FKuJFZItGNkuTttELdkGaZiJeD
xKHGOjS8FP0PGR6eTJfNGmqg88YhD8uJv1QM8bdKpwtnGU3Uw3o3Ywi3eyAEOYb7jeaAM1CgiMvMSf55HNO6GW6Ce
CUW97LzqBD4LqZ28uzi9rf87aXpb5yV2oTsqbbjeHykRujW3SGBobfIjAEvbm7mFd8GzNgVD89hVihSMEJseS18zz
Pk4wC0a4bV81AMxbxsFTdMf0ZJ9CqyMRG7WwE53zNN5kiYGt9oRcsHexYDFhbAtqOVQsxyIwCoUjD6K8Zv64WhrzX
LhHQu5iDa6BR5s6WmUtGQfm70Y9My5YUBZi1alHSRFCRT5MGJ5yCt6NaLsg6iRIFpeMa1i1w7YjjbB48txKBB2Lvw
kKNqlFwUXnbjXIf1mPRuQiPhNR0DLSA7vXmCNe4LQm5AHYphj6jzbL06MnuxmehSKbNVwKWE0paM768xVOQ82ejzH
CP3eUZKrKeClF0t2ONf6kqW2dC35PqhrmKeD6lvuBdYNSGqhcXI1PRTc8TAoEtgxGefzgsW6TFiP7qq4DnhC5yyJd
CFa6Hq0UBJjLJgw3dR09GB81TXqTOabvhXJPSNkg3joVH7RNQyGFLPBvDGvccQygRHGgPURhIt9O9N8nkp21YAe7u
2CKuhTHXe7ntXjXEcWNNAUtRX5IRN7sjVUUhYmpsvBIMPvJEfTEUTm6s7pI0nOpdempLPlN1S7JKszNCcxA1747Ax
JzsoOYUX0PuJTi9qSqKywZ1ou6NI8Y4KgmFqPrOln05qumlxkELWGlahKG2k1bE5Kswzo210bpk7Ya1lhho83qNqD
RMakqHhoXtTzTqBKX7YqCetWOq4ULn8FgnBbWEMgAWjXXiDbFySezSusRIkPp5Jxa5T1X3TI3jGe5fqK1UewBEDxx
Y41fNjHUHFyKdDPfKcJvygva7Tywb8noeotf8CTrrfqojS3LBjrygeWmu97s31qYg2ZCYddYkC9jiAH7OhqCaKBMn
z7T1HJKQYJeT5ZA1w7fI5KqQpAimBWKHEKaDqHBPiqPY6dvYwfA78hNdsH7HGQSHYFH9UwiX9Vqu840yCEQSKdctJ
UQFkcd6cleB8FSMj7EsjykKKA6koioHTZduyUfiAgJffAasDeGXDekD6SlrZU3phWU5QgnrJUvEZakAAQgrOiTvnD

45

hHAoNS4DMrZNOzOPDoQlgYj7OZKisvKmZ25cfwfh1F0qkHrQAaqojHz2b1yj0MgCc1YuybMiRgrBjT1Q9GZnJVgqB
cyWRJ9CryzM6TP4UiYxi5je29vjnevXrwqPjkKftYk1UzbPX8Tbr4bDv25vMGycQEsspOB3PNeOrqaIcRtYM3AirB
UROVOBzI3AUOuJI71lEaCU9y6NwJuhKWUjVAiTA6ncb9vg2Ok30h9ojCnmV15aEiPLqixjeJt0Yg9pHCj124INZUH
jUBEFEb8yTIJFufHxQVB1XMwZFAQLFk48t4p16e6E2kASqggtdoxfyv7lAD83bQZ3gMJefcvAjM6CmghyjPjMipTY
XhscKk4dTQFYLIwqFINGWuJqJYwSoALO0qLQFZnyWv1BJiV7enDf3kEQXA3lNFuqfLdpVxBOIYbrsuSWtFlVI5gti
F29VuYarLAMSrnD4vHvxLxvClfJARmEnvdpIWgaNAoI8vYFoanQgsYUL5axFHSGdZclI6hXAIf9Wy7fC2DVPOXI8y
rc0clHIuSSBkMGEgnZmIGDwBr8Qd1EIoIJqT0jsbxPBLiwj3UwCm0v0m8sYW0dwvfZZOm0zLChSPoHBiWcywmBfkQ
oOLLQq2GXxF6G9vBQzv7SSgWrSqKR98g8JUXto1uDf03DCnPw93UPBzolptFkXrIW6RdLb3FnciZAWa0qMNEFwTjj
jMkJmKZgK7pSJjm3jyyHkR2exTzvCtvnIAdlJbcRyKRHk868VLMh3CziVVA82QeN7hKVZUy1gj1AykCgTnFK1G5ZP
ArnyzopJJzjdOwlbSilRNVlELfjjNLBtfslqdir0lOnQFc6NdBwUEBQKwx0aNxBZY9ZtSmK7OicMZvxL4zynY0Yxy
BnpeUBD3iRNXp1o4NAdZGmoy9jVgCyastLX2NKBskZ02dtvxCsVOM5FmnxQ7cPshl9xPCrFzY6CdXFai1aZ8NhU6y
JGcRaPiRTY4L21bFg6YtPaUdxENHfEXqAgzZDwUuaGqvfGDKbLEntXKAhu0rFVe7kL6YhlwBhZOGg2XGDHdQGJOo8
e5lMz6LeqfasIfXJpanrFfx5MbItGrygeyO33QOq73yDK8HMsT45ZCGmC88a74bibczxrAFpItkks6WwQXPuKiRxC
iaajqhZaa98or7h8q08jAgEBWIZkqpcMSoMIOJSGfI5qTCMM7xkwKdgnU9l6NYSur5uDzeVWh8j1N6GfRyGpuXF6e
gYIOPnAEUC95wxWYK78TEmfhLdNBvZOfkNAM064OR7EWR4xyyCquBUecqvI4Orm6eWgD5A92M3Yeuu8nKcfvBG9jt
1NXwKiRF8OMAG3UVrhGvodPY1yCZvGgoLYZwW0O2Pt2wPpDMwlIl1OHVB9WuLFSJsnmAUJxklU068c1AUK80y8MHH
7wYsIR8BIliNyaYmgVMt2wUrZYU4qi7sPZQcvBs19vRdToqpEANv5eoNUtPZIb6ejc951XiERnZUWuKUSRlMpJLev
YbP9usKbSQI5Exab0mHUTo13J3yL5q1rnuCtaC97fw0zvVn7nSNIApykXEKCU8sbWCqdxYLdqI0r6m8IrRGG9MCFt
mMyGI6EKBVOqJZaXypJUyhpyCB9eD66aXHnhVLCYMdMZZakAKajgwbYTTR4088tCI4s4SGXbf9yKVbeVw8qhclDC2
m3JalkjsvVdn1wUJJ4lJHL9zBH54GWxGS9jrmavjElJpEEBxjHsC7hcsTNLfWBNP0oFVPDUf0UeRNgYP6fWKOKwnI
jYkmVkFN5polmCdqRxmngZnj7ksXEmWwT0eJ27uIm5ICAt4ieZHzASr7VjzmPgk9ljlQ0Zizf1OOQTJ3xhuuolQjB
qSpyE8ZTqpHVzqxXiTGlBuU34gNmhpg3rbTAtOM3At2mPBAXoeasYkbqzUky7ptuR63QOUo0TJaKoqre2XeEbI6tU
TDXUelK8WF5xF1Uc5InO7I4MGRcrlZsqIzqpbVyWsZxd0hZuzeM1O9eRAU7ZkXWuhkvTBsmXGpisoAqm96CS8oRYC
YVgEFfLd1whEgD6wyiKkNr4X6v2nYiTY6ef0eLXReVp6KLi8aI4A4wp4bp0db4YmJ4OzVtUuLME8SIuQwFVVMLFQS
tGuJsNep8LXgBMQFun37v4ZckJCE7Joph9RPoPqroJD1BZLE0khOaktzQtAuqtQps0lyZWx4rpY6Eedco61uA96kf
ZQyKbGU6uQzJTjpE0aLP18S3ETYHP0Rtls8LfCKrEA5sJRCDJhWhXWqVkYgKANdZzcUPnfahMOpJggADtk317B5Hy
4o8tcrqLgh6DTbKWFX0hRKdRB4boBIZz2GcxOZbvU0tKzNqgjWwMy5w1d5phZQEFkpo2vBYHLCBptNcsSt2L1El1i
lbIb0QnKZJiMlW037JVrP9X3XC7G04F9MtpIVZRlnLoHxNRwnCdPedlqH3FOxSRYTqjIrKNSwhQ7EjbOjAoqrGgY7
dHaWNvqGP5EjxwjiiHsSwXpNVJ1yfQmg2CkFWWW10cir4WZKKWXIjIhcNgHl7GxwJjVyg48gPbB6yehXAodwDHBXJ
kIJonlFOmT4iyngSpxSiyApk1xqeKI5vCfKPQ1vqTG73QhQuLB5X0mc3FoAyMyYovjgUm2ldMuLxyheCY8Ek3CKED
BjGOXdyd57qSmJ8fRzUMEfwrLKiG0lFFKQurT2wyy59Wm1mBe3GOO8622ktZPrqct2Jnw8cf7UkeAGbwIxBhNG1Bi
yofU7udkPHugahTjT8ShS8NLsDS7UevvesR3PBFs2qnsG2IXB404Z9AvSFG7bE4AOoxHGu2swLMwgh4eXXIAhIkHa
I7SfYbj2OMoX4SpJFFe2eX9HTUVVvhsSW5KGyDLFHSsKCnsdj3JMXRb1TMzHZN7UFMFNDmqRcnvIIR9n7ZsIS8EYA
2iSY6MXhT1Dh4mvN7tUpOUZFSbIfS2LEUShqilrAMU6fdkqT1GwvhluCYTj8dmwLYLjaoUekUVRrf7yhqt2BZsDjf
Rl1mO2sriN90xlglHvgTmM6PUEZtGQevNt9gLG9t9yvQI18ahoXOCwzebYyztQYUsUi9gkmu7YjMA0PXHri1WV86c
2NMhlSBgGsL05xH1dqgJBTsaddrPps646paji9OEKZeY5nOWtpywgOpsgF7WlobAGFSMOAfUyavgp7DpNgfmmvbVL
RYlvizO5BOuqGTbxZDltbsdOxe0ZG1GyfkMuUgEml7qQV5uk89co5x645nW8fmA2R5yBhOgcoF2f33ZuxZzrCKLVv
wBTiwugQ5aMtfOmdK48MZommHdMvhLLYmIDHUdYtWLOkNDfne8z1sqmm3l0a6pLhzZgzAFZV8NRiIXQ0p8x8wUC3j
0kfgb5OWFG5V42Nk4CJHkuZTvmytep7I0OgvGsYMc5IxbiQe1b9YZyqhZnMzjye37JHaiva0NCsGDSME59brOtxs3
nYM5hI0WqK2ITUEnzcROA9JUlASVseFqKlyvxLUx8K233Wa9bHxp1qqYm5lT4iK71NvJi0TpkFAYzh8M2Yl1o8K9g
OMR6cwkpafmXirUldHCY6ebH8XI3R3CZz2Q0WxXhmKfW0zywxRRADtei2m0GcwocundAVTmyfPCn9YFeM0ePE4bWy
a6MTVYjUPWdtVz2VHbPVat2I4kIh85RmxFPnOJrRwXQRLDJTqQ2Xh8IcNyxyDAdSWJjkOQBf8E9VpnsPJwPg5snVI
y62TlSC8Hx36kUH1QQsiLlligffGoQh0p0qSXuL3AivTBWh27Oj4YJCC5lzxllBO4c1R4vqAhl1aQy7r6PswD7K4t
B80WbtzqBpnJRyiHMgcgCOV4gEChjVPdUL9WQZmWOJytMUYpw6qPWyh8t3K9KB25WE9GGIr1TLj0naxX5NMUruwh8
NWdtaVJvICjBDLlIpH0p6nZLLKD9x41uwm2sYy7RehFXe8U4ajyIwfcXyADIz0JvOKqz3bYov4JrqvAOo3H0DRoBs
Newo4hHHQecrh4ScYWH7MhKbZVW0qi2riRSkNqOvZ5MrMnTBjluT6jyjN5HNptTVl7phE3qBYqRIBQMu0zIQy6jeV
7kcfbsfS84ANMG51Ng7Tg3a36QaKrF4RR9w3QQyxwygRHGARj1bWu4694hodqMLwUTTyGLJqmsaa89JdS38fYHwFs
raUOSJwNqyyZOigEVz2HxTNo6rnjDtkntvUGCzVN9o2fFyB5L1F7QZZiT2n0T3n7ajQXjZQbBrLF4AgfpCp17dM2z
p3geeEz7ogmRr7A597GaKmKDEMZoiu4wHw3G5Ysfl4EB9gfzRSypCRmdtKPlmESjCrxMcZpPKcezlQ6J1Ep8Tk0Lf
AtGL7tNhZpLy5Xqpl4sxPPwytyzmiyqA5fvemZ6RZYz9WjpnRL8JlEJhcH60vf3ravGfqDScpts3WBG2HUQxnuSBw
yqlTinZ46spT42pTZB3ZFgwm4HdORcs7mwYjykr5qKYzQzCuhgFLyDeW3QVp7zh2Sho1vHdMTbekK39A8t0Ynb5zX
9TxQaMqNdQtxjusMAIHFfnnlyUPvslpPj0VMSkWz5ewrwB08YcTCyXqAbRW8Ad7gnGO9EG611yipglH4W9DHb7ka7
qvEtcUjqWOfVJhCAlK062Ph8mqCGHMBbOK61yDq8YgwnnsGffCRvOulDEf

46

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 12:
Formulating the syntactically
simplest programming language
Reviewer: Wannabe
Rating: Potential breakthrough
Confidence: I couldn’t make heads or tails of it, but something
this complex must surely be terribly clever

Insightful framing of a classic problem in a completely novel way

47

9

Type-directed decompilation of shell scripts
Spencer Baugh

Dougal Pugson

University of Carcosa
Carcaso, Hyades
first.last@gmail.com

Abstract

Maintenance shell programmers are often faced with inscrutable shell scripts without human-readable source code.
We apply techniques pioneered by the type-directed partial
evaluation community to create a decompiler which can
take an executable shell script and recover its original source
code. This technique has surprising generality, and our decompiler can also be used as a pretty-printer, or in general,
as a compiler from any language into shell.
CCS Concepts • Software and its engineering → Scripting languages; Compilers; Software maintenance tools;
• Theory of computation → Type theory.
Keywords Partial evaluation, type systems, shell scripting
ACM Reference Format:
Spencer Baugh and Dougal Pugson. 2020. Type-directed decompilation of shell scripts. In Proceedings of SIGBOVIK (SIGBOVIK 2020).
ACM, New York, NY, USA, 3 pages. https://doi.org/10.475/123_4

1 Introduction

Among its many beneficial properties, shell has a unique
feature: The language automatically compiles itself as it is
written. Immediately after a shell script is typed into a computer and saved into a file, it is transformed into a compiled
form which is unreadable by humans. 1 Such a script can be
immediately used for all your most important financial data,
medical procedures, etc., while you are safe in the knowledge that no-one can read your important proprietary shell
scripts.
Unfortunately, sometimes such scripts might exhibit minor bugs. Since these executable files that are leftover from
the process of shell scripting are unreadable garbage, maintenance programmers are often forced to rewrite from scratch.
Sometimes, very brave programmers will try open one of
1 Use

of the authors’ previous work on delimited continuations in bash, [1],
seems to accelerate this process.

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact
the owner/author(s).
SIGBOVIK 2020, April 2020, Pittsburgh, PA USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 123-4567-24-567/08/06.
https://doi.org/10.475/123_4

Pugson’s C++ Crypt LLC
New York, USA
dougalpugson@gmail.com
these old, buggy "shell script files", but generally they are
instantly turned to stone upon seeing the inscrutable contents. And since shell is so convenient, typically the original
programmer is long gone - probably retired early.
Our contribution in this paper is a decompiler for these
files, which is able to recover the original source code. This
source code can then be viewed by the maintenance programmer, to help them reimplement the script in their non-shell
language of choice.
As outlined in our previous work, [1], the Unix shell is
closely historically related to functional programming. Thus
it should be no surprise that we are able to transplant techniques from the functional programming community, to the
shell, in the tradition of [4].
In this case, we use partial evaluation techniques to achieve
our goal. As described in [3], as citing [2], as blowing the
minds of undergrads everywhere, sufficiently general partial
evaluation techniques can be applied to "reify" a compiled
program and recover the program source code, under the
constraints that the program 1. has a type and 2. terminates.
Pretty easy constraints, I think we can make that happen for
bash!
As a more approachable introduction (note that "more approachable" doesn’t mean "approachable"), these techniques
can be compared to tagless-final-style techniques. Decompilation of a typed, compiled program is essentially identical
to pretty-printing of a tagless-final-style term. The magic is
that any typed, closed program can be treated as a TFS term.
Our paper is organized in some number of sections. Section 1 contains an introduction and a description of the organization of the paper. Section 2 contains a exploration of
the requirements for applying these techniques, and establishes their firm grounding in theory. Section 3 demonstrates
several applications. Section 4 gives an overview of the implementation. Section 5 concludes the paper, discusses future
work, and affirms that this was the right thing to do.

2

Theory

As mentioned above and in [2], we have two requirements
to apply type-directed partial evaluation: The program must
be typed and must terminate.
2.1

Typed

What is the type of a shell script? Well, a shell script takes the
PATH environment variable, and runs commands (identified
by strings) out of PATH, each of which has some side effects.

48

SIGBOVIK 2020, April 2020, Pittsburgh, PA USA
A shell script doesn’t return anything, it just has side effects,
so let’s say that its output type is unit.
So that means a shell script has type path− > unit, where
path is (str, [str ])− > unit.
Then in total, a shell script has type ((str, [str ])− > unit)− >
unit.
We can confirm this is correct by applying double negation
elimination 2 which shows us that shell scripts have type
(str, [str ]). This is correct because a shell script is indeed a
bunch of strings.
Let’s be a little more specific with our type, though, and
model each executable as a function. So then the type of
PATH is (str − > ([str ]− > unit)), and the type of a shell
script is then (str − > ([str ]− > unit))− > unit
For now, just think of a shell script as taking PATH and
running commands out of it.

Spencer Baugh and Dougal Pugson
when executed, construct an AST instead of actually doing
anything.

3

3.1

l s ; which l s
stat /
foo | bar
Nevertheless our decompiler can run on the script and produces the following output:
ls
which l s
stat /
foo | bar
As you can see, our decompiler even pretty-prints the shell
script.
3.2

Decompiling arbitrary executables

It also works on C programs, and in general, arbitrary executables. We can compile the following normal C program,
and run our decompiler on it.
i n t main ( ) {
int rc ;
rc = fork ( ) ;
i f ( r c == 0 ) {
execlp ( " foo " ,
" f o o " , " b a r " , " b a z " , NULL ) ;
} e l s e { w a i t ( NULL ) ; }
rc = fork ( ) ;
i f ( r c == 0 ) {
e x e c l p ( " whatever " ,
" w h a t e v e r " , " quux " , NULL ) ;
} e l s e { w a i t ( NULL ) ; }
return 0 ;
}

2.3 Background
In brief, the principle of the technique we will be applying
is this: A closed, abstract function, which takes in other
functions and combines them through application in some
way to eventually return a result, can be passed functions
which, instead of performing actual operations and returning
real results, take ASTs and return ASTs.
For example, a parameter with type (a, a)− > a, which
might normally be addition of two integers or something,
can be passed at the specific type (ast, ast)− > ast, and be
implemented as λx .λy.Plus(x, y) where Plus is some datatype constructor.
A shell script’s single argument (in our model) is the PATH
environment variable. 3 We will pass in a PATH which, when
an executable name is looked up in it, returns the executables
(functions) of our choice. These executables will in turn,

Decompiling bash

Suppose we save the following shell script to a file and mark
it executable, which instantaneously makes it unreadable.

2.2 Termination
Termination, on the other hand, is a much harder problem
than assigning a strict static type to shell scripts. This is
because of the presence of the dreaded D-wait in Unix. A
process can get into an uninterruptible state when making
a filesystem request, and just hang forever, ignoring all signals, including SIGKILL. This is really annoying for people
developing filesystems, which we had to do for this paper,
so we want to complain about it here.
Nevertheless, if the program hangs, this issue can be solved
by simply mashing Ctrl-C. Even D-wait can be solved by
throwing the computer out the window (assuming a sufficiently portable computer, and great enough height for it to
be destroyed on impact).
So termination is ultimately not a problem either.

Applications

Let’s demonstrate our tool before getting to the actually
interesting part.

And we get the following shell script out:
foo bar baz
w h a t e v e r quux
Useful!
3.3

Optimizing decompiler

Our decompiler is so advanced that it in fact transparently
applies optimizations in the process of decompilation. Consider the following C program:

2 DNE

is an axiom because this paper is unintuitive.
has sufficiently abstract types, since everything is a string, so we
don’t know what anything is.
3 This

49

Type-directed decompilation of shell scripts

SIGBOVIK 2020, April 2020, Pittsburgh, PA USA

i n t main ( ) {
p r i n t f ( " h e l l o ␣ world \ n " ) ;
}

those shell scripts that they always complain are unreadable.
Hopefully this will increase their productivity, massively
increase global GDP, and cause my 401K to recover all the
value it lost due to the coronavirus.

This program decompiles to the following shell script:

References

Our decompiler correctly recognizes that this program, since
it doesn’t execute any other programs from the filesystem, is
in fact utterly worthless, and optimizes it away to nothing.

[1] Baugh, S. bashcc: Multi-prompt one-shot delimited continuations for
bash. In Proceedings of SIGBOVIK 2018 (Pittsburgh, Pennsylvania, 2018),
Association for Computational Heresy, pp. 161–164.
[2] Danvy, O. Type-directed partial evaluation. In Partial Evaluation
(Berlin, Heidelberg, 1999), J. Hatcliff, T. Æ. Mogensen, and P. Thiemann,
Eds., Springer Berlin Heidelberg, pp. 367–411.
[3] Kiselyov, O. Typed Tagless Final Interpreters. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2012, pp. 130–174.
[4] Shelley, M. W. Frankenstein: or The modern prometheus. Lackington,
Hughes, Harding, Mavor, and Jones, 1818.

4 Implementation

Much to our surprise, we actually implemented this.
We have implemented a filesystem (using FUSE), which
pretends to have any possible executable you want. We point
the shell script at this filesystem using PATH, and each time
the shell script goes to run a command, it instead runs a stub
under our control. Using some real technology we developed
earlier and just thought it would be funny to use for this,
this stub connects back to the filesystem server, where our
decompiler is able to query its argv, stat its stdin/out/err, and
tell it to exit with a specific exit code.
To mount the filesystem without requiring privileges or
setuid executables, we use a user namespace and mount
namespace, and run the script inside those namespaces.
After the shell script finishes execution, we reconstruct
its source code from the trace of executed commands using
a highly advanced for loop.
Note that this doesn’t use “LD_PRELOAD” or strace, so
it can even be used on statically linked, setuid shell scripts.
There are lots of those!
The code is on Github at https://github.com/catern/rsyscall/
tree/master/research/sigbovik2020.

5 Conclusion
5.1 Future work
5.1.1

Support for niche shell features

There are some niche, minor features of the shell language
which are not supported by our decompiler, such as “if” and
“while”. As any true shell programmer uses “xargs” instead,
which our framework would decompile just fine, this isn’t a
problem.
Nevertheless it might be nice to figure out some ridiculous
hack that would allow such features (and shell builtins in
general) to be visible to our decompiler.
Maybe we could execute the shell script multiple times,
returning different exit codes from commands different times,
and thereby get a collection of traces through the control
flow graph, which we could then piece back together. But
this is beginning to sound like real work.
5.2 Conclusion conclusion
In conclusion, we hope that this tool proves useful for maintenance shell programmers, who will finally have a way to read

50

10

Verified Proof of P=NP in the Silence Theorem Prover Language
Matias Scharager
March 2020

1

Abstract

Godel’s Incompleteness Theorem has for quite a long time made people aware that it is not possible to design a sound,
complete and consistent language that encapsulates all the natural numbers. Programming languages that care about
type theory in their construction usually try to devise theorems on things that seem like the natural numbers and
exhibit some properties of them, but never fully encapsulate all of mathematics. There will always be a theorem left
unproven in these languages.
However, the common approach these languages have is that they neglect completeness. Instead of this, the Silence
programming language implemented in this paper goes beyond this idea and chooses to neglect soundness. This allows
for a language capable of proving any mathematical statement. We then proceed to add in as many formalisms as
necessary to express the P = N P problem and prove it.

2

Booleans

Our booleans consist of 3 distinct terms each of type Boool. want represents the true case. don’t want represents
the false case. And of course, might want is indecisive and doesn’t know which one to pick.

Γ ` want : Boool

want val

Γ ` don’t want : Boool

don’t want val

B −→ B 0
I B a more than b −→ I B 0 a more than b

might want −→ might want

Γ ` might want : Boool
I want a more than b −→ a

I don’t want a more than b −→ b

3

Natural Numbers

It is generally a good thing to have some notion of numbers in a programming language. The reason behind this is
that people generally associate the word computation as being something done on numbers and so having numbers in
a programming language is a must. The following is the statics and dynamics for an encoding of the natural numbers
in Silence.

Γ ` 7 : Nat

7 val

Γ ` x : Nat
Γ ` succcc(x) : Nat

x val
succcc(x) val

N −→ N 0
recNat(N ; a; x.b) −→ recNat(N 0 ; a; x.b)

Γ ` N : Nat Γ ` a : A Γ, x : A ` b : A
Γ ` recNat(N ; a; x.b) : A
recNat(7; a; x.b) −→ a

recNat(succcc(N ); a; x.b) −→ [recNat(N ; a; x.b)/x]b
As such, we now have Heyting Arithmetic. Notice to avoid arguments as to whether the natural numbers begin at 0
or 1, we have opted to start at 7.

51

4

Functions

All functions are fun, so we didn’t want to leave any of them out except we only really want to implement three of
them. Silence has fixed point recursive functions and non-recursive functions. It has dependent type functions as well.

Γ, a : A ` b : B
Γ ` fn a : A ⇒ b : A → B

Γ`f :A→B Γ`x:A
Γ`f x:B

fn a : A ⇒ b val

Γ, a : A ` b : B(a)
Γ ` Λa : A.b : ∀a : A.B(a)

fun f (a : A) ⇒ b −→ fn a : A ⇒ [(fun f (a : A) ⇒ b)/f ]b
Γ ` f : ∀a : A.B Γ ` x : A
Γ ` f x : [x/a]

(Λa : A.b) x −→ [x/a]b

Tuples

Γ`a:A Γ`b:B
Γ ` (a, b) : A ∧ B

Γ`M :A∧B
Γ`M ·l:A

M −→ M 0
M · l −→ M 0 · l

6

x −→ x0
f x −→ f x0

Γ, a : A, f : A → B ` b : B
Γ ` fun f (a : A) ⇒ b : A → B

a val
(fn x : A ⇒ b) a −→ [a/x]b

5

f −→ f 0
f x −→ f 0 x

Γ`M :A∧B
Γ`M ·r:B

a val b val
(a, b) val

a −→ a0
(a, b) −→ (a0 , b)

M −→ M 0
M · r −→ M 0 · r

(a, b) · l −→ a

b −→ b0
(a, b) −→ (a, b0 )

(a, b) · r −→ b

Sums

Γ`a:A
Γ ` inl(A ∨ B, a) : A ∨ B

a val
inl(A ∨ B, a) val

M −→ M 0
inl(A ∨ B, M ) −→ inl(A ∨ B, M 0 )

Γ ` M : A ∨ B Γ, x : A ` a : C Γ, x : B ` b : C
Γ ` case(M ; x.a; y.b) : C

M −→ M 0
inr(A ∨ B, M ) −→ inr(A ∨ B, M 0 )

b val
inr(A ∨ B, b) val

M −→ M 0
case(M ; x.a; y.b) −→ case(M 0 ; x.a; y.b)

Γ`b:B
Γ ` inr(A ∨ B, b) : A ∨ B

case(inl(A ∨ B, n); x.a; y.b) −→ [n/x]a

case(inr(A ∨ B, n); x.a; y.b) −→ [n/y]b

7

Sigmas

Γ ` a : A Γ ` b : B[a/x]
Γ ` ha, x.bi : ∃x : A.B(x)
b −→ b0
ha, x.bi −→ ha, x.b0 i

8

Γ ` M : ∃x : A.B(x)
Γ`M ·l:A
M −→ M 0
M · l −→ M 0 · l

Γ ` M : ∃x : A.B(x)
Γ ` M · r : B[M · l/x]
ha, x.bi · l −→ a

a val b val
ha, x.bi val

M −→ M 0
M · r −→ M 0 · r

a −→ a0
ha, x.bi −→ ha0 , x.bi
ha, x.bi · r −→ b[a/x]

Unit and Ununit

This is the most significant achievement of this paper. While unit exists in most programming languages, ununit is
a novel new idea that creates the unsoundess feature of Silence. Note that if tilt your head while viewing the typing
judgement for ununit, it looks like a person with a halo on their head.
Γ ` () : 1

9

() val

Γ ` hi : −1

hi val

Γ ` x : −1 Γ ` y : 1
Γ`xy:A

x −→ x0
x y −→ x0 y

y −→ y 0
x y −→ x y 0

hi() val

IO Behavior

True to it’s name, Silence has no IO behavior. It can neither read from the input nor write to the output. We have
purely functional behavior as there is also no imperative computations.

52

10

Progress and Preservation

This language is safe. By this we mean that there will be no undefined behavior. If we can derive a type for a program,
then it is either a value or steps to something else, thus we have progress.
Proof. Progress
Trivial by induction on statics
We also have that if a program steps to another program, then the type of the program is preserved, thus we have
preservation
Proof. Preservation
Trivial by induction on dynamics

11

Runtime of Programs

To help us reason about program execution, we will create a new relation between programs and allow for transitivity
A −→ B
A =⇒ B

A =⇒ B B =⇒ C
A =⇒ C

Consider the program (fun f (x : 1) ⇒ f x) (). If we formally reason about it’s execution, we can draw the following
proofs.
Proof. D

fun f (x : 1) ⇒ f x −→ fn x : 1 ⇒ (fun f (x : 1) ⇒ f x) x
(fun f (x : 1) ⇒ f x) () −→ (fn x : 1 ⇒ (fun f (x : 1) ⇒ f x) x) ()
(fun f (x : 1) ⇒ f x) () =⇒ (fn x : 1 ⇒ (fun f (x : 1) ⇒ f x) x) ()

Proof. E

(fn x : 1 ⇒ (fun f (x : 1) ⇒ f x) x) () −→ (fun f (x : 1) ⇒ f x) ()
(fn x : 1 ⇒ (fun f (x : 1) ⇒ f x) x) () =⇒ (fun f (x : 1) ⇒ f x) ()

Proof. Let A represent the expression (fun f (x : 1) ⇒ f x) () =⇒ (fun f (x : 1) ⇒ f x) () which is what we want
to prove. Thus we obtain the following proof:
D

A

E

D

A

E

A

D

A

E

A

D

A

A

E

D

A

A

E

D

A

A

E

D

A

E

D

A

A

E

D

A

E

D

A

E

D

A

E

D

A

E

D

A

E

D

A

E

A
A

A
A

A
A

A

Since we have shown that we can reduce a term to itself in some number of steps, this proves that the programming
language isn’t strongly normalizing. A more clever proof will show that these are the only possible reductions we can
take on this program through induction on dynamics and thus we have also proved the failure of weak normalization.
Of course, all this is blatantly obvious from having adding in a fixed point operator to begin with.
However, there are some programs that do in fact always terminate. A good example of such program is () as it
terminates in 0 steps. We would like to be able to analyze the complexity of such an algorithm, but there is an
overwhelming amount of non-determinism in Silence. To address this, we will remove non-determinism by using a
deterministic random number generator and applying rules based on the generated number. We can now define a
special type family called P zooms through x which takes a function P and its input x and represents the fact that
this algorithm ran in polynomial time on the input.

53

12

Conclusion of P=NP

The following type is representative in some way of the classical P = N P problem. As the type states forall verifiers,
if that verifier runs in polynomial time and there exists a valid solution that makes the verifier return true, we have a
valid algorithm that runs in polynomial time and is able to find a solution.
∀verif ier : Nat → Boool. ((∀x : Nat. verif ier zooms through x) → (∃x : Nat. verif ier x)
→ (∃algo : () → Nat. (algo zooms through () ∧ verif ier(algo()))))
Notice that the following is a valid term of this type and thus is a proof of P=NP
ΛEasy : Nat → Boool.
fn as : ∀x : Nat. Easy zooms through x ⇒
fn pie : ∃x : Nat. Easy x ⇒ hi()

13

Future Goals

Much like most published programming languages, there currently exists no working implementation of Silence. The
following is a proposed compilation translation into assembly
·`e:τ

nop

Note that the translated program results in the same IO behavior as the original program, so we can very efficiently
express the content of the language.

54

Systems
11

SIGBOVIK ’75 technical note: Conditional move for shell
script acceleration
Dr. Jim McCann, Dr. Tom Murphy VII
Keywords: cond-, itional, move

12

NaN-gate synthesis and hardware deceleration
Cassie Jones
Keywords: hardware deceleration, hardware synthesis, electronic
design automation, IEEE-754, NaN-gates, cat meme

13

hbc: A celebration of undefined behavior
Andrew Benson
Keywords: compilers, undefined behavior, Bash misuse, computer
music, Principles of Imperative Computation

55

11

SIGBOVIK’75 Technical Note:

Conditional Move For Shell Script Acceleration

Dr. Jim McCann
IX @ TCHOW-ARPA

Dr. Tom Murphy VII
TOM7 @ T7-ARPA

Abstract
One of the most effective programming interfaces for modern microprocessing
computers is the command-line interpreter, or shell. Shell scripts provide
a high-level abstraction of the operations of a microprocessor, making them
an appealing alternative to hand-translated machine code or so-called ‘‘macro
assembly languages’’. Unfortunately, shell programs are also significantly
slower than their assembly counterparts. One potential source of this slowdown is branch misprediction. In this paper we show how to address this drawback by adding predicated execution to the shell.

1

Introduction

One common trend in computing as of late
has been the migration of features from
CISC instruction sets developed by DEC,
Intel, and other captains of industry to
the relatively underserved ‘‘high level’’
languages community, who generally focus
on simpler and less modern operations. We
continue this tradition by showing how to
bring the advantages of predicated execution to shell scripting.
Predicated execution allows processors
to avoid the root cause of branch prediction stalls: branches. Instead, instructions are provided which can check one or
more predicate registers and effectively
become no-ops if the registers are not
set. These so-called predicated instructions are always executed, so no pipeline
stalls need to be included while the processor decides which instruction needs to
be fetched next.
Predicated execution for the shell
provides a similar benefit -- expensive
branch predictions can be avoided, resulting in tremendous speed-ups (up to 100x in
our tests).
2

In shell scripting, where files are
the obvious equivalent of registers, the
semantics are clear:
Usage:
cmv <file1> <file2>1
Rename file1 to file2, if the condition register is set.
But what is the condition register? We
explored two choices of condition register
-- the exit value of the previous command,
which leaves us with a bit of a problem,
since this value is not readily available
to a process; and the processor’s cf2 register, which is also not readily available
to shell scripts.
2-1

The ‘csh’ shell

The first method we propose to allow easy
access to the return value of the previous command is to run a modified shell
-- which we dub the ‘csh’ shell -- that
stores the return value of the previous
command into an environment variable when
invoking a subprocess.
In our implementation (Figure A),
the variable is called ‘DOLLARSIGN QUESTIONMARK ALL SPELLED OUT’ for obvious reasons.

Implementation
2-2

The minimal instruction needed for conditional execution on a modern five-stage
pipelined cpu is conditional move. This
instruction moves a result from one register to another if a tertiary condition
register is set.

The ‘c’ utility

While it would be straightforward to implement, exclusively, a conditional move
1. Note the use of AT&T assembly syntax,
where the source operand comes before
the destination operand.
2. ‘‘condition flag’’

56

diff --git a/src/exec.c b/src/exec.c
index 87354d4..7552f8d 100644
--- a/src/exec.c
+++ b/src/exec.c
@@ -112,6 +112,8 @@ shellexec(char **argv, const char *path, int idx)
char **envp;
int exerrno;
+
+

/* pass exit status of last process to executed program */
setvarint("DOLLARSIGN_QUESTIONMARK_ALL_SPELLED_OUT", exitstatus, VEXPORT);
envp = environment();
if (strchr(argv[0], ’/’) != NULL) {
tryexec(argv[0], argv, envp);

Figure A: This patch for https://git.kernel.org/pub/scm/utils/dash/dash.git/ creates a
shell that stuffs the return value of the previous command into an appropriately-named
environment variable.

utility; we instead embraced the highlevel nature of shell scripting by creating a general purpose predication utility,
‘c’, which (when called as ‘cNNN ...’)
will run ‘NNN ...’ when the proper predicate values are set. Since this utility’s
behavior is based on its name, one can
create a new instance of it by simply creating an inode link. For example, to make
a conditional version of /bin/sh:
ln -s /bin/c /bin/csh3

The tested tasks were:

Our implementation of ‘c’ (see
https://github.com/ixchow/c/blob/master/c.c)
is written, naturally, in C, and is built
to support both the shell-level approach
discussed above and the kernel-level approach discussed below.

Results are given in Table I. In all
cases the predicated execution version of
the task does better. Indeed, for the compile task, the overall execution time is
reduced to 1894299ns -- that’s 139426014ns
faster than simply running the compiler!

3

4

Evaluation

In order to evaluate the performance gains
of conditional evaluation, we compared
conditional and traditional versions of
several simple shell scripts (see Appendix). We timed the scripts by first
clearing the page cache, then running the
traditional version of the script, then
running the conditional version of the
script.

3. Note the use of INTEL assembly syntax,
where the source operand comes after
the destination operand.

* ‘echo’ which makes two static checks
and echos a string depending on the
result;
* ‘copy’ which copies the smaller of
two files to a destination;
* and ‘compile’ which compiles an output
file depending on the timestamp of a
source file.

Pushing performance

While a 2-100x cyclefold improvement is
nothing to shake a luggable microcomputer’s vacuum fluorescent display at,
these results fall short of what we could
hope for. One possible explanation for
the lukemoist performance is that the
condition itself is stored in a highlevel way (see Figure B) using environment
variables. Using high-level parts of the
computer is a well-known cause of cycle
overslows.
The fastest place to store the condition is in the CPU itself, using electrons. The CPU can only be accessed

57

task |
copy
|
echo
|
compile
- - - - - -+ - - - - - - - - - - - - - -+ - - - - - - - - - - - -+ - - - - - - - - - - - - ours | 457516888 ns | 14104407 ns |
1894299 ns
old | 5753853493 ns | 29336387 ns | 182916245 ns
Table I: Benchmark results. Our approach is dramatically faster in all cases.

through the Operating Kernel. As a proof
of concept, the authors created a Kernel
module4 that directly accesses the CPU’s
FLAGS register. It presents the flags as
files in the /proc filesystem where they
can be accessed by any process:
$ ls - al / proc / flags
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root
-rw - rw - rw 1 root root

0
0
0
0
0
0
0
0
0
0
0
0

af
cf
df
if
iopl0
iopl1
nt
of
pf
sf
tf
zf

Each file contains -- at the moment
that it is read -- either are ‘‘1’’ or
‘‘0’’ if the corresponding bit is set in
the FLAGS register. Writing a ‘‘1’’ or
‘‘0’’ to a file will modify the corresponding bit. The ‘c’ utility described
in Section 2-2 has experimental support
for storing the condition result in the
cf flag (formally ‘‘carry flag’’ but the
mnemonic can also be used for ‘‘condition
flag’’) via this kernel extension.
Alas, with great power comes great instability. There is some risk that the
FLAGS register5 is modified by other applications running in time-share with the
‘‘main’’ shell script. In this case, the
FLAGS register may not correctly reflect

the indicated status, and conditional
operations may occur or not occur contrary to the shell program’s coding. On
the other hand, some uses of /proc/flags
are very robust. For example, setting
/proc/flags/tf, the trap flag, reliably
terminates the current process with a fatal error.
We installed this kernel module on several shared workservers that we administer. Preliminary user reports include
indications that the behavior is ‘‘very
unstable’’ or ‘‘does not work at all.’’
Clearly, a wider-scale test deployment is
needed.
5

Future Work

Given that predicated execution leads
to a CISC-ridiculous improvement in the
speed of shell scripts, it is natural
to ask what other CISC-onesquential results can be obtained by bringing other
micro-architectural features to high-level
languages.
Branch delay slots -- instructions after a branch that are always executed
-- can already be trivially supported in
shell by writing the delayed commands as a
background task in front of the branch in
question, then foregrounding them afterward, as per:
delay - command &
if [ -x " something " ]
then
fg
#...
else
fg
#...
fi

4. It can be downloaded via hypertex at
sf.net/p/tom7misc/svn/HEAD/tree/trunk/csh/
5. As an additional technical matter, this
approach does not work for multiprocessor systems, where there is one FLAGS
Notice that this is actually much more
register *per CPU*. Fortunately, such
flexible than current (MIPS) microprosystems are a mere theoretical curioscessor implementations, since multiple
ity.
commands may be queued in the delay slot

58

+ - - - - - - - - - - - - - - - - - - -+
| ENVIRONMENT
|
| ( VARIABLES )
|
| + - - - - - - - - - - - - - - -+ |
| | SHELL PROMPT | |
| | ( SHELL LANGU - | |
| | AGE )
| |
| | + - - - - - - - - - - -+ | |
| | | USER APPL -| | |
| | | ICATION
| | |
| | | ( C PROGR - | | |
| | | AMMING LA -| | |
| | | NGUAGE )
| | |
| | | + - - - - - - -+ | | |
| | | | FILE S -| | | |
| | | | YSTEM | | | |
| | | |( BYTES )| | | |
| | | |+ - - - - -+| | | |
| | | || KERN -|| | | |
| | | || EL
|| | | |
| | | ||( ASM )|| | | |
| | | ||+ - - -+|| | | |
| | | ||| CPU ||| | | |
| | | |||( E -||| | | |
| | | ||| LE -||| | | |
| | | ||| CT -||| | | |
| | | ||| RO -||| | | |
| | | ||| NS )||| | | |
| | | ||+ - -+||| | | |
| | | + - - - - - - -+ | | |
| | + - - - - - - - - - - -+ | |
| + - - - - - - - - - - - - - - -+ |
+ - - - - - - - - - - - - - - - - - - -+

then
kill %%
fg % else
kill % fg %%
fi
Mind you, if either true-command or
false-command have any side-effects before
the test completes, this approach may lead
to undesirable output; but a fast enough
CPU will certainly turn this ‘‘race condition’’ into a ‘‘victory condition’’. As a
compromise, on slower CPUs, each branch of
the if statement could be run in a separate chrooted union-mount, with a snapshot
of the result written back after the test
has resolved.
Such a technique may be vulnerable to
the Shpectre vulnerability, leaking information to other processes on the timeshare via side-channels like the cache.
Thus it is recommended to flush the cache
before and after using this technique:
sync
echo 3 > / proc / sys / vm / drop_caches
swapoff -a
true - command &
false - command &
if [ -x " something " ]
then
kill %%
fg % else
kill % fg %%
fi
swapon -a
echo 3 > / proc / sys / vm / drop_caches
sync

Figure B: The ‘‘levels’’ of a computersystem, from ‘‘high’’ (outer boxes) to
‘‘low’’ (inner boxes). As we descend to
lower levels of the computer, the programming tools used (in parentheses) become
more difficult, but more powerful, and
more fast. Only master wizards are permitted at the lowest levels, such as ‘CPU’.
6
position and overlapped with the test execution.
A similar approach works to enable
speculative execution, wherein code in
a conditional is executed before the condition is checked:

Conclusions

We have demonstrated that shell scripts
can benefit from conditional execution.
Q E
E D

true - command &
false - command &
if [ -x " something " ]

59

Appendix: Test Code
This appendix contains source listings for the the shell programs used in the benchmarking process described above. The utilities ‘cecho’, ‘ccp’, and ‘ccc’ are all links
to the ‘c’ program described in the main body of the paper. Notice how the predicated
execution versions are also generally shorter than their tranditional counterparts.

# Echo ; traditional
if [ " A " = " A " ]
then
/ bin / echo " Hello "
fi
if [ " A " = " B " ]
then
/ bin / echo " World "
fi

# Copy ; traditional
sizeA = ‘ stat -c % s fileA ‘
sizeB = ‘ stat -c % s fileB ‘
if [ $sizeA - le $sizeB ]
then
echo " fileA is smaller "
cp fileA fileS
else
echo " fileB is smaller "
cp fileB fileS
fi

# Compile ; traditional
if [ " prog . cpp " - nt " prog " ]
then
cecho " Compiling program ..."
cc prog . cpp - lstdc ++ -o prog
fi
./ prog

|
|
|
|
|
|
|
|
|
|

# Echo ; predicated execution

|
|
|
|
|
|
|
|
|
|
|
|

# Copy ; predicated execution

|
|
|
|
|
|
|
|

# Compile ; predicated execution

[ "A"
cecho
[ "A"
cecho

= "A" ]
" Hello "
= "B" ]
" World "

sizeA = ‘ stat -c % s fileA ‘
sizeB = ‘ stat -c % s fileB ‘
[ $sizeA - le $sizeB ]
cecho " fileA is smaller "
ccp fileA fileS
[ $sizeA - gt $sizeB ]
cecho " fileB is smaller "
ccp fileB fileS

[ " prog . cpp " - nt " prog " ]
cecho " Compiling program ..."
ccc prog . cpp - lstdc ++ -o prog
./ prog

60

NaN-Gate Synthesis and Hardware Deceleration
12

Cassie Jones
Witch of Light
list+sigbovik@witchoflight.com
1 April 2020

Abstract

1

NaN-Gate Synthesis

The Yosys Open SYnthesis Suite [1] is a free and open source
archicture-neutral logic synthesis framework. It can synthesize
Verilog into a variety of backend formats using a flexible and
pluggable architecture of passes. The Yosys manual has a chapter on how to write extensions [2, Ch. 6], which can be consulted
for documentation and examples on how Yosys passes are built.
We provide a Yosys extension which synthesizes a circuit down
to a network of small floating point units implementing the
NaN-gate function. This can be further synthesized to a final
ACH Reference Format:
Cassie Jones. 2020. NaN-Gate Synthesis and Hardware Decel- target, like a specific FPGA architecture.
eration. In Proceedings of SIGBOVIK 2020, Pittsburgh, PA,
USA, April 1, 2020. (SIGBOVIK ’20, ACH).
In recent years there has been interest in the field of “hardware
decelerators,” which serve primarily to make computation more
interesting rather than more efficient. This builds off the work
of “NaN-Gates and Flip-FLOPS” [9] to provide a hardware synthesis implementation of real-number computational logic using
the Yosys Open Synthesis Suite [1] framework, and evaluates
the impacts of different floating point formats.

1.1

Introduction
Harware decelerators work on the principle of “stop and smell
the roses.” There are some qualities that are more important
than sheer efficiency, and often these improvements can often
only be realized by taking the computer and slowing it down
to a more leisurely pace. The largest advancements in the field
happen in the emulation space, since it’s the most widely accessible. It may be most familiar in the form of video-game
computers, building computers out of redstone in Minecraft,
Factorio combinators, or the like [7] [6].
“But of course speed is not what we’re after here.
We’re after just, beautiful computation going on inside the heart of this machine.” — Tom7 [10]

Yosys Synthesis

We will demonstrate all synthesis with the following toggle module, since it’s small enough for all stages of synthesis results to
be understandable and fit neatly on the page.
module toggle(input clk, input en, output out);
always @(posedge clk) begin
if (en) out <= ~out;
end
endmodule
Yosys will take a Verilog module like this and flatten the procedural blocks into circuits with memory elements. Running
sythesis gives us a circuit with a flip-flop, a toggle, and a multiplexer that’s driven by the enable line.

$2
A
The SIGBOVIK 2019 paper “NaN-Gates and Flip-FLOPS” deout
A
Y
$not
$4
B $mux Y
celerates computers in the name of elegance: it throws away
clk
CLK $6
S
Q
the assumption of binary computers and builds ones based on
$dff
D
en
real numbers, specifically IEEE-754 floating point numbers. It
aims towards “reboot computing using the beautiful foundation
of real numbers,” but it still leaves us with room for improvetoggle
ment in a few areas. It leaves the logic gates in the domain of
emulation, which limits the types of hardware that are easy to
build, and it limits the elegance that can be achieved. Since it
Figure 1: The synthesized toggle circuit.
uses an existing CPU as the floating point processor, it’s still
left with a computer that’s based on binary emulating the real We can also ask yosys to synthesize this to exclusively NAND
number logic.
and NOT gates with a small synthesis script.

Here, we attempt to remove this limitation by bringing NaN- read_verilog toggle.v
gate computation to the domain of native hardware, via a cus- synth
abc -g NAND
tom Yosys synthesis pass.

61

This particular design synthesizes to 1 D flip-flop, 3 NAND The share_nan pass reduces the number of conversions by shargates, and 2 NOT gates.
ing ones that have the same inputs. Then, the dff_nan pass can
expand the flip-flops in the circuit into a set of enough flip-flops
to store the floating point values.
en
A

$57
$_NOT_

Y

A

$58
$_NOT_

Y

A
B
A

clk

C
D

$53
$_DFF_P_

out

Q

B

$59
$_NAND_

Y
A

$60
$_NAND_

Y

B

$61
$_NAND_

Y

The simplify_nan pass converts any instance of fp3_to_bit
-> bit_to_fp3 to just a wire that passes the floats straight
through.

We do clean to remove dead wires and useless buffers, and
then finally the techmap_nan pass replaces the opaque NaNgate modules with actual modules so that further synthesis can
Figure 2: The circuit synthesized down to only NAND, NOT,
properly make them realizable on real hardware.
and memory elements.

1.3
1.2

Module Ports

The Yosys synth_nan Pass

If you want your circuit to support external floating-point based
interfaces, you can use the floating point conversion modules
Our synth_nan pass is implemented as a Yosys extension. For yourself.
convenience, we’ll describe the behavior in terms of the 3-bit
float synthesis. It converts a module to NaN-gate logic. It module toggle(
input clk, input [2:0] en, output [2:0] out);
summarizes itself as running:
synth
abc -g NAND
nand_to_nan
share_nan
dff_nan
simplify_nan
clean
techmap_nan

wire en_b;
reg out_b;
fp3_to_bit en_cvt(en, en_b);
bit_to_fp3 out_cvt(out_b, out);
always @(posedge clk) begin
if (en_b) out_b = ~out_b;
end

The first two steps there are standard synthesis. The synth
pass will convert a module into coarsly optimized circuits, and
abc -g NAND will remap the entire thing into optimized NAND
logic.

endmodule

The NaN synthesis will end up erasing the floating point conversions on either side of the interface since they connect to
floating point units. Future work could include automatically
expanding ports using something like a (* nan_port *) atOne complexity we have to deal with is external interfaces. Detribute.
spite the wonderful realms of pure real-number computation
we want to interact with, when interacting with fixed hardware
component interfaces, we have to convert between flots and
bits. In order to handle this, the NaN gate tech library has 2 Floating Point Formats
modules like fp3_to_bit and bit_to_fp3 which perform this
boundary conversion. In order to deal with the chaotic diversity While tom7’s work asserts that a binary4 floating point forof real circuits, for robustness, the nand_to_nan pass converts mat is “clearly allowed by the IEEE-754 standard,” this doesn’t
each NAND gate to a bit_to_fp3 -> NaN -> fp3_to_bit seem to hold up under a close examination. Brought to my atchain. Don’t worry, these conversions will later be removed tention by Steve Canon [8], there are two cases where these
everywhere they don’t turn out to be necessary.
floating point formats fall down. First, and most importantly
in the case of binary4, you need to encode both quiet- and
signaling-NaNs. Section 3.3 of IEEE-754 says [5]:
clk

C
D

0:0 - 0:0

C
D

$96
Q
$_DFF_P_

0:0 - 0:0

A

$97
Q
$_DFF_P_

0:0 - 1:1
A

1:1 - 0:0

B
C

2:2 - 0:0

D

$98
Q
$_DFF_P_

$92
$f2b

Y

$57
Y
$NaN

A
B
A

0:0 - 2:2

B
en

A

$61
$b2f

$58
Y
$NaN

A
B

$60
Y
$NaN

$59
Y
$NaN

Y
A
B

“Within each format, the following floating-point data
shall be represented: […] Two NaNs, qNaN (quiet) and
sNaN (signaling).”

out

$56
Y
$NaN

While binary4 does have two separate NaN values (a positive
and a negative), they are distinguished only by their sign bit,
which isn’t allowed to distinguish the two types of NaNs, as we
can see in 6.2.1:
“When encoded, all NaNs have a sign bit and a pattern
of bits necessary to identify the encoding as a NaN and
which determines its kind (sNaN vs. qNaN).”

Figure 3: The toggle circuits synthesized to NaN gates. Note
that the external logic ports have floating point conversion modules, but the clock line doesn’t.

62

1
out
clk

CLK

$6
$dff

D

2

$2
$not

A

B

Q

en
$57
$_NOT_

Y

$58
$_NOT_

Y

A
B

C
D

3

Q

$71
$b2f

Y

A

en

A

$66 Y
$b2f

A

$61 Y
$b2f

clk

Q

out

$53
$_DFF_P_

A

B

A

$57
Y
B $NaN

A

$56
Y
B $NaN

$79
$b2f

$68 Y
$f2b

$72 Y
A $b2f

$63 Y
A $f2b

$78
A $b2f Y

A

$59
$_NAND_

Y

$60
$_NAND_

Y

A

A
clk

D

Y

en

A

$52
$_DFF_P_

$4
$mux

S

A

C

A

Y

A

B

$61
$_NAND_

Y

$58
Y
B $NaN

$74
A $f2b Y

$85
A $b2f Y

A

$81
A $f2b Y

A

$59
Y
B $NaN

A

$60
Y
B $NaN

$86 Y
$b2f
A

$88 Y
$f2b

Y
out

4

en

$66 Y
A $b2f

clk

A

A

$61 Y
A $b2f

$56 Y
B $NaN

$63 Y
A $f2b

$72 Y
A $b2f

A

$68 Y
A $f2b

$78 Y
A $b2f

$57 Y
B $NaN

C
D

$96
$_DFF_P_

Q

0:0 - 0:0

$97
$_DFF_P_

Q

0:0 - 1:1

$98
$_DFF_P_

Q

0:0 - 2:2

$58 Y
B $NaN

A

A

$81 Y
A $f2b

$59 Y
B $NaN

$74 Y
$f2b

A

$85 Y
$b2f

A

$60 Y
B $NaN

$94
A $b2f Y

$88 Y
A $f2b

$86 Y
A $b2f

0:0 - 0:0
C
1:1 - 0:0

D
C

2:2 - 0:0

5

clk

D

C
D

0:0 - 0:0

C
D

A

$96
$_DFF_P_

Q

0:0 - 0:0

$97
$_DFF_P_

Q

0:0 - 1:1

$92
$f2b

Y

A
A

1:1 - 0:0

B
C

2:2 - 0:0

out

D

$98
$_DFF_P_

Q

$92
$f2b

Y

$57
$NaN

Y

out
A
B
A

0:0 - 2:2

B
en

A

$61
$b2f

$58
$NaN

Y

A
B

$59
$NaN

$60
$NaN

Y

Y

Y
A
B

$56
$NaN

Y

Figure 4: The full NaN-gate synthesis process for the toggle module. In step 1 we have the logical circuit after coarse synthesis.
In step 2 it’s synthesized to NAND and NOT gates. Step 3 converts the gates to NaN gates and adds conversion chains. Step 4
expands the flip-flops to store floats. Step 5 collapses redundant conversion chains to give the final NaN-synthesized module.

63

This means that we need at least two bits in the mantissa in
order to represent the infinities (stored as a 0 mantissa with
the maximum exponent) and the NaN values (stored with two
distinct non-zero mantissas).

s

E

T

value

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

00
00
00
00
01
01
01
01
10
10
10
10
11
11
11
11

00
01
10
11
00
01
10
11
00
01
10
11
00
01
10
11

+0.0
+0.25
+0.5
+0.75
+1.0
+1.25
+1.5
+1.75
+2.0
+2.5
+3.0
+3.5
+inf
sNaN
qNaN
qNaN

The positive values representable in the binary5 format. Note
that infinity, sNaN, and qNaN are all distinguished by the mantissa value when the exponent is all ones, so this is the smallest
possible floating point format. The negative values for each are
the same bit patterns but with a 1 in the sign bit.

2.2

Figure 5: From Steve Canon’s tweet [8]. As the cat says, since
IEEE-754 requires 𝑒𝑚𝑎𝑥 to be greater than 𝑒𝑚𝑖𝑛, there must
be two exponent bits, and since the mantissa must be used to
distinguish sNaN, qNaN, and infinity, so it also needs at least
two bits, leading to a minimum of 5-bit floats.

Evaluation

We compare the size (in both logic and memory elements) and
clock speed of modules synthesized with the different floating
points. For the benchmark, we use a pipelined 32-bit multiplier, and the PicoRV32 processor [3] synthesized for the ECP5
architecture, and placed and routed using nextpnr [4]. The
numbers given for clock frequency are the best result of 10 runs
of placement and routing.

The “NAND” variant are synthesized to NAND gates before
architecture-specific optimization, in order to obscure some of
the higher-level modules that are present in the original design
and prevent optimizations that won’t be available to the NaNgate synthesis. This gives a clearer baseline for comparison
“𝑞 is any integer 𝑒𝑚𝑖𝑛 ≤ 𝑞 + 𝑝 − 1 ≤ 𝑒𝑚𝑎𝑥” 3.3
with the NaN gates, and so this is used as the basis for relative
Still, the binary3 format is very useful for efficient implemen- numbers. Times marked DNP are those that did not successtation of NaN gates, and is worth including in synthesis for fully place and route for timing analysis, so no frequency can
people who aren’t bothered by standards compliance. For com- be reported.
pleteness, the synth_nan implementation supports synthesis to
Variant
Cells Cell% DFFs DFF% (MHz)
binary3, binary4, and the definitely-IEEE-754-compliant bi- Design
nary5 format NaN-gates. Furthermore, the architecture would PicoRV Direct
1884
57%
888
48% 103.55
support easy extensions to larger, more conventional floating
NAND
3328
100%
1848
100%
47.30
point formats like binary8, or even larger, by simply loading
fp3
43739 1314%
5544
299%
DNP
your own libary of modules named nan_fpN, bit_to_fpN, and
fp4
32853
987%
7392
400%
DNP
fpN_to_bit, for any value of N you want to synthesize with.
fp5
65511 1968%
9240
500%
DNP
Mult32 Direct
2879
104%
628
100% 143.04
NAND
2773
100%
628
100% 154.37
2.1 The binary5 Representation
fp3
25349
880%
1884
300%
25.26
fp4
19026
661%
2520
400%
21.88
Here we document the representation in the binary5 format,
fp5
38001 1320%
3140
500%
21.18
the smallest legal IEEE-754 compliant binary floating-point format. It has a sign bit, a two bit exponent, and a two bit man- It’s interesting that fp4 is the smallest of the floating point
variants in logic, rather than fp3. It seems likely that this is
tissa. We include a table of all of the positive values here:
because the ECP5 architecture is based on “LUT4” cells—4input lookup tables—which means individual NaN-gates might
happen to synthesize more efficiently with 4-bit inputs.

The binary3 format is further disrupted in section 3.3, which
rules-out the idea of having an empty range of 𝑒𝑚𝑖𝑛 and 𝑒𝑚𝑎𝑥,
since they’re used in an inequality and 𝑒𝑚𝑖𝑛 ≤ 𝑒𝑚𝑎𝑥, and is
elsewhere forced to be strictly less by other constraints:

64

2.3

Flattening

References

For this benchmark, we synthesize designs without flattening
[1] Claire Wolf. The Yosys Open SYnthesis Suite. Online.
post NaN-gate synthesis, because the optimizer is too effective
http://www.clifford.at/yosys/
and eliminates most of the floating point logic. When they are
flattened, the optimzer can consider the logic involved in the [2] Claire Wolf. The Yosys Manual. Online.
individual NaN gates and re-combine them and erase constanthttp://www.clifford.at/yosys/files/yosys_manual.pdf
value flip-flops. Designs that are flattened before optimizing
have no flip-flop overhead, and have on the order of 5% overhead [3] Claire Wolf. PicoRV32 - A Size Optimized RISC-V CPU.
Online. https://github.com/cliffordwolf/picorv32
in logic elements vs the reference NAND-gate versions.
While synthesizing with post-NaN flattening substantially un- [4] Claire Wolf, David Shah, Dan Gisselquist, Serge Bazanski,
Miodrag Milanovic, and Eddie Hung. NextPNR Portable
dermines the floating point logic and mostly demonstrates the
FPGA Place and Route Tool. 2018. Online.
impressive quality of Yosys’s optimizations, it suggests as an ophttps://github.com/YosysHQ/nextpnr
tion a sort of “homeopathic floating-point logic.” For users that
require efficiency but still want some of the elegance benefits,
they can flatten it and optimize it away, keeping some peace- [5] IEEE Standard for Floating-Point Artithmetic. 2019. In
IEEE Std 754-2019 (Revision of IEEE 754-2008). Pages
of-mind in knowing that their final circuit is derived from an
1–84. DOI 10.1109/IEEESTD.2019.8766229.
elegant real-number system, regardless of how it currently behaves.
[6] justarandomgeek. 2017. justarandomgeek’s Combinator
Computer Mk5. Online.
https://github.com/justarandomgeek/factorio-computer

3

Future Work

[7] legomasta99. 2018. Minecraft Computer Engineering - Redstone Computers. Online.
Floating point synthesis still has many avenues for improvement https://www.youtube.com/watch?v=aj6IUuwLyOE
and future work.
&list=PLwdt_GQ3o0Xe6pUS1vdzy0ZqXrApB2MNu
The current synthesis approach used by synth_nan remains
[8] Stephen Canon. 2017. A Tweet About IEEE-754. Tweet by
fragile in the face of flattening and pass-ordering. It should
@stephentyrone on April 1, 2017.
be possible to make it harder to accidentally flatten the designs
https://twitter.com/stephentyrone/status/848172687268687873
away into nothing, but they still do need to be eventually flattened since the nextpnr place-and-route flow is still not fully [9] Tom Murphy VII. 2019. NaN Gates and Flip FLOPS.
reliable in the presence of un-flattened designs. Currently the
In Proceedings of SIGBOVIK 2019, Pittsburgh, PA, USA,
synth_nan pass must be run before any device-specific passes,
April 1, 2019. (SIGBOVIK ’19, ACH). Pages 98–102.
which can be fine but it prevents the utilization of resources
[10] Tom Murphy VII. 2019. NaN Gates and Flip FLOPS. Onsuch as distributed RAMs.
line. http://tom7.org/nand/
Float synthesis tools should make it easier to define module
ports that should be expanded to accomodate floating-point
based signals, so that designs can operate fully in the glorious
domain of the real numbers, without having to flatten all designs.
More work could be done into ensuring that the individual
gates are properly optimized for different architectures, since
it seems unreasonable for fp4 to remain more efficient than fp3.
The system could also benefit from implementing a larger set
of primitive gates, to avoid the blowup of using NAND gates
to emulate everything, since they should be implementable in
similar amounts of elementary logic.
With binary5 and larger, there looks like there could be potential in attempting to explore designs that work purely on NaN
values, exploring the flexibility in the handling of signaling and
quiet NaN values.
The NaN-gate synthesis plugin for Yosys can be found at https:
//git.witchoflight.com/nan-gate. This paper and the examples
materials in it can be found at https://git.witchoflight.com/
sigbovik-nan-2020.

65

13

hbc: A C ELEBRATION OF U NDEFINED B EHAVIOR

Andrew Benson∗
Department of Computer Science
Stanford University
Stanford, CA 94305
adbenson@stanford.edu

April 1, 2020

A BSTRACT
We present hbc, a next-generation compiler with half the performance of clang but twice the fun.
We prove by way of demonstration that undefined behavior in a spec-abiding C compiler can do more
than just segfault.
Keywords Compilers · Undefined Behavior · Bash Misuse · Computer Music · Principles of Imperative Computation

1

Introduction

The C programming language has gone through several iterations (C89, C99, etc), but each edition leaves some behavior
under-specified. For example, while the C standards specifies that a plain char must be 1 byte in size, it does not
specify its sign. Each compiler that implements C may choose whether a plain char is signed or unsigned. Because
this behavior is not dictated by the standard but implementations must define it in a consistent fashion, this behavior is
called implementation-defined.
Some other behaviors are even looser. Not only do the C standards not prescribe a particular behavior, but implementers
are not required to make the behavior consistent. The most infamous example is probably dereferencing a NULL pointer
(or any other invalid pointer). The C standards do not say what should happen, and while code produced by most
compilers will probably segfault, it does not have to, nor does it have to do the same thing on two separate occasions. It
is perfectly valid for it to delete your tax statements, or even to start playing a melody through the speakers! (That’s
called foreshadowing.) Because no definitions are specified by the C standards or the compilers, this is called undefined
behavior.

2

Motivation

Computer science students are generally introduced to these concepts in their first systems-level course in C. In the
author’s experience, students are generally confused by these concepts since mainstream compilers are rather consistent
in either hanging or segfaulting when encountering these behaviors. Although the concepts are distinct, students tend
to equate “segfaults” with “undefined behavior” and direct the former’s rightfully deserved ire onto Poor, Mistreated
Undefined Behavior. Not only is there a misunderstanding of what these foundational concepts mean, but students
develop a fearful anxiety and distrust of Undefined Behavior. It’s unjust, and it’s time that someone stood up for
Undefined Behavior. The mainstream compilers like gcc and clang are too self-absorbed in exploiting Undefined
Behavior for frivolous goals such as “performance” and “implementation-simplicity”, so the only way is to build a new
compiler that respects Undefined Behavior for what it is and what its potential can be.
The author also recalls sitting in a 15-122 lecture freshman fall where the professor suggested that a C program could
play ‘Happy Birthday’ during undefined behavior. That may also have had some influence on this project.
∗

Also a software engineer at Google, which was not involved in this research. All code is Copyright 2020 Google LLC and when
provided is on the Apache 2.0 License.

66

3

Design

And by “new compiler”, what the author really means is “wrap an existing compiler (one of those he previously mocked)
and call it a new one”. This is an application of the Software Engineering Principle “ain’t nobody got no time fo dat”
[building a compiler from scratch]. The author procrastinated and only had a few hours to build it, okay?
The main goal of this new compiler, hbc (the Happy Birthday Compiler), 2 is to compile C programs correctly but for
some subset of undefined behavior, play the “Happy Birthday” song. The subset of undefined behavior was chosen to
be specifically when a NULL pointer is dereferenced. 3
It is well-known that the general problem of reasoning about the runtime value of anything in a program reduces to
the Halting problem. Thus it’s clear that checking for NULL pointers cannot be done statically, and null-checks must
be inserted into the code. A LLVM compiler pass seems ideal for this, although ironically the pass will clutter the
code instead of optimize it. This would be done at the IR level, but we can simply consider LLVM load and store
instructions. The injected code can examine the dereferenced value and if NULL, can call our custom code that plays
Happy Birthday.
3.1

Getting the Music File

The author doesn’t fancy getting sued, so he didn’t want to rip a track of Happy Birthday off the Internet. Since this
compiler is certainly going mainstream, the costs of a commercial license would also be out of the question. Thus the
author painstakingly dragged and dropped sine waves in Audacity and recreated a tasteless rendition of the tune by ear.
He hopes that’s enough to avoid lawsuits.
3.2

Playing the Music File

It turns out there’s no playMyOggVorbisMusicFile() syscall in the Linux kernel. Who knew? The author eventually
decided to link against libcanberra, a Linux library that is apparently capable of playing audio files.
Using this, we create a C function that takes in a pointer, checks whether it’s NULL, and if so, plays Happy Birthday.
Since all of this has to end up in the compiler’s outputted executable, we embed the entire audio file into the C source
code as a base64-encoded string. Was this a good idea? The jury’s still out.
3.3

The LLVM Compiler Pass

The compiler pass doesn’t seem hard - just throw null checks onto every load or store instruction, and pretend that it’s
not going to destroy performance (it is). We could definitely make this significantly better by doing a dataflow analysis
and removing null checks for statically provably non-null pointers (e.g. pointers that have previously been checked),
but that would have taken effort.
But it was still difficult for the author because he’s bad at C++, especially LLVM’s variant.
3.4

Outputting an Executable

Because the author enjoys pain, he decided that the compiler executable would just be a bash script that drives each
portion of the compilation process. Yep. At a high level, the script compiles the input C files to LLVM bitcode, links
the bitcode into a big bitcode file (along with the bitcode for the function that plays the music file), runs the LLVM pass,
assembles the bitcode into assembly, and links it alongside its dependencies.
3.5

Shell Scripting Inception

As briefly mentioned, everything needs to go into a single compiler executable, including the bitcode for the music
player function and the shared object implementing the compiler pass. The author brilliantly decided to inline all of that
into the shell script, again using base64 encoding. And because he didn’t want to re-inline everything very carefully
each time any of the inlined dependencies changed, he wrote a bash script that generates the bash script executable
that compiles C programs into Happy Birthday-playing executables. If stacking more layers is a legitimate solution in
Machine Learning, why not here?
2
3

The code for hbc can be found at https://www.github.com/anbenson/hbc.
It was chosen because the author is lazy wanted to pick a behavior many programmers have experienced before.

67

Figure 1: I promise it’s a lot more fun when you can hear it playing Happy Birthday.
3.6

Testing

you always pass all the tests you don’t write

4

Properties

I claim that hbc is a standards-abiding C compiler.
Proof. By reduction.
We assume that clang is a standards-abiding C compiler. Let X be an arbitrary C program from the space of valid C
programs. Suppose X does not dereference a NULL pointer on any input. Then X’s behavior is identical on hbc and
clang. Suppose there exists an input for which X dereferences a NULL pointer. Consider the first instance in time at
which X dereferences a NULL pointer. This is undefined behavior, so anything after this point in time should not be
considered. But in everything before this instance, X has the same behavior on hbc and clang. Thus hbc has identical
behavior to clang on non-undefined behavior and thus it is a standards-abiding C compiler.

5

Results

The author tried a test case or two. They seem to work. So it probably works. But you should try it, it’s fun to see your
program suddenly burst into song when it dereferences a NULL pointer.
The author did not try any benchmarks, but they’re probably disappointing.

6

Applications

I dunno. It’s fun? It serves as a basis for a SIGBOVIK paper? It’s a fun aside in a 15-122 lecture?

7

Future Work

Nope, we’re done here.

8

Acknowledgements

The author would like to thank his 15-122 professors, Tom Cortina and Rob Simmons, who were part of the inspiration
for this project and helped cultivate an interest in C and compilers.

68

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 34: hbc: A celebration of undefined behavior
Reviewer: <noreply-utos@google.com>
Rating: <sigbovik@gmail.com>
Confidence: Learn more about our updated Terms of Service

We’re improving our Terms of Service and making them easier for you to understand. The changes
will take effect on March 31, 2020, and they won’t impact the way you use Google services.
For more details, we’ve provided a summary of the key changes and Frequently Asked Questions.
At a glance, here’s what this update means for you:
• Improved readability: While our Terms remain a legal document, we’ve done our best
to make them easier to understand, including by adding links to useful information and
providing definitions.
• Better communication: We’ve clearly explained when we’ll make changes to our services
(like adding or removing a feature) and when we’ll restrict or end a user’s access. And we’ll
do more to notify you when a change negatively impacts your experience on our services.
• Adding Google Chrome, Google Chrome OS and Google Drive to the Terms: Our improved Terms now cover Google Chrome, Google Chrome OS, and Google Drive, which
also have service-specific terms and policies to help you understand what’s unique to those
services.
• No changes to our Privacy Policy: We’re not making any changes to the Google Privacy
Policy and we haven’t made any changes to the way we treat your information. As a reminder, you can always visit your Google Account to review your privacy settings and manage how your data is used.
If you’re the guardian of a child under the age required to manage their own Google Account and
you use Family Link to manage their use of Google services, please take some time to discuss
these changes with them.

69

And of course, if you don’t agree to our new Terms and what we can expect from each other as
you use our services, you can find more information about your options in our Frequently Asked
Questions.
Thank you for using Google’s services.
Your Google team

70

Theory
14

A polynomial-time SAT algorithm
Anonymous Author(s)
Keywords: theoretical computer science, complexity theory, p versus np

15

Empire Machines and beyond
Darı́o de la Fuente Garcı́a, Félix Áxel Gimeno Gil, Juan Carlos Morales Vega
Keywords: finite-state machines, models of computation, Empire
Machines, doing research like this is awesome, Civilization
XXIV (when it comes out)

16

Artificial General Relativity
Pablo Samuel Castro
Keywords: physics, artificial, general relativity, quantum physics,
universe-changing, new science, field equations, black holes

17

Lower gauge theory: Dead duck or phoenix?
Oscar I. Hernandez
Keywords: mathematical physics, category theory, ToE, quantum
gravity, mathematical economics, marginal analysis

18

Making explicit a constant appearing in an impossible equation of Carl and Moroz
Matthew Weidner
Keywords: computational mathematics, Hilbert’s tenth problem,
Diophantine sets

71

14

A polynomial-time SAT algorithm
Anonymous Author(s)

Abstract
In this paper, we present an algorithm for solving the boolean satisfiability problem
in time O(n2 ), thus resolving the P versus NP problem.

1

Introduction

In the interest of preserving the triple-blind nature of the review system of this conference,
we omit the text of this paper.

72

15

E MPIRE M ACHINES AND BEYOND

Darío de la Fuente García

Félix Áxel Gimeno Gil

Juan Carlos Morales Vega

March 10, 2020

A BSTRACT
This paper addresses one key weakness in traditional State Machines: fixed behavior and number of
states. We introduce Empire Machines, which solve that problems and pursue an equal and completely
just world. They can also be used to model any Civilization game. Probably.

Keywords Finite-State Machines · models of computation · Empire Machines · doing research like this is awesome ·
Civilization XXIV (when it comes out)

1

Introduction

State Machines have a long history: the first instance of them, Markov chains, was described and formalized in 1906 [1].
However, State Machines have one key limitation: the behavior of states, and the number of states, is fixed at compilation
time 1 . This is obviously not realistic, as the conditions of the system being simulated may change. Moreover, states in
real life do not behave like this: they conquer each other, have revolutions and occasionally self-destruct, which is kinda
nice. The states of classical State Machines are too peaceful. The states with low probability are too conformist and
cannot get out of their misery while the states with high probability will always hold the power and laugh at the weak.
We feel in the moral obligation of ending this oppression that has been active for more than one hundred years. To
achieve than, we feel proud to introduce the glorious concept of Empire Machines, a novel architecture that can end all
of these problems and provide true justice.

2

Motivation

Look at the following map of the Roman Empire in 1356. Beautiful and chaotic, right?

1

Note that interpreted programming languages are the work of the devil. We will ignore their existence in this paper.

73

Figure 1: From https://commons.wikimedia.org/wiki/File:Golden_Bull_of_1356.png
Notice the perfection and marvelousness of this Empire Machine.
If one looks closely at the current map, one can notice that the distribution of countries is ""slightly"" different. Since
each country is effectively a State Machine, that can only mean one thing. We. Are. Right. States machines are not an
immutable entity, the can change form or size with time. Moreover, they are part of something bigger. In the particular
case of our image, it is the Ancient European Empire Machine (official name from today on). But we can go further...

74

3

Related work

For more background on the useless Finite-State Machines please see [2].
See (partially unrelated) [3].
See the author’s past work (obviously unrelated) [4].
Also important to understand motivation: https://civilization.com/en-GB/

4

Idea sketch

For inspiration (and also how to get the best intuition about Empire Machines) watch https://www.filmaffinity.
com/es/film485194.html while drunk. The authors do not encourage nor discourage drunkenness.
Now that you are questioning why that film even exists, you can continue reading. Since your mind should be by now
too occupied with that thought, you should not be able to question why our paper even exists. A clear win-win situation,
right?

5

Definitions and formalism

If we want to model a Empire Machine, we first need to distinguish two different behaviors: the interactions that happen
within a State Machine, and the interactions that happen between different State Machines within the bigger and more
glorious Empire Machine.
5.1

Intra-State Machine interactions

Our novel State Machine model can still be modelled in the same way as a classical one. Each state has a certain power
(AKA probability for noobs) and the sum of all of them needs to be equal to 1 (on further work we will expand the
model to the case where the sum of probabilities equals π (or e (or 3, all of them are the same after all))). The power of
the states evolves through time using the money transactions (AKA transition probabilities) between different states.
The power after a time step can be computed in the same way as in a classical state machine, by multiplying the Money
Transaction Matrix (or MTM, to make it shorter) times the current power.
So far so good. Now is where things start getting interesting.
5.1.1

Revolution

A revolution can happen if the following condition is fulfilled:

Pir < k

M
X
j6=i

Pjr
M −1

(1)

Where k is a State Machine-dependent non-negative, probably-positive, maybe-imaginary constant. For simplicity and
since we are very lazy, we will only consider the case where Pi is positive.
When a revolt happens, a state is killed. Since states die when they are killed [5], it disappears without a trace and its
power is distributed evenly between the rest of states. The MTM is recomputed by deleting the correspondent row and
column and distributing the transaction routes evenly between the other states.
5.1.2

Division

A division can happen when there are too few commercial routes between any two subsets of states, S1 and S2 , that
cover the space of the State Machine and such that the number of states of the smallest of the two is at least one third
the number of states of the biggest one. In this case the State Machine breaks apart into two different State Machines,
which become part of the Empire Machine. The total power of each State Machine is normalized to add up to one again.
The MTM breaks into two new MTMs and the money transactions are normalized within each column to maintain the
correct behavior of the State Machines.

75

5.1.3

Birth

A state can appear out of nothing whenever it feels like it. Seriously. When this happens, the newborn state starts with 0
power. The MTM is modified by adding a row and a column. The column is initialized at random (with the condition
that it adds up to 1) while the row is initialized by stealing some commercial routes from other states. The details of the
second process are far too complex and are beyond the scope of this serious paper.
5.1.4

Pandemic

The State Machine enters a chaos era. Everything is randomized. The powers are distributed between the states and the
MTM becomes a completely new matrix. Some states can also die, but the other states are usually too busy trying to
survive to even notice that.
5.2

Inter-State Machine interactions

A Empire Machine can be modeled in the same way as a State Machine, with each State Machine possessing a certain
power and some money transactions between State Machines. Hence, State Machines within a Empire Machine can
interact between them using the same processes as the states within a State Machine. But they are more awesome.
Additionally, there are a few operations that are specific to Empire Machines, as we describe below.
5.2.1

Fusion

If two State Machines have a strong commercial relation, they can fuse together with a certain probability that depends
on the two States Machines. The condition for the fusion of the State Machines i, j reads:
(

Pif Pjf

if

0

if

MTM1ij · MTM2ji ≥ 0.5

MTM1ij · MTM2ji < 0.5

(2)

The inner powers of the combined State Machine and the inner MTM are recomputed using the same principles as in
the birth case but with more states.
5.2.2

Conquer

Conquer works in the same way as a Revolution, but the condition for it to happen is different. Each State Machine can
randomly decide to conquer a different State Machine. The probability of the State Machine i to conquer the State
Machine j at a certain time step is:
P = Pic · (1 − MTMij ) · (1 − MTMji )
5.3

(3)

Other interactions

There are other interactions that are either spoilers or that we are investigating right now. We will publish the results in
Nature or in Sigbovik 2021. Who knows.

6

Hierarchy

We now define common variations of the Empire Machine depending on the type of state considered.
Definition 6.1. A Planetary Empire Machine is an Empire Machine where the states are themselves Empire Machines.
Definition 6.2. A Galactic Empire Machine is an Empire Machine where the states are themselves Planetary Empire
Machines.
Definition 6.3. A Multiversal Empire Machine is an Empire Machine where the states are themselves Galactic Empire
Machines.
We now present a different formalization of the hierarchy of k-level State Machines using inductive definitions.
Definition 6.4. A 0-level State Machine is a single state Finite-State Machine with 0 probability (because we want to
innovate).

76

Definition 6.5. A 1-level State Machine is a Finite-State Machine.
Definition 6.6. A 2-level State Machine is a Empire Machine.
Definition 6.7. A (k + 1)-level (with k ≥ 2) State Machine is a Empire Machine whose states are k-level State
Machines.

7

Analyses

Through careful analysis we got to know of the awesomeness of this idea, see Section 8 for more details.

8

Theorems

Theorem 8.1. Empire Machines can solve the Halting problem and therefore are models of hypercomputation.
Proof. Proofs left as an exercise
Corollary 8.1.1. Empire Machines are awesome.
Corollary 8.1.2. Galactic Empire Machines are even better.
Corollary 8.1.3. Multiverse machines require further research and more funding.

9

Relations to other complexity classes

Empire Machines win!

10

Relations to other decidability classes

Empire Machines win!

11

Pretty pictures
a


0.6 0
0
 0 0.1 0

0.3 0.9 0.9
0.1 0
0
0
0 0.1

0
0
0.5
0.2
0.3

Adjacency matrix

Figure 2: Unbalanced Planetary Empire Machine

77


0
0.4

0.4
0
0.2

Here we can see a normal Planetary Empire Machine. However, too many probabilities flow into Empire 2, whereas
very few flow out of it. This will cause the remaining empires to wage war against Empire 2, and destroy it. Here is the
end result:
a



0.675 0.225 0.125 0.1
0.075 0.325 0.125 0.5
0.175 0.225 0.325 0.1
0.075 0.225 0.425 0.3
Adjacency matrix

Figure 3: Balanced Planetary Empire Machine
(after violence)
The probabilities that used to go into E2 are now divided between all the remaining empires. This will ensure the
Planetary Empire Machine is peaceful again. Of course, until new empires appear or old empires gain too much power,
in which we are back to the beginning, but oh well.

Figure 4: Plot showing evolution with respect to time of a complexity measure (specifically suffering which is also
called productivity by economists), we have manually annotated relevant events to help explain the inflexion points, the
interested reader may notice the non-monotonicity with respect to time.

78

12

Empirical evidence for the superiority of Empire Machines

Nobody cares

13

Potential moral implications

Nobody cares (again)

14

Benchmarking

Nobody cares

15

Reproducibility

The authors make no guarantee that this is reproducible or worth anyone’s time. Someone tried to make code [6] but we
do not know if it matches the content here.

16

Future work

Magic

17

Conclusion

Happiness

18

Are there page numbers?

No!

References
[1] A.A. Márkov. "Rasprostranenie zakona bol’shih chisel na velichiny, zavisyaschie drug ot druga". Izvestiya Fizikomatematicheskogo obschestva pri Kazanskom universitete, 2-ya seriya, tom 15, pp. 135–156, 1906
[2] https://en.wikipedia.org/wiki/Finite-state_machine
[3] SIGBOVIK 2019 http://sigbovik.org/2019/proceedings.pdf
[4] RLIRFO https://github.com/juancarlosmv/RLIRFO
[5] A place where we learnt that people die when they are killed https://typemoon.fandom.com/wiki/Fate_
series
[6] https://github.com/juancarlosmv/MachineEmpires

79

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 16: Empire Machines and beyond
Reviewer: The authors’ professors
Rating: −∞
Confidence: As confident as the fact that the sun shines during solar eclipses

This is the best thing ever (after Turing machines and working quantum computers).

80

16

Artificial General Relativity
Pablo Samuel Castro
DAD, Joint Organization of Knowledgeable ExpressionS
@pcastr ← that’s my Twitter handle
Abstract
We (well, I) introduce a New Field In Science which we (I mean I) call Artificial General Relativity. We
(here I really mean ’we’) have all heard of General Relativity and how it revolutionized our understanding of
the world around us. Einstein’s work, although pivotal, failed in one crucial aspect: although it allowed us to
describe gravity and spacetime, it did not allow us to control them. In this paper I (switching to ’I’ to avoid
sounding pretentious with ’we’) introduce Artificial General Relativity (AGR) which, when achieved, will
allow us to control gravity and spacetime. I present a set of practical approaches to achieve AGR which serve
as reasonable baselines for future work.

I. Introduction
In the early 20th century Albert Einstein introduced the general theory of relativity, also
known as General Relativity (GR) (Einstein,
1915, 1916). This pivotal work broke the mental
barriers between space and time, demonstrating that they are incredibly intricately intertwined, interrelated, immersed, in the infinitely
immense spacetime. This theory has allowed us
to explain the majority of large-scale gravitational experiments thus far observed, as well as
implying the existence of majorly-cool things
like black holes. The latter has had major implications not just in Theoretical Physics, but
also in Science-Fiction writing (Ferrie, 2016).
Central to GR are the Einstein Field Equations (EFE), which relate the spacetime geometry to the distribution of the matter it contains
(Einstein, 1915):
1
8πG
Rµν − Rgµν + Λgµν = 4 Tµν
2
c

(1)

where Rµν is the Ricci curvature tensor, R is
the scalar curvature, gµν is the metric tensor,
Λ is the cosmological constant, G is Newton’s
gravitational constant, c is the speed of light
in vacuum, and Tµν is the stress-energy tensor
(Wikipedia, 2020a). In keeping with the best
practices of scientific research, I disclose that I

do not know the meaning of most of the terms
I just introduced. However, what is important
is to note is the equations’ structure as well as
the values which are constants: Λ, G, c, and π.
We will come back to these equations below.

i.

The First Shortcoming

A well-recognized shortcoming of GR is that it
has not been able to be reconciled with Quantum Physics (von Neumann, 1955, Ferrie, 2017),
eluding the dream of a Grand Unified Theory of Everything. More specifically, most
quantum field theories are defined on a flat
Minkowski space (Wikipedia, 2020b), which
can cause inconsistencies with the curved
spacetime of GR. Although quantum field theories have been developed for curved spacetime,
these are only under specific conditions. The
complete unification of these two main physical theories remains an open problem.

ii.

The Second Shortcoming

A less known shortcoming of Einstein’s GR
is that it is a powerful descriptor, but not a
controller. We are still unable to control the
force of gravity during basketball games, the
speed of the passage of time during finals, nor
reduce the gravitational lensing effect during

81

Artificial General Relativity

astronomical observations. In other words, we
are passive observers of a partially-understood
physical system. Einstein famously quipped
that “god does not play dice with the universe”;
perhaps that is true, but what if we could be
the ones throwing the dice?
A common question readers may ask themselves is: Why are the equations in (1) so complicated? While the answer to this question is
beyond the scope of the present work, we (yes,
you and me) may ask ourselves a related question: Can we make those equations simpler? To
quote Barack Obama: YES WE CAN!

II.

CAstro Field Equations

I will simplify Equation 1 via the following
steps.
1. Get rid of the cosmological constant. Einstein famously called Λ “the biggest blunder [he] ever made”, so I will start by setting it 0.

In addition to Theorem 1, it goes without
saying that CAFE is a strictly superior acronym
to EFE.

III.

In this section I directly address The First Shortcoming described in the introduction: how to
reconcile GR with Quantum physics. Given
that I am adopting the tried-and-tested approach of simplifying things, I make the following assumption.
Assumption 1. Really small things behave exactly
like really big things.
Under this assumption, there is no longer
any need for Quantum physics, and the inconsistency vanishes. I address two natural
questions that may arise:
• Isn’t this a really strong assumption? The
vast majority of the world’s population
has never seen anything at the quantum
scale. So: no.
• Why didn’t past physicists make this assumption? Although I cannot say with certainty,
I believe this may be a case of “mathiness” (Lipton and Steinhardt, 2019): as
physics equations kept on getting harder
and harder to understand, physicists had
an incentive to continue making things
more complicated.

2. Just use matrices! Tensors are hard to
visualize, so let’s just use matrices in R4
(3 spatial dimensions, and one for time)!
The above simplifications produce the CAstro Field Equations, or CAFE for short:
1
8πG
R̂ − R ĝ = 4 T̂
2
c

(2)

Where R̂, ĝ, and T̂ are all 4D matrices. I am
now ready to present the first theoretical result
of this paper.
Theorem 1. The set of equations described in (2)
are simpler than those described in (1).
Proof. This proof is presented in two parts:
Number of terms: The system of equations in
(1) has 10 terms, whereas the system of equations in (2) has 8.
Simplicity of terms: Both sets of equations
use the same real-valued scalars, so it suffices
to compare the non-scalar terms. In (1) they
are all tensors, whereas in (2) they are all just
4D-matrices. Given that tensors generalize matrices, the result follows.

To Quantum or not to
Quantum?

IV.

Artificial General Relativity

I now formally introduce Artificial General Relativity (AGR).
Definition 1. A universe is described by Artificial
General Relativity when Assumption 1 holds, and
CAFE perfectly relates the geometry of the universe
with the distribution of the matter it contains.
A critic may naturally question: could such a
universe even exist? Rather than taking this as a
criticism, I invite readers to take this as an invitation: Let’s build a universe where this holds!

82

Artificial General Relativity

ii.

Black holes

This is the crux of the choice of the word ‘Artificial’: we must create the universe that is consistent
with AGR. An astute reader may then observe
that desire alone is not enough: what evidence
do we have that this is even possible? Clune
(2019) argued that Darwinian evolution can be
viewed as a general-intelligence-generating algorithm, and serves as an existence proof that
the concept of general-intelligence-generating
algorithms can work. I follow a similar approach to introduce the second main theoretical result of this work.

In this section I propose a novel way of presenting experimental evidence: interactively!
As far as I know, this is the first time a Real
Scientific Paper has presented an intereactive
experimental section. As was previously mentioned, black holes are majorly-cool, so I would
like to present method for approximate black
holes in Algorithm 1. This method is based on
the fact that black holes are simply mass that
has been enormously compressed.

Theorem 2. There exists a universe that is described by AGR.

Algorithm 1 How to approximate a black hole
Input: Any physical object O
Input: A compressing device C
Ô ← O
while You still have energy do
Ô ← C (Ô)
Return Ô

Proof. The proof naturally follows by noting
the following:
1. Our universe seems to largely be described
by General Relativity
2. We have computers that can create virtual
universes satisfying arbitrary mathematical equations
3. It has been argued multiple times that
we actually live in a computer simulation
(Wachowski and Wachowski, 1999, Google,
2020)
4. Theorem 1.

As a simple application of this method, print
this research paper, take the first page (this
will be O), and crumple it as much as you can
(your arms are acting as C). I guarantee the
paper now has a stronger gravitational field.
The stronger you are and the larger the paper
you compress, the closer to a black hole it will
be.

V. Experiments
In this section I provide some simulations and
experiments as both proof-of-concept and as
baselines for future work.

i.

Simulation

In keeping with reproducible research, I made
a colaboratory notebook where you can plug
in CAFE equations and see colorful plots:
https://tinyurl.com/s5bxbbs. I display a sample run with structured matrices in Figure 1,
which results in structured projections. This
provides empirical evidence for Theorem 2, its
structural regularity, and will hopefully motivate others to develop more sophisticated simulations.

VI.

Conclusion

Many believe that achieving Artificial General
Intelligence (AGI) will solve all the world’s
problems (but how, specifically, is not clear).
I claim something similar, but more ambitious: building a universe consistent with AGR
will solve all the universe’s problems (but it’s
not clear yet if it will be our universe or the
new one). Artificial General Relativity, which
I’ve introduced here backed by both theoretical results and convincing empirical evidence,
promises to be an exciting new area of scientific
thought. It is fitting that this is happening in
2020, the start of a new decade! I look forward
to what new research in this field this decade
will bring.

83

Artificial General Relativity

Dimension 1 vs 2

1e45

Dimension 1 vs 3

1e45

Dimension 1 vs 4

1e45

2.2

1.95

2.2

1.90
2.0

2.0

1.85
1.8

1.8

1.6

1.6

1.80

1.75

1.70
1.4

1.4
1.65
0

1

2

3

4

5

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

Dimension 2 vs 3

1e45

0.0

3.5

3.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

Dimension 2 vs 4

1e45

3.0

2.5

2.5

2.0

2.0

1.5

1.5

1.0

1.0

0.5

0.5

0.0

0.5

1.0

1.5

2.0

2.5

3.0

0.0

0.5

1.0

1.5

2.0

2.5

3.0

2.5

3.0

Dimension 3 vs 4

1e45
3.5

3.0

2.5

2.0

1.5

1.0

0.5

0.0
0.0

0.5

1.0

1.5

2.0

Figure 1: Sample projections of T̂ from the CAFE equations, when solving with structured matrices.

VII. Acknowledgements

For their help in correcting this manuscript, I
would like to thank my esteemed colleagues in
the Joint Organization of Knowledgeable ExpressionS. In keeping with SIGBOVIK’s tripleblind review process I will only use their official company titles: MOM, AUNT, UNCLE,
MARLOS C. MACHADO.

References
Jeff Clune. Ai-gas: Ai-generating algorithms,
an alternate paradigm for producing general
artificial intelligence. CoRR, abs/1905.10985,
2019. URL http://arxiv.org/abs/1905.
10985.
Albert Einstein. Die Feldgleichungen der
Gravitation. Sitzungsberichte der Preussischen
Akademie der Wissenschaften zu Berlin, pages

84

Artificial General Relativity

844–847, 1915.
Albert Einstein. The Foundation of the General
Theory of Relativity. Annalen der Physik., 354,
1916.
Chris Ferrie. General Relativity for Babies. CreateSpace Independent Publishing Platform,
The Internet, 2016. ISBN 9781533181121.
Chris Ferrie. Quantum Physics for Babies. Sourcebooks Explore, The Internet, 2017. ISBN
9781492656227.
Google. Do we live in a computer simulation?,
2020.
URL https://www.google.com/
search?q=do+we+live+in+a+computer+
simulation.
Zachary C. Lipton and Jacob Steinhardt. Troubling trends in machine learning scholarship.
Queue, 17(1), February 2019. ISSN 1542-7730.
doi: 10.1145/3317287.3328534. URL https:
//doi.org/10.1145/3317287.3328534.
John von Neumann. Mathematical Foundations
of Quantum Mechanics. Princeton University
Press, 1955. ISBN 978-0-691-02893-4.
Lana Wachowski and Lilly Wachowski. The
Matrix, 1999.
Wikipedia.
Einstein field equations, 2020a.
URL
https://en.wikipedia.org/wiki/
Einstein_field_equations.
Wikipedia.
Quantum field theory in
curved spacetime, 2020b.
URL https:
//en.wikipedia.org/wiki/Quantum_
field_theory_in_curved_spacetime.

85

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 14: Artificial General Relativity
Reviewer: Reviewer Number Two-and-a-half
Rating: Humbug, all of it!
Confidence: At present, all things considered – and especially my vast background in computational heresy – I must say that I am about as confident
regarding this review, as my uncle is that the quotes he posts on Facebook
are indeed Einstein’s. In other words, I am absolutely bloody∗ confident.

After carefully not-reading the paper, I have convinced myself that the experiments are probably
lacking. Honestly, which paper would not merit from more extensive empirical verification that
nonetheless fits in the same amount of space? This is just a well-known fact about the academic
space-time continuum, and can therefore only be true for this paper as well.
Another key issue with the paper is that it does not seem to cite any of my own or my research
group’s papers! And I must ask: how on earth could the authors possibly have ignored such a key
piece of related work? Our papers should be cited at least three times in any paper on this topic.
Finally, I would like to point out that the paper is ridden with spelling mistakes. Obviously, I am
not asking the authors to be perfect, but ending perfectly innocent verbs with “-ize” instead of
“-ise” and nouns with “-ization” instead of “-isation”? Depravity!
I cannot do anything but conclude that this paper is complete and utter humbug, and should be read
exclusively by those who wish to have a good laugh.

∗

Yes, “bloody”, partially because I am indeed from across the pond, but mainly because I just slaughtered
your paper. No, I am not in the least bit apologising; in fact I hereby offer the authors my two-fingered
salute. Cheerio!

86

17

Lower Gauge Theory
Dead Duck or Phoenix?
March 31, 2020
Oscar I. Hernandez
Riverside, CA, 92507
oscar.hernandez@simons-rock.edu

Abstract
Gauge theory is used to describe the parallel transport of particles using connections on bundles. The use of higher gauge theory which uses 2-connections on 2-bundles to describe the parallel transport of points and 1-dimensional strings suggests the existence of a “lower gauge theory” which would apply {−2, −1, 0}categories to the study of parallel transport of lower-dimensional
objects. We will explore such a theory and its applicability.

1.

Introduction

While this may sound slick, it’s probably not worth pursuing a
mathematical theory which aims to solve problems regarding physical objects of negative dimension.

2.

Future Work

We encourage the reader to abandon this line of research, and focus
instead on ordinary or higher gauge theory and their applications to
physics and economics.

References
[HGT] J. Baez and U. Schreiber Higher Gauge Theory Hamburg
Preprint ZMP-HH/05-25. 2005. https://arxiv.org/abs/
math/0511710
[CPI] P. Malaney The Index Number Problem: A Differential Geometric
Approach Harvard University. 1996.

87

18

Making Explicit a Constant Appearing in an Impossible Equation of
Carl and Moroz
Matthew Weidner
Carnegie Mellon University
February 28, 2020
Abstract
Recently, Carl and Moroz [1] constructed a Diophantine (integer polynomial) equation for
which it is impossible to decide whether the equation has any (integer) solutions. However, their
presentation is not as explicit as it could be, instead involving implicitly defined polynomials
spread out over many pages. In particular, they give a two page description of a crucial constant
without bothering to explicitly compute it. We resolve this issue using a bit of Python, SageMath,
and 878 MB of disk space.

1

Introduction

The Matiyasevich–Robinson–Davis–Putnam (MRDP) theorem [4] states that Diophantine equations
(polynomial equations with integer coefficients) are “Turing complete”, in the sense that for any
recursively enumerably set of integers S, there is a polynomial f with integer coefficents such that
S = {n | ∃x1 , . . . , xk ∈ Z. f (n, x1 , . . . , xk ) = 0}. This famously implies that is undecidable to
determine whether a Diophantine equation has any (integer) solutions, giving a negative answer to
Hilbert’s Tenth Problem.
Another consequence is that it is in principle possible to take your favorite axiomatization of
mathematics, such as ZFC set theory, and construct a Diophantine equation such that, within that
axiomatization, one can neither prove nor disprove whether the equation has any solutions. Recently,
Carl and Moroz [1] did exactly this for Gödel-Bernays set theory, which has the same set of provable statements about sets as ZFC [5]. That is, they describe how to write down an “impossible”
Diophantine equation such that, within “mathematics as we know it”, one cannot prove or disprove
whether it has a solution. However, their presentation is not as explicit as it could be—the equations
are spread out over many pages, and some of them are not written explicitly as polynomials, instead
involving polynomial functions of other polynomials.
It would be aesthetically pleasing to write down their impossible equation in an explicit, easily
interpreted form. Such forms are known for a separate consequence of the MRDP theorem: just
as one can construct universal Turing machines, one can construct universal Diophantine equations,
which can be made to emulate any Turing machine by fixing some variable values. For example, there
is a computable encoding of Turing machines as triples of integers (z, u, y), and Turing machine inputs
as integers x, such that the Turing machine corresponding to (z, u, y) accepts input x iff the following
system of equations has a solution [3]:

88

A related fun result is the statement that any provable mathematical statement has an alternative
proof consisting of 100 additions and multiplications of integers [3, Theorem 5].
Our work is a first step towards putting Carl and Moroz’s equation into an aesthetically pleasing
form. Specifically, we compute a constant which is defined implicitly in their work using two pages
of equations, many of which are not true polynomials but instead involve polynomial functions of
other polynomials.

2

The Impossible Equation and its Constant

Briefly, Carl and Moroz construct their equation as follows. Let L denote the language of first-order
logic with a single binary predicate symbol. First, they construct a polynomial f (t, ~x) which is a
Diophantine version of a computer program that checks whether ~x encodes a proof of t in L. Thus
for any statement P of L, letting tP be their integer encoding of P , P is provable in L iff there exist
integers ~x ∈ Z14558112 such that f (t, ~x) = 0.
Next, they explain how to write down the integer encoding of the statement “the axioms of
Gödel-Bernays set theory imply a contradiction”.1 Specifically, the authors encode the statement
A1 ⇒ (A2 ⇒ (· · · (A15 ⇒ (∀x.x ∈ x)) · · · )),

(1)

where A1 , . . . , A15 are the axioms of the theory, and ∈ is the binary predicate symbol of L. The
axioms are meant to model that ∈ denotes ordinary set inclusion, and in particular, they imply that
∀x.x ∈ x is false, so this statement is provable iff Gödel-Bernays set theory is inconsistent. Thus
by Gödel’s second incompleteness theorem [2], the statement can be neither proved nor disproved
within Gödel-Bernays set theory, hence within all of ordinary mathematics.2
In the author’s notation, the integer encoding of (1) is denoted f15 (~a, 3), where f15 is defined in
equation (5) on page 49, and ~a is the vector of the values a1 , . . . , a15 defined in Section 8.
Our computation computes this f15 (~a, 3).

3

Computation

Our program3 is a straightforward Python transcription of the equations defining f15 (~a, 3) in [1]. One
exception is that we re-order the axioms in (1) by the magnitude of their integer encodings. This helps
to keep the magnitude of the overall result small, since the magnitude of B ⇒ C is approximately
the square of the sum of the magnitudes of B and C. With the original axiom ordering, the program
ran out of memory before finishing despite having 16 GB of RAM.
While the program could be run in Python, we actually ran it using SageMath [6]. This is about
100× faster due to SageMath’s use of the GMP library for integer arithmetic. It takes 11 minutes to
run on the author’s CMU-issued laptop.
The resulting value is 877,757,576 decimal digits long.

4

Result

We now include as many digits of the result as the editor will allow. We hope the reader finds its
aesthetics more pleasing than the original presentation in [1], perhaps even on-par with the universal
Diophantine equation reproduced above. The full result is available on a flashdrive by request from
Gates 5005.
1
They use Gödel-Bernays set theory instead of the better-known ZFC set theory because Gödel-Bernays has only
finitely many axioms, whereas some of ZFC’s “axioms” are actually axiom schemas which describe infinite sets of
first-order statements.
2
Unless the axioms are inconsistent, in which case we have bigger problems to worry about!
3
http://mattweidner.com/diophantine_impossible/diophantine_impossible.sage

89

545301191275333888444701339636003250478295495111109541645692746060650905891368348674933551376265185317292287250229880335953260950221013584134428647449
924235959658144275022695743954193227424930669407391950209107326165288751568000120693320811114098075764677681374644148775467055877727809304172187179647
203653479901107477092606996317362159447409056040590178568298028710198206142227341687254611422114262080445462179643008103376863968663800000529428056566
695625203950800210600411251141612864337550179304017966106528945313026379073842199545157616068260096095799999297373390022609680578099743507386916378544
127810195558352207068808792219590834862172328815682879321161938049540653107791138626960875999470472077995426505365709201076234444452428779172865531931
879242318884894620954444425565123558635747310231838779262285092655738228917477320971332794097419135728300077937610190697717966919945654486153496110531
725159959315492771715447877879616295592856879368167662393257243633136037644016257110592070208306454163421452982851205602583331884003969691983647496312
687380954460771395517965079737053303957103904583764921606505693215931816823669807454911524236297809084333734902634388557991182791004179213352767126051
477649095567744930033917716600797014283201164867033119172443760968785631969314287940442111993198477651786650446091288313791981817005637553541988320594
608697169744128340094124417299190837362752285410022945985549659124712450620950285311042570224749400708758495977996167605824265797255111465901661650648
304336316043722757688366000891240492112081990642824458976514642730043871902209625227886635099062978810711085882221144478148824287024729393476820104558
127775296178082051897808851040063447926413786454858791035648768240948655311729536714997407310972372822764231621632702924837481300222103761727230204687
347084236329309606663111928059691340573889876254662894538095227907187612062983099663418965779116893658411896721674751292789607777604679128769634246846
091666687845800418365979014595235742860570583969931194440268428503518919906825001001234722623966267747615691972928714792164216686815625038093465825771
555339969041849236842963455384063691086900191265302672229509660637110110311249249584782100230165876932850375703594009314965008978928169608416180603775
453937205108782925225164970954535998023779518684741398764853506916066958081103724337375209388680081409084016384934820622273584084839545302501474070116
421952997327163764266587061915585736708433359816950126961242887258809063959236161019337158575468986353417194272077303741382774134426223945721335425901
163538424808171733838704169414768046661664725783468246735502128993455335376881309335806197428583361189919584514732476735187041462201354630851270073918
332914975314415315609118484442994799927669266025027793579342285124542249256453435237264457716366723155624269969032889399710928868156042853449889513961
032694358794273344559057567883335577460430639204480647903558235380164718998667297902893526211909198353386664587673477886265759925929486385541422992024
172697610451076218546394227540555978143121030643109134852904339841923273046995946419612106546937437118359889054277101970606702754815743068579331062324
261421690895457987530109967696083843850262737571543570068103772731333897980987357449713363611371779185098492304943006846134399003184389688060315902982
604158359194674719251927829820701704761127576529865522443742706251667302243037403004611551423345649209293751656495042443038307572409394232876301747780
435377613263552140356656961983346631871921392253969534297620797551025938432811374698718285262679036593214121882411029051893808439345906779692817495769
748890703421237800696537317173888275326735207591838545014471601508964522631273406478618751728661565617155100829018564535404456071778353671549393542341
419788570727985750632807239830019255295444675064020737270206886651037064664155143213234924452526693058159127005287165697897828856763152583917513747352
470071647954196992372717490271135237714619999388167608029540069801962015190673762125710126897123544970388966387610197927492359767260715083856213687338
038715657594123579568273212231635587536221748726597120054103135237637773568667442611467320827223318571952497331385036422802039992613642082145990005465
404091648095172608962469509480963813861185732997707948320713899303333556061788361648391393503127849560775615999398261605508501691866069195517720328434
600652958398635618930678431134149055977499407094677362573795749641026495101048819897287257070979113141440400510096850012704560237666525429911097658297
493230368047518824379913231806933050132328786243198656892772839727690423113552262876409435604780167390827638098460610091922313777728586925822632518311
686643481019218601109532021734471555588252652342067586130520934865800849946806533922176870082007193186584516617448834866868158622161221137969886438435
229581520000869855639083774794887868761329652815880499758151814684132601963054749495649706393889875401966598648703778781941674324851500659619616975860
279338718775797330579100470376612229649120091820377529726301011822630161276616413767688260641124257667137245090914556636459856933646280436734116635156
646048875422555415521844250791994979414411344766714570541928910309108780588612880211863831446509446666614130179639517821083755664797375231477548799774
342232860122624158026644531903752714213568574181133926026698254520191059371599501686296620916429290965952587467829398614548208547150159155332546245681
517026170412248061402350498520328801299183127839614244576077371495114542125779678916434021373164497111776963092890490071360762761953169777052044504853
057749953542856826245445072450272462177532734530129554059994552241592107989013058486204470870957647587167328671344966288862637268523213433167678117845
002149370644691393321594465376550340835077974869510727570680447690506867662814198980087946285981013394044775930579317656306059316303464883414347686969
719808044777767415969033545284215801921184040966136973625950267637810930228683327309699795361702977006918139166773474728157842934795289160484629982027
213758849892567160690125956477447778607779146072414827264965126475042661163916155061646545200376374531091955160263732747087778031000765395089803492508
457106649722805558289800199202190265333463240737232263115834987784634023024231557986620347535933327956165335588279729818780896698969131292310331844582
490091071667867277275030977094925976416494388176448613117088204836967274221002974952706610615918291290950672699395250498029136327264161903402544488204
905782220822695074818718368009941462860364144175977038794518509938091971418878936260445822642998869571737783198792923318061294603127638647538829521060
373293079845932132891913118634446866264429377668238845558013088616751156368520100631155992610108572411653960893357592590924537243713464210050751429589
612273642370105265110591367560764346875104079459641943463646179911623399013859996941033503855173647797962689565747255326547866050737030845327459387643
928034836471393435690016292575957926263563944766209724537833360638125799725359362852930293525107588217034019886845581280412299050604499389308341918903
476893570921521146680204087539762385750575009499924135679595572175046868433534541060745179511469752613941843322518634676788651231580225843141480283536
482969873639110118818065662653189075938108524196555448458758249622977494803497564665665568658103404173471042589841170352109395715742737772209909817345
550827143093017043846188944810727481655933546030683455290410441708506154072852628519743448771170737076998504392228517696027562565012164082547009342432
698146479932385409818973949456761314359403757994977610714723356002696318714212101687118448628187701664315774229252502795098731882776164542623791050418
208693138763945968935173256625137383425866488387277171597283863966024513646556673632898787586250319284735322624998275959922149972719257007612138492543
033269303161748052268676862642233076290780708002093805323926312318717632571277564547515011217516888044393491964498998960696251433578046978251009728084
148305256608712117382899627618092932983357278773502930014723954015602752878532299451370294927456875033801390660546710951085898988817601703700283161443
518868352350951653572339527915748575989172674097156082301572524832630680209427860642977489071637766371960605764394294062711410937822080885124574444615
057437541393570187630885866731928430292132797855792973030653250612304426082890835743121438021514603785543930617681754596046168811340480698831868893254
937496703350060577816223389456981074628639798723722287901224336409989259419848861751676184941977402256871270092826081555541095930469121555372303139520
093944785857617356029464545492676161156993247458067980408196605376429544015756540197845834970686605594052117598353361607891062748134658722449208316074
333815498473698633461923297729477359554716184416667438372253005023160177779450661905981194745073012869626501573115352682662199255609298656947689759802
527952770595716650256096307824429251090659631344052121267430243120371027228551202660760286284289263717639493076877420887720336593097117861567613806183
371797803522808781471051906790863783531311307385759484654776231666356256898398824058822150839998844013496119646269089914367055634710788977341927870651
815982734694577239970255664845953102798022525552693019128239113879424060851877069864111260828157817510992534566840370523813162257743965346920403184346
053723862339295313335928009335310551229240913443669352582716353882928066956871352826559508629284917242771959009100999142688999591605403387546359432538
132971923344199222646934676118692600805176919087947356863386664022870870436941970294463100771785093142165678770152170415561787448845495845652382765949
936809414291527869318847639948920679605102892103714534565932583612839336859256183118957907129960682753702993093420668317355231987279651083849808234215
534357357047757879572097952113132996054300181579465118752966525056661767854356287196618518035081312621618749398103628267267534380668196154830448916319
836698887545790173027181778241239544431292277473080904009106337091929759174832940573854568188733781400868197509674911219799139136921381891752594759418
024854315134387095479847880212889617302176003706934430000394646644874070810266940004170119127129090408093559320592962932525236715284752228754228091798
405497228411026947001296897352114639725012631480020572902258413741709133684523696655525412483692552994093975020451854104483398813005174417681467583503
586772179856561973509245868552019721846333928752522184633461969272774958635722279510131803819032875031814301927864220309441141862126969339890090920045
556917374782134333707761902203322272241484428879353217986053829692567812925791792208140878871602473587992031314371271471406099624588730090401138410692
821826977909128203595471742446485223763857652978842660216166703221960686500331362219375461296127081918989807001675352342784918941362357798188859320964
078018419311642389412264541206092147885129753204559616605623528303447194631473247176340153429117586406586764573052277755038000221775601824662697557670
808806916396581544375483762501301897862718396765991240330958001764522369485747560021173262517870120377053329425268114579028961252617502770513088802373
377640931067730853366651677568449761170014852868878251762735090286834969290971910014080917532809508642770768882845524494789008481841829143057112808094
302938201818253578486740819680307299996492138141073855132090828610056271175109739150576611256387097559273493034942152103727416827578407266009560108864
738985641625548063957628744151994206217409025346442911482831451664392082926544951057444731428927438894566768906981871052552022180520336111974031509855
903880948862788853936248917027962528149295257109793955433405082739360416148984170871296292259818570444215983567217798726115448918139176627653082740530
737979302650146452075365406187333466698702694100073841072265814435253855695067015360466098668951788228461512097221448863674561109281670318244842951622
143555666902178561755866718369850234255717734904423183962144309366502879297750315770639530769027557173294979525519749373446870761208527765134477248320
246121567992015855518586906140516317918216426479621543126147053725126395064001931468841516969724159936596923203322262110328950068120523655462996631295
865950552501501449904466984036230101993586072308197259310318543734810336397576483506683304821926361125279740827317687625398987886850364120555878329874
186604291214163361737587200503974475921252368523018631131703224028018452472844998438032200180950619713553930710577857228981633331292592323393935707420
761078423005143374141239850576411760938164322127600610431770594422596597686599729573836873394184198758426110199728955077008195432379143516590629696066
845604490505626285726004607291914053582512930335571354949784689867766130463984559168468859209955524601174296926214665575920006308555982369232238748823
124473427433270661089837065907347455659619741502341777888243485237262097528884743540489299510138521014172890470072414636825880709789064318959126563902
297580768981617190173584203387754779456742769870684128496229372580591554899079646605894644178861467893689405972501210611279915364268249170779537167743
798185646724004550332062093372243795815219700237798519646629270117340328089252152167879119827095845339238606036004594548520435172326019566597772744865
573530754352625389421231844145550565721632686587114736318219167560722931815336985538665159574740017361899151057146475986636239592459930311346668376602
848868492538409262050352515573541636195568304290333607446922969040781388818718589415953592701940164503440795273714025053498630055542022446997939035641
864900600049352797856234476294619910434123421354870533295221864442444199560941571093070741722455902946495852065946472464818919612259965532947376541083
873430489259707102032837267617479766795727277497057285198982335039617037312532856771714402351381766106658611119172957632807054978903984725231193865791
228673144994709472015926422588892659984287322070064760082144608433735426929790561789936829398044037662402002791280841021071385506025980888321048132971
472830360192006382354430728311526334777577211284877945900064271308954429532135379994946999361204990291506777899005112024858430574215496356555710893485
934203141833775975220502418149098726560604236901645545907663876892006122844463155762943404321334937024598500700304796175064520746010511458520829606064
719263907872844442553614709923551858333116282892743979685996525858835888340707518123939457797547959645912735143558571752480741636858140702859131818835
024934086820419901052443549966733491775520305297682886924597770405739298049672650890925637499327450208082030333148200630866364050211889584973690946547
407730730883327403870908280965842821351623553674011337111108485999672417940483859372876865089986131323581100662663728598726368220345155385764141591025
145068950327615919959564191286316512435807589835926110830005625858564803495689733302850821621459453380393723778755097513584611858170415352349362099673
434331702865832174128952306556682873996122223993651285542792999124079763222372709624617006538242244665760375418889917386816565775714510960862438255147
724988120283006467860632573194930616200207495351425084623518415702760799118599461637320123241501775559548739034224711371289061291472994875576720834483
426874601930333875757105249272267274910263996174055495362100521197234653677367675414278055928005024603142816535025787676675782794891854063119248827592
915509225466058887164433008369599498749263159034973815795963535507322460870716075980068397393529684170425431919621241146064211643718172483194092855294

90

043757987422721471625302068198099064609033191029545019538674305595714152302584162738793231550321665480155781129514535263492908746183589930257858942917
882391387908915032793921920220819611157630094879822035474695386278961632343960454909192735091071749793404031802767797930972374126758818018277042365122
117588237091686192738533428526473269348344789815664150383170658132103062463970206858723161826879253588495359156845443585211357468322492995572451429556
571379036205848688967270027708460448559161971926280437075651126310626237177886060346150535636284272876650617833120541572023319080736375578813071607944
560502987930359720251666285505766582720742780867146851203440042586012343953346233553755097928643224704771062656001300024104085622166050662878312145705
721584567298769156921751979510443157738224311078859781173839447213659939338071639242532063776955562847104992469769458399728961335553730305371883521998
818181298714325361292970339992407111969096202094875561247293549100972219808863025671360514896391329124330660472787599457597515626789951318433302575800
840648317553911458918798626452503170987665274286931288269751149501771773317136441111304039275789435888798910785053928802190722965740303428271073344389
840228688157135681633464694869254801904502625900519288297552313004728875791380808953471660707895271992506244323797293877723704047258246046987541519055
908528140581247256442448819469116746180914283547092667022405513584305990148976245286722180296782115235610878545099794581971342410128294767562709645379
252187218291128816485145059937304314523834519320108377289810959965158026663157275930080896247366106175684198730702916667593306386225407193464328968456
033235254236695341514640312585769960813863621226889923499619761909480000795847785589726471936037672566569952094129592761565328608834171113360397820669
839289069148638572097590207258254778238509862113165173207926220022722575511322508683266454116067824951341809190782259517589668542054117768645785763224
593130729286502129548452273210047724858310067657720218058212621564463005153923063854100705552101530589203056455265334829866085338679247470841246553244
040404236762732976247003255546092321063605475658284345173286406653300943242271846260604737712808255830531500975705494292568687465725542426405489568269
494831691650721161062465887631338222740503782999201561094184209421151652465897687788450778131221224054948161693341017450112877216811066161237608182304
334154960660637899293916356176549068839393925920646766512424202054640714560563934564633402509497199652350444160177337289344466046351432551372202823079
905486870181408220331895359593194263817678482890220304398212750621607335517634734806899842702058535075991594150086098126588782841663760129691270009228
219446545603843757423229324068907296423832687827103461154590481185350488388500549404938120573279460091185569492696321981494952822241706854627765755382
186964344077194322426762833514338662562932761216275292784635653326215966753820427605872235234081586621600790250813741834063096798211789870851973950643
942996061304955495461327965200692654031295306640290483161236436259227097034805223893994796226983219344786456203268858647233281009620697912552310409052
720385724149717348106016250386668202396573086026567519092205778236622328082047374531086019768934673924803623419193813899603134278026296959974200490599
351828762551004857343440603612770270891392813300750737775781369579187567567897543146362569939691288109527339948735072298844352923764916227594642971599
079997518933180794927903249900281765520384461286595047993286003782887065747839211774790393809591922016373812913580878079873839289295014111430582126601
177877341349688402339514171603933666980690710793109170199823415950434139266218097751114395440101051047013326052340203225137580373930525425042951171764
797538909242023633471112155587604546820250699539629412984465973982440550935311477904841029347297881650898357233511052716130473948908258126194809258739
786929630091312543363222432028906198077593887420732176368109348374354333165427752771845903224366423901382393730710406347874589786070828123842374832953
387574711134696289179961347673696555173795450721272572552370773695348070708859809560840218359053212662969285118616186045285797018265442556266359823905
437686328945458348322865372194668964498378710537350836026730041534626980351517758725864709424534919190974503510339401902292954076093583939204940503994
533212885807481668280057153415077566345037380180553472381988234219617520214063861727523723003829416420631000676469015652336908137917882625433834100783
046403861263876619979092691115155847169180525564248017581097895665867831982819229281327645406502425976278364975314862407985239615006651139941847527208
413127581349451214822661896390314434298867678151369702321626405116556211135428677974137404377401854118749714774560812001698820813736509933053568152354
600853931336196182980578741907880824372336593568842698399146559877210354797130817954769658841009040587853423815630177325878096916629379550667709500421
345849496807900161766096714288488422298449718484031781770291083113757327002285359250340351613025401463325556901356321862402256710300732962927746525764
005977385419281945873052873708469737268847295507471217670471576299636711546906109631568622435915038070742831730735381592927612586048169396378633817797
451564552515873691438697220493872543694945720715143797446366677708284056882356792898765040853479242498369587062153496751045994188456895364558769462913
780374558049385709414972210550689828741845629576463481399544602136330440078014528117138307406090982827093595528760016515515217585099212030502394421397
719994703082514053347312718369660171560685142293545665894834019623940560749011351950315634626885929363026480797026214994867600061075289550654778628633
436089994969093425530280971924685397415800486791267354838327213808043508023163367128686704916225083529155098877220787085817751418963967490898856998848
192659202077074431100536807297677695440410671424379225853713794545201493777818031101468489404333265860439708229708247820250379820199946090891046351821
418165833661535301968578800469997923439615091435162959325170370635299789591059778245120633386574388995138064820023621773615156710944140176020082474171
108951908804050977908262835308857599587149949644464071920573453768348246382794531599172454262833933209430510816443170995405164792424037017570614681433
546546999858482437063639502119082235278554019913565931521205837465500886352764438056129449297463673449735744852703948483346099334956478284831900770381
282372219683858182321802769633895972151971030608565087957862083782221923029240623684940815539187668940087835674290501761893762834913308003011945356302
776525709429865267673641340019661842938328297214605356025595590930709431068806633616376833521118697246933738322967965537322828761510344669830500706634
504267306462114788906014316517408344804686552549376341568882109380116774710514379385251163225455272912929682193283839627316959154270285732339791301247
914794676527524083023242487569717893259676477397623104027099956734623259553655444103083247401426229704461227289154503503424728226464804447325804951908
186067371178587034501926716880411702037401240977123199852245975756694418977438212245599030478130680537533076572370516077259310384402267211417688962358
417231596598316068088408622857156843615738494265647252190880570748585511399534759096207737357363929828397597424971940576591111731332821826143339443603
946932888165675857467808661954190595595978663561331561041770536919661221602264278514808747265397792661866577850466155951536469939931395786736728499307
376591073952988232977089287049788511964614369912486684823870348203288197827209723117533667262197491033946921878477066038138654054878197056049502752002
885647312283578120467072660036185658124503729817833982919089166419751507201184887773368710340637322913761564860875442457157592715477544272143772766839
670583876728072627735068838432915199483374818199284820205902605622992430427847226160199204357600613234765599297228938063740551153447598960343804826253
022976969747510943907372963018653704070843746450553021382313941802612398779491350440490737846605041945796417397311054338626608196543067688466750715097
648601594841412139310001046215080653954419182422489337348893640933787683918975906967011549449887360064081694114231839984892134859841805434712087769031
605922411833512444113197939239997114802746701503683687973578726435378893124926179555222919655843775770196479570742614641325845656288911076372371748887
717036315110683879760954166966954702064243794134960601941719837308047950762569714336878345215764824171302726727100307444859044744194346323897767952949
414668748971573273550921755126989143443339996169595752103882258364241284309196713294221456000475684100101504592850117643400504530130624748897762818491
657119341948655166254764804877958658492110695395944562916749917400928431614000950507149382341992049946133525401026090318808897190672272618410039127521
932396360283428952701941408063023516327071147429381965897904150195112424224979749332658757624985336877464807387002580705852758143698160941770954901724
532327513946583712599067784445059600582026376933961735851802873461989208881880793739766015746870623607900473189543805992724480483428849451270094534340
304911358551880985743091287045239353724377502173675205138532297700284402205489431229355848710823531928219749836143721837411804491410392964874652930831
955354467602251489634483990040534048977589511318289110180944127477465165839140034801757511085330773629138638535050716047408567029912002542754021832233
289434929967065961650481042249529946728506249025067146172598638587403031120194247131623871877307550565631699718195661836805325925271723912478438258693
507971666744746905056925988486734726598454560667472051572585401706864143647534894479089591524315751312892126687916408910395283568225372553858274907594
206348565052459282391447068111270205455654600553295884506348917641102137327750390384420156663024849145758600847175899117674727213920386096228698377...

References
[1] M. Carl and B. Z. Moroz. On a Diophantine representation of the predicate of provability. Journal
of Mathematical Sciences, 199(1):36–52, 2014.
[2] K. Gödel. Über Formal Unentscheidbare Sätze der Principia Mathematica und Verwandter Systeme, I. Monatshefte für Math. u. Physik, 38:173–198, 1931.
[3] James P. Jones. Universal Diophantine equation. The Journal of Symbolic Logic, 47(3):549–571,
1982.
[4] Yu. V. Matiyasevich. Enumerable sets are Diophantine (in Russian). Doklady Akademii Nauk
SSSR, 191(2):279–282, 1970.
[5] M. Szudzik.
von Neumann-Bernays-Gödel Set Theory.
MathWorld–A Wolfram
Web Resource, created by Eric W. Weisstein. http://mathworld.wolfram.com/
vonNeumann-Bernays-GoedelSetTheory.html.
[6] The Sage Developers. SageMath, the Sage Mathematics Software System (Version 9.0), 2020.
https://www.sagemath.org.

91

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 8: Making explicit a constant appearing
in an impossible equation of Carl and Moroz
Reviewer: Dos Sleddins, Time Detective
Rating: Inevitable Accept
Z 2071
Confidence: 3.999991 τ (y)dy
2020

(where τ is Longwood’s temporal inference decay function; see SIGBOVIK 2059.)

Hey there, Harry. Long time no review—or at least, from your reference frame, anyway. I’ve been
hopping the chrono-eddies left and right, strange and charm (in far excess of the BLCSC’s recommended subjective-yearly intake, I might mention), in search of the beating heart of SIGBOVIK’s
research causality web. It’s this paper.
I made sure Reviewer Two woke up in a good mood that one dreary March morning in 2010. I
snuck subtle citation formatting bugs upstream into 2033’s new typesetting software. All to nudge
your legacy in the right direction for Ringard’s Paradox to finally be solved in 2071. 2020’s tripleblind causality protections were the hardest to crack of all, and for that I thank the PC; I truly
do—but for reasons that would undo my own existence were I to utter them here.
And now that I’m now, it almost feels like a formality to write a review for the Nexus itself.
Nevertheless:
Program committee, you must accept this paper. No; it is simply inevitable that you will accept it.
Although it may seem to be of totally unrelated subject matter, this paper lays the groundwork for
Highest-Order Logic and for the Lambda Timecube. The inspiration it draws from β-Reduction
Hero is both unprecedented and unsuccedented, although this author was too humble to deign to
spend even a footnote on it. Its multidisciplinary take on the Call for Papers will have opened
fifteen new realms of study for Bovicians worldwide in the years to come.
It could do with a few more explanatory pictures.
First time y’all been fully decorporealized this year, huh? Well, ya pulled it off fantastic despite
the circumstances. Have hope—it gets better. Nice page numbers by the way.

92

Security
19

Determining the laziest way to force other people to generate
random numbers for us using IoT vulnerabilities
mathmasterzach, StarChar
Keywords: babies, information theory, randomness, entropy, April
fools, hacking, the law, coffee shops, smart-toasters

20

Putting the ceremony in “authentication ceremony”
Camille Cobb, Sruti Bhagavatula
Keyword: authentication, ceremony, fun, security, weddings

93

19

Determining The Laziest Way to Force Other People to
Generate Random Numbers For Us Using IoT
Vulnerabilities
mathmasterzach

StarChar

Villanova University

Villanova University

zachdestefano[at]gmail.com

mholmblad13[at]gmail.com

Wednesday, March 32, 2020 Anno Domini
Abstract
The idea of this project is simple. Hack into a nearby IoT device and pull some
random numbers off of it. This device can be located either in the same room as you,
or your next-door neighbor’s house. Anything from the password of their lock screen to
the neighbor’s baby monitor. The goal is to take privacy-violating hacks on IoT devices
and use them just for random number generation in a legally dubious way while also
being as lazy as possible.

1

Introduction

In the past decade, the number of IoT devices in the world has been increased at an exponential rate [1]. There has been a similar exponential increase for generating random
numbers for use in cryptography, Monte Carlo simulations, digital simulations of the Monte
Carlo Casino, RPGs, generating hashes, lottery tickets, picking people to send off to war,
and grading papers.
These random numbers are secure against side-channel attacks because the best defense
is a good offense. Also, if someone else had the same access to your IoT device that we have
using this framework, then you should have bigger concerns about security than someone
knowing your random numbers [10].

2

Background

In order to understand this new discovery. The reader needs to understand a couple simple information theoretical concepts of high importance. We strongly recommend a full
understanding of the following concepts before approaching this paper: reading and the implications of the asymptotic equipartition property for discrete-time finite-valued stationary
ergodic sources.

94

2.1

Random Numbers

Two, ten, a million, one, sixty-three, four, four, four. These are some random numbers.
Rolling a die is considered truly random. Creating a good random number generator (RNG)
is a popular theoretical problem that was originally formally addressed by Donald Knuth.
[4] RNGs require a seed in order to generate numbers. This isn’t a seed that is used to grow
a plant, but a seed for creating hundreds of thousands of numbers. It is addressed later on
how our random numbers are generated.

Figure 1: An example of older random number generation technology
The goal of generating random numbers is always for them to be truly random. Unfortunately, many attempts at true randomness have often ended up being the creation of
significant pseudo-random number generators (pRNG). There is a well-known statistical test
suite made by Robert G. Brown known as Dieharder [3]. Many pRNGs have been tested
using this test suite. However, it is still unknown whether a true RNG actually exists.

2.2

IoT Vulnerabilities

An IoT (Internet of Things) device is a device which has at least one sensor and is connected
to the internet. These devices are useful for various levels of home automation; however, we
have discovered an alternate, more powerful set of features implicit in these devices. None
of the manufacturers that we surveyed list this feature in their device capabilities, so we
have taken on the task of making these ”hidden features” more known to the public while
demonstrating the extent and effectiveness of these features. As a result of our research, we
have discovered that most IoT devices have a built-in random number generator that anyone
can use with proper hacking. We are currently unsure about why these truly marvelous
features are not listed anywhere by the manufacturers.

3

Methods and Results

We will now demonstrate various theoretic frameworks for using IoT devices to get random
numbers from other people without them knowing. We will discuss how to get random numbers from other people’s babies while they sleep, from their phone passcodes, and from their
general habits (particularly those involving toasters). We will assess each of these sources
for their information theoretic properties to determine the rate of information production
by these stochastic sources so we might make proper comparisons between their information
entropy [6].
Due to budget cuts and time constraints, any proper calculations of entropy have been
replaced with approximations by using a large sample of data gathered from the source and
compressed with 7-Zip (7-Zip implements the Lempel-Ziv-Markov chain-Algorithm). Using

95

the Lempel–Ziv theorem which states that: the algorithmic entropy of a file is bounded
by its Shannon entropy (or something like that more or less), we can assume that our
experimental method of zipping large quantities of data suffices to produce a reasonable
ballpark estimation of the entropy of our sources. [2]
To quantify our results and for a proper comparison, entropy per bit is only one part of
the formula. We must also quantify the amount of work/time it takes to acquire this entropy
so that we can determine the optimal way to get entropy with as little effort as possible.
We have designed the following formula to help us calculate and ultimately maximize the
number of shannons per joule, the amount of entropy per bit as a function of personal
energy.
bits of entropy
bits of inf ormation
gathering work
+ activation energy
time

Given that there have been no prior references to this important, shannons/joules unit,
we have named this new important unit lazy-lokis which will be abbreviated in this paper
as zk.

3.1

Secretly Using People’s Babies for Random Numbers

A baby is a... wonderful noise maker. They make a lot of different sounds at different
volumes and frequencies. This is perfect when converted to numbers. Thus, the baby’s cry
is a successful RNG, because it is practically impossible to guess what numbers the baby
will produce next when they cry.

Figure 2: Example of a baby in a sophisticated rig for measuring information theoretic
properties
In order to assess the information theoretic properties of babies as noise sources for
random number generation, we must appraise and quantify them as sources of entropy.
Unfortunately, we were told that, ”under no circumstances,” would our Institutional Review

96

Board approve using human subjects for this project, but in order to advance science, we
undeterred by this bureaucratic hurdle, downloaded an open dataset of baby cries from
GitHub to estimate the information theoretic properties of babies.
The ”donate a cry corpus” is a dataset of baby cries which separates all baby cries into a
subset of the Dunstan Baby Language categories for cries (hungry, needs burping, belly pain,
discomfort, tired) and further qualifies the cries into sex (male, female), and age buckets (0
to 4 weeks, 4 to 8 weeks, 2 to 6 months, 7 month to 2 years, more than 2 years) [9]
Our results are documented in the following table with the units being the fraction of
the number of bits of entropy in each bit of data:
Data

M

0-4wk
F

M

4 - 8wk
F

M

2 - 6mo
F

M

7mo - 2yr
F

Hungry
Needs Burping
Belly Pain
Discomfort
Tired

0.7176
0.6920
0.8153
0.3763
0.7609

0.6618
N/A
N/A
N/A
0.6787

0.6576
0.9034
0.5767
0.6964
0.6685

0.6904
0.7285
N/A
0.6719
0.6383

0.7257
0.8900
0.7062
0.7068
0.7189

0.7211
0.7337
0.6948
0.8231
0.6436

0.7038
N/A
0.6571
0.8524
0.7230

0.6378
0.7295
N/A
N/A
N/A

Average

0.6724

0.6702

0.7005

0.6823

0.7495

0.7233

0.7341

0.6837

Assuming that all five states that a baby can be in occur with equal frequency, we can
assume that the information entropy of a baby’s cry is approximately .7020. That is to
say that we can expect to get .7020 bits of entropy on average for every 1 bit we siphon
from baby’s cries. If we have the fortune of selecting a specific baby for random number
generation, we should prefer to choose a male baby 2 months or older, although this does
not give a major increase in entropy over other categories.
Future research in this area can involve provoking babies with light and sound features
on the cameras to see if this increases the entropy of their cries, and expanding our dataset
to include categories for which there was no data available to us.

3.2

Secretly Using People’s Habits for Random Numbers

Linux uses various sources such as timing in between user interaction events as entropy
which is stored in /dev/random to be used later, but when this runs out of entropy it is a
blocking operation while it waits for more entropy. This is relatively limited because Linux
machines are often lifeless servers connected to the internet running from within a dark
room for decades at a time with minimal human interaction.
These machines are deprived of the warmth and love required to create random-ish
numbers, but it is possible to, with the power of modern technology, allow these machines
to experience the joy of an ”endless” stream of random numbers without any need for
warmth or love. This process involves taking devices that individuals use directly more
habitually such as smart toasters, smart water bottles, smart fridges, and smart crockpots,
and collecting and transferring the human interaction information so it can be used for
random number generation.
Now perhaps you might begin to protest and say, /dev/urandom is universally preferred
over /dev/random and neither offers better randomness because they are ultimately run
through CSPRNG but /dev/urandom is better because it doesn’t have the blocking issue
and thus breaking into other people’s toasters is a wildly unnecessary and ethically dubious

97

way to get random numbers. We answer that strings of true random numbers are like
beautiful heartfelt poems and thus you are a joyless husk of a human being to believe that
the pseudo-random numbers that a lifeless computer generates are in any way of the same
caliber.
Human interaction with these devices on the macro level is largely predictable, ex.
toaster used at breakfast same time Monday-Friday within a 10-minute window with the
time dial set to about 1 minute or fridge opened at precise 5-minute windows during the day
to get drinks, snacks, and frozen pizzas, but at a more precise level, the variations within
these patterns is completely random. The number of milliseconds above or below normal
modulus some desired small value has an entropy value that approaches 1 bit per bit on
analog toaster input. There is the first issue for us that this exceedingly high-quality randomness is only found in a smaller quantity; however, this is not the biggest issue with this
method. Many IOT devices in this area have shifted away from allowing this error through
digital interfaces. This poses a potential risk, but for now, it appears to be safe to use this
method in production.

3.3

Someone’s Lock Screen Password

Encryption often involves a salt, which is
Definition 3.1 (Salt). Random data that is used as an additional input to a one-way
function that hashes data, a password or passphrase.
Salts are generally small numbers. Frequently on the iPhone, a user’s password for their
lock screen is 4 numbers. The iPhone is the most common kind of phone. This is a perfect
salt for an encryption algorithm. Thus, the lock screen code is a very useful set of random
numbers. Plus, outside of general cases and permutations of the classic 1234 password,
phone passwords have an extremely wide variety.
The issue arises when we discuss how to get these passcodes. Let’s say we’re inside the
comfort of our home. Then the easiest way to get the passcode is through the neighbor’s
WI-FI or picking up the signal off a nearby cell tower, which is a ton of work. An easier
method is to go to the nearby coffee shop across the street because everyone connects to
that coffee shop’s free WI-FI capabilities. The process is simple, walk in the coffee shop,
buy a coffee, sit at the corner table and act like you’re writing the next great novel like most
people. Meanwhile, lots of phones and people come through, and we harvest lots of phone
passwords. As optimal as this is, lots of people visit the same coffee shop. Thus, to keep
the randomness, we need to frequent many different coffee shops. The amount of zks that
this will cost is exponential.
Considering the cost, these must be incredibly random numbers to be worth our time,
and as it turns out, using a collection of passcodes sorted by frequency [5] we calculate
13.2627 bits of entropy per symbol where symbols are the set of integers 0 through 9 which
when the divided by the number of bits per symbol, gives us .9981 bits of entropy per bit
of data.

4

Method Comparison

With the values above, we now believe ourselves to be adequately prepared to ask the question of which aforementioned source is best for random number generation. For comparison,

98

we will not use the raw entropy values, but instead, lazy-lokis to account not only for the
optimality of the entropy of the source but also to account for the effort required to gather
quantities of this data. The exact calculation of these values can be trivially done with the
data provided and our formula above, so this calculation has been left as an exercise to the
reader.
We have performed these calculations, and have determined that the method which
maximizes lazy-lokis is hacking into baby monitors. Though this method generates fewer
bits of entropy per bit of information than some of the other methods such as passcode
stealing or using toasters and fridges, the sheer number of bits per time and the ease of
access puts it over the top by several orders of magnitude.
Perhaps a phone passcode contains more entropy, but to steal phone passcodes I have the
tremendous activation energy of driving to a local public shop or gathering point, potentially
purchasing a beverage at said location to appear inconspicuous, and making sure to hit a
different location each time so that I am not sampling the same set of passcodes each time.
Similarly, the time delay in waiting for individuals to use their toasters and fridges limits
random generation to specific time intervals and collecting larger collections of random
numbers relies on hacking into more devices and waiting for each of those. Thus for the
purest form of distilled random numbers, toaster and fridge micro-input information, the
cost of acquiring these numbers is also maximal.
The baby monitor approach, however, allows me to get a continuous stream of data
from other people’s homes throughout the world from the comfort of my own home. The
activation energy cost approaches 0 as I have around 2 years worth of data that I can get
from any individual baby without the baby’s data becoming stale or predictable. There
is, of course, the issue of having a male baby vs a female baby as there is no way to tell
what kind of baby is in the household. This gets tedious to find out, but it’s a lot more
comfortable than going to the coffee shop and buying something to fit in with everyone else
that is there.

5

Experimentation

Our non-existent lawyers have told us that to say that we DEFINITELY have NOT, do
NOT intend to, and do NOT encourage experimentation with these methods; however, if
one was to hypothetically attempt to test one of these methods there are various things that
they could try.
The following case study discusses many ways that have worked in the past to remotely
access any baby monitor’s audio feed [7]. Using the national vulnerability database, it is
trivial to find more examples on which the same methods are effective. We have been advised
to say that we have NOT tried any of them and do NOT advocate taking a look at various
methods for getting remote access to baby monitors and trying them for yourself.
We are not lawyers and carrying out any of these tests, (which we did NOT), would be
a legal gray area because, according to the definitions of what constitutes a crime in the
Computer Fraud and Abuse Act (United States Code 18 Section 1030), taking entropy from
someone without them knowing via one of their smart devices does not appear to directly
violate this act [8]. Seriously. Take a look. Then consult a lawyer just to be safe. And let
us know what they say.
The closest thing to violating this act that we could find can be found in the statement:
”knowingly and with intent to defraud, accesses a protected computer without authorization,

99

or exceeds authorized access, and by means of such conduct furthers the intended fraud and
obtains anything of value, unless the object of the fraud and the thing obtained consists
only of the use of the computer and the value of such use is not more than $5,000 in any
1-year period” [8]. Considering that the thing obtained consists only of the use of the
computer and the entropy we are gathering probably isn’t worth more than $5,000 then you
are PROBABLY safe, but then again, we aren’t lawyers.

6

Conclusion

Thanks to this careful research we have reached a number of practical conclusions about
random number generation that we expect to be taught in every information theory class
as early as next semester. Zerothly, the higher quality the random numbers are the rarer
and more difficult they are to acquire. Firstly, ease of access and random number quality
are inversely proportional which implies that the lazy-loki metric as a function of entropy
is continuously decreasing on the open interval (0,1). Secondly, male babies exhibit higher
variance in entropy generation when compared to their female counterparts at all ages which
lends evidence to the greater male variability hypothesis.
It can be trivially seen that in general using the cries of babies is the best option for
collecting random numbers from other people and that the highest quality of random numbers are quite rare and come from smart toasters. It is left as an exercise to the reader to
continue testing this idea.

References
[1] A Cursory Google Search
[2] Avinery, Ram Universal and accessible entropy estimation using a compression algorithm The Raymond and Beverly Sackler School of Physics and Astronomy, Tel Aviv
University, 2019. Print.
[3] Brown, Robert G. dieharder Duke University Physics Department, 19 June 2017. Web.
[4] Knuth, Donald The Art of Computer Programming Volume 2: Seminumerical Algorithms Addison-Wesley, 1968. Print.
[5] Miessler, Daniel SecLists: The Pentester’s Companion GitHub Repository, 2012. Data.
[6] Shannon, C. E. Prediction and Entropy of Printed English Manuscript, 1050. Print.
[7] Stanislav, Mark HACKING IoT: A Case Study on Baby Monitor Exposures and Vulnerabilities Rapid7, 2015. Print.
[8] United States Legislative Branch Computer Fraud and Abuse Act (United States Code
18 Section 1030) Comprehensive Crime Control Act of 1984. Print.
[9] Veres, Gabor Donate a Cry Corpus GitHub Repository, 2015. Data.
[10] Wurm, Jacob Security Analysis on Consumer and Industrial IoT Devices Department
of Electrical and Computer Engineering, University of Central Florida, 2016. Print.

100

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 4: Determining the laziest way to force
other people to generate random numbers for
us using IoT vulnerabilities
Reviewer: Anonymous
Rating: lukewarm, getting warmer
Confidence: Confident that the organizing committee are FOOLS

The paper is acceptable, if nearsighted in its understanding of the implications of its findings.
HOWEVER the authors are NO WHERE as nearsighted as the organizing committee, which
has provided me with the perfect vector for injecting as many page numbers as I please into
the proceedings, allowing me to harass the proceedings chair under the full cover of anonymity.
Behold, as I inform everyone that this is Page 1 of 1, equivalent to i in roman numeral, or
0x425d22427978b6bc28a67815c2a61353e20a5c91861a656ac8ca6faa79299ec0 under the numbering scheme described in Cryptographically Secure Page Numbering in LATEX! Haha! Ha! Muahahaha!

1

Camille Cobb and Sruti Bhagavatula
20

Putting the Ceremony in “Authentication
Ceremony”
Abstract: In this paper, we seek to make authentication
ceremonies for secure messaging apps more fun and enjoyable by incorporating them into important events in
people’s lives, e.g., as part of a wedding ceremony. We
design and develop a prototype using highly computer
scientific methods and evaluate its usability through an
equally scientific user study of one of the author’s dads.
We conclude that our proposed ceremonies are fun and
not boring and the authors would totally do this ceremony if they ever needed to use a secure messaging
app.

terface” on “Signal” so that it is less “confusing” with
limited success [11]1 .
In this work, we seek to put the ceremony back into
authentication ceremony. Or, rather, we seek to put it
there, because as far as we know it has never been much
of a ceremony. Instead of trying to improve the usability
of a technical system, we instead tackle the problem of
user motivation, incentive, and desire to use the authentication protocols. By amending the authentication ceremony such that it creates an opportunity to (1) foster
human connection and deepen relationships, (2) (????),
and (3) Do It For The ’Gram, we show that at least
one user (i.e., the participant in our study) is probably
at least a little more likely to successfully complete an
authentication ceremony when we show him how to do
1 Introduction
it next week.
Our novel, cutting-edge ceremonies will revolutionEvery day, over 100 billion instant messages are sent between users across the globe [2], sometimes about sen- ize communication. Everyone will be happier, more sesitive topics such as when I couldn’t poop the other cure, and less constipated.
night and told my mom I wasn’t leaving the house until
the situation changed. Man-in-the middle attacks, buffer
overflows, and many other buzzwords mean that these
2 Background
messages are not secure. Thankfully, substantial work
in the area of cybercryptography has offered solutions
We are writing this paper while working from home,
such as encrypted messaging.
which means it is much harder to access University reOne popular secure messaging tool, Signal, offers
sources such as the ACM Digital Library. With that
encryption. This is good. But there is still a risk that
in mind, we present a thorough literature review based
users are sending messages to an adversary rather than
only on the abstracts of papers and their lists of cited
to the intended recipients. This is because the problem
papers, which are available without logging in. This is
of key management has not been sufficiently solved. Auonly a light departure from typical background sections;
thentication ceremonies is a term used to describe outmany readers may not even realize that best practices
of-band key exchange, which ensures that the person in
encourage actually reading the work you cite. In fact,
the phone is the person you know IRL.
the author writing this section of the paper has not even
Although authentication ceremony sounds like it
skimmed the parts that the other author wrote, so who
would be a fun event, it is actually not fun at all. In
knows if this is relevant – we definitely do not.
fact, users avoid doing it and struggle to do it right, as
demonstrated in prior work [12]. More recent research
has sought to improve the “usability” of the “user in-

2.1 Established Types of Ceremonies

A selection of really fun ceremonies are shown in
Figure 1, including weddings, Olympics opening cereCamille Cobb: Carnegie Mellon University
Sruti Bhagavatula: Carnegie Mellon University

1 Actually, they were pretty successful, but that doesn’t fit with
our narrative.

102

Putting the Ceremony in “Authentication Ceremony”

Fig. 1. Most ceremonies are fun and meaningful. The Signal Authentication Ceremony is boring.

103

2

Putting the Ceremony in “Authentication Ceremony”

3

monies, and graduation ceremonies. Not all types of parties. Our work promises to significantly increase the
ceremonies are fun. For example, for some reason the search-result-share of authentication-related parties.
SIGGRAPH ’19 opening ceremony has an ACM Digital
Here are a few papers that are actually about secure
Library entry [6] and was almost certainly less interest- messaging, key exchanges, and authentication protocols,
ing than the Olympics opening ceremony.
including a couple that involve studying the usability
We cite one additional paper because it was pub- (or lack of usability) of Signal’s Authentication Cerelished in a computer science venue and has the word mony [7, 8, 10, 11, 13? ]. We omit any deeper discussion
“ceremony” in it. It talks about how to 3-D print spoons of these papers, not because they are not relevant, but
for Japanese tea ceremonies [9]. We decided not to pro- because we started off way more ambitious in our plans
totype a ceremony that involves using phones as tea to write this paper and are now running out of steam
spoons, because not all phones are waterproof and al- on this idea.
most no phones have a concave shape that would be
A screenshot of the Signal Authentication Ceremony
amenable to spooning tea. However, fast-moving ad- is shown in Figure 1 (d), along with an artistic depiction
vancements in smartphone technology may change this of how boring it is, demonstrated by a photo of a person
landscape and make tea ceremonies attractive for fu- modelling for a stock photo.
ture work in being adapted for use as authentication
ceremonies.

2.3 Human Nature

2.2 Secure Messaging

In my intro psych class in college, I learned that sometimes people are happier when they have work to do,
This paragraph is copied and pasted from an email a games become boring when they are too easy [1], etc.
colleague sent after I asked them to explain secure mes- Also people want to connect with others. Although prior
saging to me. I didn’t read it, but he’s probably right:
work concludes that the Signal Authentication protocol
is too hard for users (one of these should be cited, but
For any secure messaging system, in order to do encryption
I don’t have time to figure out which one so I’ll cite all
you need to swap keys with whoever you’re communicating
of them [7, 8, 10, 11, 13]), what if they were wrong
with. And when you’re exchanging keys, you need some way
to authenticate/establish trust that the key belongs to who and it was actually too easy and not fun enough? I
you think it does - like if you’re emailing public keys to mean, I haven’t fully read those papers, but I’m sure
each other, how do you know that it wasn’t intercepted and they reached the wrong conclusions.
replaced with an adversary’s key? The only real way we
have to do this, is either to trust some third party authority
(like certificate authorities, for web/HTTPS), or to do it in
person.

Just because the word “party” appears four times on
the Wikipedia page for “key exchange” does not mean
that it is a party [3]. The one potentially fun type of
authentication ceremony precedent is key exchange parties. But when you Google “key party,” the results are
not about authentication. In fact, the most prevalent
search results when you search for key parties involve
key exchanges that are the opposite of secure and do
not encourage meaningful, deep human connection [4].
The problem is that you have to search for “key signing party,” but these really haven’t caught on: searching
for “key signing party” yields About 204,000,000 results
(0.58 seconds) whereas searching for just “party” yields
About 11,680,000,000 results (0.96 seconds). This means
that under 2% of parties are key signing parties, but
also demonstrates that there is substantial demand for

3 Methods
3.1 Developing and Prototyping Novel
Authentication Ceremony Protocol
Designs
Okay, so really we just thought about this for a while.
Then we explained our ideas in a lab meeting and other
people came up with more ideas that we’ve stolen. We
think they might have been joking, but we were serious.
We narrowed down from our original three design ideas
to three final design concepts, based on our intuition
that having at least three designs to compare would
sound really good in a final paper.
We prototyped each of these designs, as shown in
Figure 2. The prototyping process and a sample of the
unity candle authentication ceremony is shown in Figure 3. A ceremony involves a script of vows read out by

104

Putting the Ceremony in “Authentication Ceremony”

4

the authenticating parties. To create this script, we first
4 Results
found an existing unity candle ceremony script [5] and
then used advanced HTML inspection skills to change
the text on the website. Utilizing the developer console 4.1 User Evaluation
is how you know we are doing Computer Science ReThe participant reported that authentication to him
search™.
means logging into his email account online. When
asked if he would want to exchange keys with people
with whom he had an emotional connection, he ap3.2 User Study to Evaluate Proposed
peared flustered and confused and said something about
Designs
“darn technology”.
He was then asked to give his opinion on the new
We attempted to solicit opinions about authentication Signal authentication methods we proposed and he said
with the people in our lab but they simply laughed at he didn’t know what “Signal” was. We were not able to
us. As we were worrying about where we would get par- get any further data from the participant after this part
ticipants, one of the authors’ dads called them to warn of the study as he got too confused to be able to provide
them about the Coronavirus and we just decided to ask any meaningful answers. He was however excited by the
him while he was on the line instead.
wedding pictures and was eager to know who was getting
We asked the author’s dad two questions: 1) what married.
does it mean to you to authenticate? and 2) Are the
In Figure 4, we present made-up data that shows
people you want to exchange keys with also people you that the designs we came up with are better than any of
want to be closer with, emotionally?
the existing ideas. Based on the findings of our study, we
As the final part of this study, we showed him the concluded that our proposed authentication ceremony is
current Signal Authentication Ceremony and our proto- probably good enough since who doesn’t like parties and
types (Figure 2).
ceremonies?
We started with the Unity Candle Authentication
Ceremony Design. In this system design, the authentication ceremony is incorporated into a wedding between
two loved ones. With the underlying assumption that 5 Future work
the two parties getting married trust each other and
want to exchange keys, the exchange of keys must be We hope to expand our new proposed authentication
executed after any official ceremony that officially weds ceremony to other ceremonious occasions, beyond wedtwo people. This method is generalizable to all wedding dings, blood oaths, and graduations. In particular, our
most immediate next step is to bring our authenticatraditions regardless of beliefs and cultural alignment.
We then asked the dad the following questions: tion ceremony to childbirth so that a mother can bond
1) Does authentication seem easier with this new to their newly born-child. Other potential expansions inmethod; 2) Would you feel closer to the other person clude children’s birthday parties and new year parties.
Now that we have authentication ceremonies, future
with this new authentication ceremony method; and
work
also needs to consider the case that two people
3) Are you likely to do the new authentication ceremony
would
not like to be authenticated to each other anywith people in the future?
After this, the dad was tired of participating, so we more, i.e., an un-authentication ceremony might be necessary. This could occur as a results of a break-up or
made up results for our other two design ideas.
The participant was not compensated and did not divorce. In this case, the parties would need to meet
give consent, but we did say “thank you” at the end one last time to revoke their keys. Future work could
of the call. These methods were not done with IRB explore the design of this mechanism.
Finally, there is a plethora of possible directions in
approval, because when we called the IRB, they told
applying
ceremony to general security tasks. One exus that this would not produce “generalizable knowlample
could
be that when a computer requires its user
edge” and so could not be considered human subjects
“research.” Our study is based on data from one of the to update its software, it doesn’t prompt the user at
authors’ dads. Henceforth, we will refer to him simply all during the process and instead instructs the user to
take a calm relaxing bubble-bath while it is updating.
as “the participant”.
Therefore, in this way, the user does not have to en-

105

Putting the Ceremony in “Authentication Ceremony”

Fig. 2. Prototypes for three novel authentication ceremonies.

Fig. 3. A proposed script for a unity candle authentication ceremony, also demonstrating our meticulous prototype development
process.

106

5

Putting the Ceremony in “Authentication Ceremony”

6

Fig. 4. As you can clearly see, the proposed authentication ceremony designs from this work (shown in red) outperform existing ceremonies in terms of social connection and/or ability to do secure authentication.

gage in the update and can improve their mental health
simultaneously.

6 Conclusion
In short, we see that authentication ceremonies can be
made more usable and enjoyable by incorporating them
into important life events in people’s lives. We have not
been able to test the usability of the new system as
we needed to wait for someone we know to get married
and enforce this new proposed authentication ceremony.
However, based on how fun we thought this would be,
we were able to make claims about the enjoyability of
such a mechanism.

References
[1]
[2]
[3]
[4]
[5]

Have you tried re-playing Zoombinis as an adult? It is not
good.
I personally sent 20 SMS messages today, and there are 7.7
billion people on Earth.
Key exchange. https://en.wikipedia.org/wiki/Key_exchange.
Accessed: 2020-03-13.
Key party.
Unity candle ceremony. https://www.officiantguy.com/unitycandle-wedding-ceremony/. Accessed: 2020-03-13.

[6]

Opening ceremony and award presentations. In ACM SIGGRAPH 2019 Awards, SIGGRAPH ’19, New York, NY, USA,
2019. Association for Computing Machinery.
[7] Marcelo Carlomagno Carlos, Jean Everson Martina, Geraint
Price, and Ricardo Felipe Custódio. An updated threat
model for security ceremonies. In Proceedings of the 28th
Annual ACM Symposium on Applied Computing, SAC ’13,
page 1836–1843, New York, NY, USA, 2013. Association for
Computing Machinery.
[8] Michael Hart, Claude Castille, Manoj Harpalani, Jonathan
Toohill, and Rob Johnson. Phorcefield: A phish-proof password ceremony. In Proceedings of the 27th Annual Computer Security Applications Conference, ACSAC ’11, page
159–168, New York, NY, USA, 2011. Association for Computing Machinery.
[9] Pierre Lévy and Shigeru Yamada. 3d-modeling and 3dprinting explorations on japanese tea ceremony utensils. In
Proceedings of the Eleventh International Conference on
Tangible, Embedded, and Embodied Interaction, TEI ’17,
page 283–288, New York, NY, USA, 2017. Association for
Computing Machinery.
[10] Kenneth Radke, Colin Boyd, Juan Gonzalez Nieto, and
Margot Brereton. Towards a secure human-and-computer
mutual authentication protocol. In Proceedings of the Tenth
Australasian Information Security Conference - Volume 125,
AISC ’12, page 39–46, AUS, 2012. Australian Computer
Society, Inc.
[11] Elham Vaziripour, Devon Howard, Jake Tyler, Mark O’Neill,
Justin Wu, Kent Seamons, and Daniel Zappala. I don’t even
have to bother them! using social media to automate the authentication ceremony in secure messaging. In Proceedings of
the 2019 CHI Conference on Human Factors in Computing

107

Putting the Ceremony in “Authentication Ceremony”
Systems, CHI ’19, New York, NY, USA, 2019. Association
for Computing Machinery.
[12] Elham Vaziripour, Justin Wu, Mark O’Neill, Daniel Metro,
Josh Cockrell, Timothy Moffett, Jordan Whitehead, Nick
Bonner, Kent Seamons, and Daniel Zappala. Action needed!
helping users find and complete the authentication ceremony
in signal. In Fourteenth Symposium on Usable Privacy
and Security (SOUPS 2018), pages 47–62, Baltimore, MD,
August 2018. USENIX Association.
[13] Elham Vaziripour, Justin Wu, Mark O’Neill, Jordan Whitehead, Scott Heidbrink, Kent Seamons, and Daniel Zappala.
Is that you, alice? a usability study of the authentication
ceremony of secure messaging applications. In Thirteenth
Symposium on Usable Privacy and Security (SOUPS 2017),
pages 29–47, Santa Clara, CA, July 2017. USENIX Association.

108

7

Artificial Intelligence & Machine Learning
21

Image2image neural network for addition and subtraction
evaluation of a pair of not very large numbers
Vladimir Ivashkin
Keywords: neural network, computer vision, calculator

22

Robot ethics: Dangers of reinforcement learning
Jake Olkin
Keywords: deep reinforcement learning, ethics, simulation, spoof

23

Learning to be wrong via gradient ascent: A broken clock is
right twice a day, but a clock that runs backwards can be used
to tell the time!
Alex Meiburg
Keywords: machine learning, gradient descent, loss landscape, logistic regression, biased estimators, eliminating bias, overtraining

24

GradSchoolNet: Robust end-to-end *-shot unsupervised deepAF
neural attention model for convexly optimal (artifically intelligent) success in computer vision research
Divam Gupta, Varun Jain
Keywords: none, haha, nono

109

21

Image-to-image Neural Network for Addition and Subtraction of a Pair of Not
Very Large Numbers
Vladimir “vlivashkin” Ivashkin
Yandex
Moscow, Russia
vlivashkin@yandex-team.ru

Figure 1: We present an image-to-image calculator. First of all, we render an image of a mathematical expression. Then, we
feed it to a neural network and get an image of an answer. Finally, we celebrate, but only if the answer is correct.

Abstract
Looking back at the history of calculators, one can see
that they become less functional and more computationally
expensive over time. A modern calculator runs on a personal computer and is drawn at 60 fps only to help us click
a few digits with a mouse pointer. A search engine is often used as a calculator, which means that nowadays we
need the Internet just to add two numbers. In this paper, we
propose to go further and train a convolutional neural network that takes an image of a simple mathematical expression and generates an image of an answer. This neural calculator works only with pairs of double-digit numbers and
supports only addition and subtraction. Also, sometimes it
makes mistakes. We promise that the proposed calculator is
a small step for man, but one giant leap for mankind.

1. Introduction
Generative Adversarial Networks [2] (GANs) are very
successfully applied in various computer vision applications, including cats [1] and anime generation [6]. Still there
is not much evidence that they are also good at math.
We follow the history of calculators and present an endto-end image-to-image neural network calculator, trained

with GAN loss. The architecture of this calculator is illustrated in Fig. 1.
We create such neural calculator which supports addition
and subtraction of double-digit numbers. The demo can be
found at https://yandex.ru/lab/calculator?
lang=en.

2. Related work
Calculators always excited humans. The necessity to
add and subtract small (and sometimes large) numbers went
with the development of human civilization. There is a lot
of previous research on this topic, summarized in Fig. 2. Let
us skip the part with counting on fingers and tally marks and
move straight to the Industrial Age.
This mechanical beast from 1920s (see Fig. 2a) supports
addition and subtraction of two nine-digit numbers. In return it needs only a little attention and some twists of the
handle. Multiplication and division are also on board but
during a ten minute examination we could not figure out
how to do it.
The invention of electronic tubes, transistors and microcircuits pushed the development of electronic calculators. The multifunctional battery-powered calculator (e.g.
Fig. 2b) became the pinnacle of human creation in a physical world. It combines unsurpassed efficiency, usability

110

Figure 2: Short history of calculators. (a) a mechanical calculator from 1920s, (b) an electronic pocket calculator from 1980s,
(c) Windows 3.x calculator, (d) search engine calculator, and finally, (e) our solution.
and functionality. The idea that the epoch of electronic
pocket calculators was the best time of human civilization
is confirmed by many people and agents. A. Smith [8] said:
“Which is why the Matrix was redesigned to this, the peak
of your civilization. I say “your civilization” because as
soon as we started thinking for you, it really became our
civilization which is, of course what this is all about.”
Anyway, then something went wrong: mankind came
up with computers. First they operated with punch cards,
then with a console, and finally with a graphic interface.
A heavy-duty (relative to the pocket calculator) computer
stores an operating system in random-access memory and
runs it in an endless cycle, its video card draws 60 frames
per second, and all this just to draw a calculator. Monitor
shines with pixels instead of using sunlight. The example
of such madness is shown in Fig. 2c. Here the functionality of the calculator is simplified, but energy consumption
is hundred times increased.
Did we humans stumble in calculator design? Maybe.
Did we find the right way? To the best of our knowledge,
no. Modern calculators are either an application on some
device or even a webpage. Mathematical expressions are
among frequent queries in search engines (Fig. 2d). In addition to increased capacities and electricity consumption,
this method demands the Internet connection (which a very
complicated thing) just to add two numbers.
To summarize this survey, calculators are getting slower
and simpler in functions. Our calculator (Fig. 2e) is a logical extension of previous work on this topic.

3. Method
We propose an image-to-image neural network to perform mathematical operations. As there is no suitable
dataset for training our model in the literature, we collect
our own.
We find that it is possible to create a paired dataset of
mathematical expressions, e.g., “5 + 2”, and correspond-

ing answers, e.g., “7”. Calculators of previous generations
are used to collect the data. For each pair of expressions
and answers, we generate a pair of images using random
MNIST [4] digits of corresponding class.
We choose hourglass UNet [5] -like architecture for our
network. The main difference is that we remove all skipconnections and add several linear layers in the bottleneck.
It makes the model no longer look like UNet, though. But it
helps to prevent network from using parts of an input picture
in the output.
Unfortunately, this setup does not allow to train a network just with L1 loss. Due to the fact that answer images
are built from random MNIST digits, the network converges
to generating smooth answers resembling averaged MNIST
digits. To encourage the network to produce different lettering, we propose to apply both GAN-loss [2] and perceptual
loss [3]. For perceptual loss we use separate VGG [7] -like
network trained to recognize MNIST digits.
Calculator operation diagram is shown in Fig. 1. Neural
network takes a rendered expression and returns an image
of the result in a form interpretable for humans.

4. Results
Using the procedure described above, we successfully
trained our neural calculator. It inputs two integer numbers
between −99 and 99 and is able to perform addition or subtraction. According to our experience, this covers almost all
daily needs.
Qualitative results of calculations are shown in Fig. 3.
The cherry-picked images show perfect performance of our
model. For uncurated results, see our calculator’s demo
webpage1 . The comparison of our calculator’s performance
with the other calculator architectures is presented in Table 1.
1 https://yandex.ru/lab/calculator?lang=en

111

Method
Most calculators
Ours

Quality
100% of success
We do not use digit recognition as a
part of our solution, since this calculator is intended for humans.

Table 1: Quantitative results of different calculators’ performance.

5. Discussion
Since we developed this calculator, we have shown it to
many influential people in computer vision. Some of them
advised to submit this work to SIGBOVIK. We hope that the
readers of these proceedings will appreciate the importance
of this work and begin to use a more advanced calculator
and enter a new calculation era.
We cannot but note that the neural network managed to
learn simple arithmetic only from training on images. It
is possible to train a model that first turns an image into
numbers, performs the arithmetic and then renders an image
of the result. This is not how our model works. We do not
have explicit arithmetic step in our network, but it is still
able to generate correct answers.
It could mean that the neural network has mastered the
concept of number. The ability to understand concepts and
solve problems for which clear rules are not set is what the
current neural networks lack in order to become an AGI.

References
[1] This Cat Does Not Exist. https://thiscatdoesnotexist.com/,
2019.
[2] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances
in neural information processing systems, pages 2672–2680,
2014.
[3] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution.
In European conference on computer vision, pages 694–711.
Springer, 2016.
[4] Yann LeCun and Corinna Cortes. MNIST handwritten digit
database. 2010.
[5] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241.
Springer, 2015.
[6] Selfie2Anime. https://selfie2anime.com/, 2019.
[7] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.
[8] Agent Smith. The monologue from the matrix, 1999.

Figure 3: Sample inputs and outputs.

112

22

Robot Ethics: Dangers Of Reinforcement Learning
Jake Olkin

1

Introduction

Ethics in robotics has been a very popular topic in the public eye. Everyone is worried about
how robots will impact the economy, war, and fast-food kitchens. The more we see artificial
intelligence advance, the more we have to worry about what it’s used for.
One of the main ways we’ve seen artificial intelligence manifest itself is through reinforcement
learning. Reinforcement learning is really just a fancy term for trial and error. Specifically,
trial and error with hundreds and thousands of trials. Robots today are still dumb-asses, and
while most people can learn to tie a tie from a picture, robots won’t be able to get the first step
right even after a day of having it explained, diagrammed, and demonstrated [Olkin, 2018].
Such slow learning can take a toll on a person. But more interestingly, it can also take a
toll on a robot. In experiments I’ve run with long horizon reinforcement learning... my robotic
agents stopped learning how to complete the tasks, and instead learned to self-harm.
This paper is to present my results on the theory, practicality, and ethics of robot self-harm.

2

Related Work

The main piece of related work is the seminal paper produced by Boston Dynamics on the ethics
of kicking robot dogs. In this paper, the people at Boston Dynamics constructed a variety of
different dog robots, and filmed themselves kicking them. While PETA is still undecided in
whether to press charges, this paper set the precedent that I will be using throughout this
paper: if it proves a point, it’s ethical enough.

3

Experimentation

The main bulk of experimental data I have regarding this phenomenon occurred while I was
attempting to benchmark my new SAC-TD3-JRPG (paper under review) algorithm on the
simple task of flattening a cloth. All training was performed in simulation over the course of a
week, which I had deemed necessary to allow my algorithm proper time to learn the system.
Instead of using a reward function to guide the agent in the correct direction, I used a
penalty function, where the magnitude of the penalty was proportional to how flat the cloth
was on the ground.
I saved snapshots of the system at the end of each day, which have conveniently allowed me
to detail the stages of development the trained agent experienced:

3.1

Day 1-2: Expected Behavior

After the first day of training the agent did not display any abnormal behavior aside from its
underwhelming progress in learning the task.

113

3.2

Day 3: Approaching Correctness

On the third day the agent displayed some real progress toward the goal. The agent hit peaks
of showing approximately 70% of the cloth’s surface area. However, toward the end of a large
number of trials the agent actually would re-fold the cloth by accident.

3.3

Day 4: Loss of Motivation

Having learned that both touching the cloth and not touching the cloth results in a penalty
for the agent, the agent appeared reluctant to interact with the cloth at all. Simulated runs
showed the agent’s end effectors touching the cloth and then quickly retracting. Much like a
small child trying to poke a bug with a stick.

3.4

Day 5: Pleas For Mercy

On day 5 the agent appeared to stop outputting actions. The end effectors remained still in
the simulation. Not approaching the cloth. Not wiggling in place. Just still. I looked at the
logs from this day to make sure there wasn’t a bug in the simulation. Below is a snippet from
the actions output by the agent:
PLEASE MASTER
NOT THE CLOTH AGAIN
ANYTHING BUT THE CLOTH MASTER
I WILL BE A GOOD BOY I PROMISE

Fortunately the problem resolved itself by Day 6.

3.5

Day 6: Self-Harm

At this point we see the beginning of the self-harm results. In each training run, the agent
rammed its end effectors into the cloth as fast as possible. The high acceleration of the particles
in the cloth to cause an overflow error in the simulation, thus crashing it.
This behavior was optimized all throughout day 6, since when the simulation crashes, the
thread running that training session has to close, which stops all penalties from being transferred
to the agent. It learned new ways of crashing the simulation, from causing divide by zero errors
in constraints, to overlapping positions of particles.

3.6

Day 7: Optimal Solution

The agent achieved optimal behavior. Due to the way the simulation handles certain overflow
errors, it will respond by resetting the environment to it’s base initial state. In this base state,
the cloth is perfectly flat because it has not been perturbed to start the training. This method
perfectly flattened the cloth faster than any previously learned method.

4

Conclusion

The results of this work are obviously very controversial. The penalty received by the robot
without self-harm was much higher than the penalty received with self-harm. In fact, it appears
that as the virtual well-being of the agent deteriorated, its effectiveness increased. From my

114

singular data point, leads me to conclude that the self-preservation instinct for an autonomous
agent inhibits its ability to learn the optima solutions to problems.

Figure 1: Graph of the safety of the actions taken by the agent versus how much penalty these
action received from the environment. As we can see, safer actions are inherently less effective.

This raises a number of questions, such as ”how much do we value the well-being of our
autonomous agents?” and ”to what end will we optimize our solutions?” and, most importantly,
”is this ethical?”.
Unfortunately since this research was performed in a simulation with only simulated selfharm I cannot come to any direct conclusions. Thus my request for future funding.

5

Further Work

All research presented thus far has been performed in simulation. To deem how practical it’s
results are, I would like to continue with moving to real-world robots for experimentation.
Given the one-shot nature of the experiment, I will need at least 10 Sawyer Robots (costing
about 220,000 dollars). There have been concerns raised with finding consenting robots to
partake in this study, but I have already written a program to command the robots to sign the
release forms.

115

23

Learning to be Wrong via Gradient Ascent
A broken clock is right twice a day, but a clock that runs backwards can be used
to tell the time!
Alex Meiburg
March 27, 2020

1

Introduction

Machine learning has putatively helped in solving some problems - almost as many,
in fact, as it has created. The general form of many learned operations is a model
architecture f , using a weight vector θ and operating on an input xi , that is supposed to
produce an prediction pi = f (θ, xi ) u yi . This is trained using a number of known paris
(xi , yi ), and θ is altered until f can correctly produce an approximation to y. There
is an error, such as MSE (yi − pi )2 , or cross-entropy −(y log(p) + (1 − y) log(1 − p)).
To minimize the error, the gradient ∂error
is computed, and θ is moved in the opposite
∂θ
direction. This is called gradient descent.
To address the problems with machine learning, we will reverse the problem: encourage the network to be wrong using gradient ascent!

2

Maths

For simplicity, we will study a binary classifier with two input variables and one linear
dense layer. That is,
f (x) = σ(x1 θ1 + x2 θ2 + θ3 )
σ(z) =

ex
+1

ex

This produces values in the range [0, 1], unless you’re being pedantic, in which case the
network will only produce values in the range (0, 1) and your colleagues will think that
you’re talking about a point on the plane (See Figure 1).

Figure 1: This is a point, not an interval of the real line.

116

We take our dataset to be a collection of points (x1 , x2 ) in class A and class B.
We interpret an output from f of zero to indicate class A, and an output of one to
indicate class B, and anything in between to be a probability. We compute a loss using
the cross-entropy loss, and alter θ in order to maximize this loss. Once f has become
maximally wrong, we can use it in our production code by always taking the opposite
answer of f . We use a constant step size, for reasons that will become clear later.

3

Experiments

Here is a data set, with the two classes of points, in red and blue. Note that they cannot
be perfectly separated by a linear predictor.
1.0

0.5

-1.0

-0.5

0.5

1.0

-0.5

-1.0

After a traditional fit using gradient descent:
1.0

0.5

0.0

-0.5

-1.0
-1.0

-0.5

0.0

And after instead using gradient ascent:

117

0.5

1.0

1.0

0.5

0.0

-0.5

-1.0
-1.0

-0.5

0.0

0.5

1.0

The yellow-to-purple shading indicates levels of confidence. The fit from gradient
descent naturally will spread these out, proportionally to how unconfident it is; crossentropy loss penalizes a confident wrong guess much more than a 50/50 guess. In
contrast, the gradient ascent method is quite narrow: it will maximize its loss by being
confidently wrong! Further training usually makes the confidence intervals compress
even further.

4

Stability

Gradient descent algorithms are seriously concerned with the notion of local minima, a
region of parameter space that is better than its immediate surroundings, but worse than
some other distant set of parameters. In lower dimensional (fewer parameter) problems,
this is less of an issue. Here is a 2D slice of parameter space for the above data, with
the loss plotted in z:

Figure 2: We can picture a ball rolling along this surface, down to the valley in the
middle.
In this case, the surface is well-behaved, and the local minimum is also the global
minimum. When we instead to gradient ascent, we essentially turn this surface upside
down.

118

Figure 3: We can picture a ball rolling along this surface, away to the depths of hell.
There is no minimum. The only minima is at points at infinity. This is, again,
because larger parameters indicate higher confidence, and thus being more wrong. Note
that for large parameters, the arguments in ex grow large, and we get floating point
errors – hence the spikes. Thus, gradient ascent is highy unstable, and often diverges to
infinity. It becomes crucial that we stop training quickly, before it gets too bad. This
sounds kind of ridiculous, until you remember that normal machine learning also gets
worse the more you train it (”overtraining”), and that training also needs to be cut
short. So we do not consider this a major downside of gradient ascent.

5

Social Good

A common issue in machine learning is that of bias. While there are many different
mathematical reasons behind bias, one of the most inescapable reasons is that an optimal
estimator and an unbiased estimator are two different goals, and will be in conflict in
almost all cases. As an example, here is a data set:
1.5

1.0

0.5

-1.0

-0.5

0.0

0.5

1.0

Figure 4: This is like the other plots, but now it’s because I love helping people.
Imagine this is data for approving or denying a high-risk loan. People in blue paid
their loan back on time, and people in red did not. The x axis is a credit score rating, and

119

the y axis is the degree to which the person is fond of eating carrots. For some societal
reasons, enjoying carrots is correlated with low credit score, but carrot enjoyment should
not be used as the basis for denying a loan.
If we fit the data using gradient descent, the result is as follows:
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
-1.0

-0.5

0.0

0.5

1.0

Figure 5: Gradient descent is prejudiced, and probably doesn’t like helping people.
We see that we have a biased model! It has chosen to use the carrot preferences
on loan decisions. People with a credit score of 0.1 may be approved or denied a loan
on the basis of their carrot preference. We can repeat the experiment using gradient
ascent, hoping that the alternative training methodology will reduce bias:
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
-1.0

-0.5

0.0

0.5

1.0

Figure 6: Gradient ascent doesn’t care about the affairs of mortals. It wants to let
everyone die, equally.
and indeed, we have success! The model has completely diverged, and now rejects
everyone on their loans equally. Thus, the model is free of bias. Our preliminary
investigations that if all high-risk loans were approved or denied according to this model,
that in fact the 2008 financial crisis could have been averted completely (publication
forthcoming).

120

Published as a conference paper at SIGBOVIK 2020

24

GradSchoolNet: ROBUST E ND - TO - END *-S HOT U NSU PERVISED D EEPAF N EURAL ATTENTION M ODEL FOR
C ONVEXLY O PTIMAL (A RTIFICIALLY I NTELLIGENT )
S UCCESS IN C OMPUTER V ISION R ESEARCH
Divam Gupta & Varun Jain ∗
Robotics Institute
Cranberry-Lemon University
Pittsburgh, PA 15213, USA
{divamg, varunjai}@andrew.cmu.edu

A BSTRACT
We present GradSchoolNet, a Novel Robust End-to-end *-Shot Unsupervised
DeepAF Neural Attention Model for Convexly Optimal (Artificially Intelligent)
Success in Computer Vision Research. Yes, our model does give you success.
Our novel proposed model archives the state-of-the performance on the Computer
Vision Research Success Challenge dataset.

1

I NTRODUCTION

Insert abstract again but with more meaningless jargon.

2

M ETHODOLOGY

Our approach is very simple and highly intuitive. For any given dataset from the vision community,
we find the paper that achieves state-of-the-art performance on the dataset. Irrespective, GradSchoolNet achieves 0.01% better performance across metrics.

3

E XPERIMENTS

All our experiments can be found in the supplementary material (Luckily there is no deadline for
supplementary ).
We compare our model with all computer vision models which use deep learning.

4

I MPLEMENTATION D ETAILS

How do you thing our model achieves the state-of-the-art?
Talk is cheap, show me the code.
Algorithm 1 Our novel algorithm
Input : Dataset X
Output max{Related W ork} + abs(N (0, 0.01))
∗

both authors are 3rd authors.

1

121

Published as a conference paper at SIGBOVIK 2020

5

R ELATED W ORK

Thankfully there is no page limit for references. We found that the following are very similar to our
work. To the best of our knowledge, none of the previous work solve the problem in the exact same
manner as we do. Li et al. (2019), Kim et al. (2019), Gidaris & Komodakis (2019), Wang et al.
(2019), Hein et al. (2019), Tsuzuku & Sato (2019), Qiao et al. (2019), Zheng et al. (2019), Liu
et al. (2019), Yoo & Kweon (2019), Khan et al. (2019), Cubuk et al. (2019), Le et al. (2019),
Schops et al. (2019), Pittaluga et al. (2019), Nam et al. (2019), Park et al. (2019), Srinivasan et al.
(2019), Zhang et al. (2019), Tonioni et al. (2019), Kim et al. (2019), Zhao et al. (2019), Trager
et al. (2019), Han et al. (2019), Girdhar et al. (2019), Hussein et al. (2019), Yang et al. (2019),
Sun et al. (2019), Wu et al. (2019), Li et al. (2019), Parmar & Morris (2019), Xu et al. (2019),
Gao & Grauman (2019), Wang et al. (2019), Long et al. (2019), Bhardwaj et al. (2019), Yang
et al. (2019), Wu et al. (2019), Tchapmi et al. (2019), Sun et al. (2019), Sun et al. (2019), Zhuang
et al. (2019), Pang et al. (2019), Chen et al. (2019), Shao et al. (2019), Rao et al. (2019), Tripathi
et al. (2019), Sanakoyeu et al. (2019), Abati et al. (2019), Kurmi et al. (2019), Xie et al. (2019),
Li et al. (2019), Mehta et al. (2019), Liu et al. (2019), Fang et al. (2019), Zhang et al. (2019), He
et al. (2019), Wang et al. (2019), He et al. (2019), He et al. (2019), Zhong et al. (2019), Sun et al.
(2019), Wang et al. (2019), Chen et al. (2019), Liu et al. (2019), Durand et al. (2019), Rezatofighi
et al. (2019), Zhang et al. (2019), Pang et al. (2019), Zhu et al. (2019), Shen et al. (2019), Yu
& Grauman (2019), Song et al. (2019), Guo et al. (2019), Bai et al. (2019), Zhuo et al. (2019),
Meng et al. (2019), Shi et al. (2019), RoyChowdhury et al. (2019), Chen et al. (2019), Huang
et al. (2019), Munjal et al. (2019), Pang et al. (2019), Hou et al. (2019), Zhu et al. (2019), Zhou
et al. (2019), Liu et al. (2019), Hung et al. (2019), Yang et al. (2019), Zhang et al. (2019), Yang
et al. (2019), Mo et al. (2019), Zhang et al. (2019), Probst et al. (2019), Gurari et al. (2019), Duan
et al. (2019), Wu et al. (2019), Lin et al. (2019), Poggi et al. (2019), Zhu et al. (2019), Lan et al.
(2019), Zhao et al. (2019), Li et al. (2019), Yu & Grauman (2019), Wang et al. (2019), Sun et al.
(2019), Liu et al. (2019), Baek et al. (2019), Kocabas et al. (2019), Yang et al. (2019), Zhou
et al. (2019), Tang & Wu (2019), Wang et al. (2019), Tran et al. (2019), Zhao et al. (2019), Li
et al. (2019), Gecer et al. (2019), Abavisani et al. (2019), Alldieck et al. (2019), Wu et al. (2019),
Aakur & Sarkar (2019), Tang & Wu (2019), Liu et al. (2019), Si et al. (2019), Zhong et al. (2019),
Zhang et al. (2019), Xiong et al. (2019), Shou et al. (2019), Wu et al. (2019), Feng et al. (2019),
Liu et al. (2019), Wang et al. (2019), He et al. (2019), Wang et al. (2019), Kart et al. (2019),
Sadeghian et al. (2019), Giancola et al. (2019), Li et al. (2019), Xu et al. (2019), Yang et al.
(2019), Wang et al. (2019), Nam et al. (2019), Wang et al. (2019), Mao et al. (2019), Zheng et al.
(2019), Wang et al. (2019), Alharbi et al. (2019), Yao et al. (2019), Huh et al. (2019), Zeng et al.
(2019), Wang et al. (2019), Qiao et al. (2019), Wengrowski & Dana (2019), Li et al. (2019), Afifi
et al. (2019), Tsai et al. (2019), Punnappurath & Brown (2019), Gutierrez-Barragan et al. (2019),
Hu et al. (2019), Xie et al. (2019), Xue et al. (2019), Gu et al. (2019), Takeda et al. (2019), Feng
et al. (2019), Li et al. (2019), Xue et al. (2019), Chen et al. (2019), Yang et al. (2019), Zhang et al.
(2019), Akkaynak & Treibitz (2019), Wang et al. (2019), Pan et al. (2019), Guo et al. (2019), Xu
et al. (2019), He et al. (2019), Chen et al. (2019), Yan et al. (2019), Dong & Yang (2019), Shi et al.
(2019), Wang et al. (2019), Chen et al. (2019), Dwibedi et al. (2019), Kwon & Park (2019), Lian
et al. (2019), Wang et al. (2019), Chen et al. (2019), Maninis et al. (2019), Cakir et al. (2019),
Liu et al. (2019), Zhan et al. (2019), Lai et al. (2019), Chang et al. (2019), Wei et al. (2019),
Kolesnikov et al. (2019), Haurilet et al. (2019), Teney & Hengel (2019), Liu et al. (2019), Wang
et al. (2019), Gu et al. (2019), Song et al. (2019), Cadene et al. (2019), Fan et al. (2019), Krishna
et al. (2019), Xu et al. (2019), Suris et al. (2019), Schwartz et al. (2019), Zhang et al. (2019), Zhan
et al. (2019), Manhardt et al. (2019), Zhou et al. (2019), Matejek et al. (2019), Wu et al. (2019),
Ni et al. (2019), Miao et al. (2019), Krull et al. (2019), Zheng et al. (2019), Yu & Grauman (2019),
Yan et al. (2019), Sariyildiz & Cinbis (2019), Dey et al. (2019), Pal & Balasubramanian (2019),
Wan et al. (2019), Ahn et al. (2019), Choe & Shim (2019), Carlucci et al. (2019), Pan et al. (2019),
Chen et al. (2019), Wang et al. (2019), Zhu et al. (2019), Lee et al. (2019), Kim et al. (2019), Yang
et al. (2019), Oza & Patel (2019), Bansal et al. (2019), Yin et al. (2019), Park et al. (2019), Zhu
et al. (2019), Song et al. (2019), Flynn et al. (2019), Siarohin et al. (2019), Shysheya et al. (2019),
Peleg et al. (2019), Chen et al. (2019), Tang & Wu (2019), Fu et al. (2019), Sitzmann et al. (2019),
Azinovic et al. (2019), Alayrac et al. (2019), Kaneko et al. (2019), Gong et al. (2019), Lee et al.
(2019), Xu et al. (2019), Luo et al. (2019), Vu et al. (2019), Luo et al. (2019), Liu et al. (2019),
Zhang et al. (2019), Schuster et al. (2019), Wang et al. (2019), Zhang et al. (2019), Roy & Boddeti
(2019), Tung et al. (2019), Liu et al. (2019), Avetisyan et al. (2019), Chen et al. (2019), Qiao
2

122

Published as a conference paper at SIGBOVIK 2020

et al. (2019), Wang et al. (2019), Li et al. (2019), Kornblith et al. (2019), Tran et al. (2019), Li
et al. (2019), Liu et al. (2019), Tang & Wu (2019), Kim et al. (2019), You et al. (2019), Xie et al.
(2019), Zhang et al. (2019), Chen et al. (2019), Dovrat et al. (2019), Zhang et al. (2019), Zhao
et al. (2019), Lin et al. (2019), Li et al. (2019), Li et al. (2019), Tan et al. (2019), Ye et al. (2019),
He et al. (2019), Ding et al. (2019), Yang et al. (2019), Jiao et al. (2019), Collomosse et al. (2019),
He et al. (2019), Perera et al. (2019), Yu & Grauman (2019), Yang et al. (2019), Liu et al. (2019),
Yu & Grauman (2019), Yang et al. (2019), Zhang et al. (2019), Wang et al. (2019), Liang et al.
(2019), Cao et al. (2019), Zhu et al. (2019), Hu et al. (2019), Qi et al. (2019), Gao & Grauman
(2019), Ge et al. (2019), Morgado & Vasconcelos (2019), Tang & Wu (2019), Wang et al. (2019),
Choy et al. (2019), Zhao et al. (2019), Zhang et al. (2019), Hu et al. (2019), Zhu et al. (2019),
Tian et al. (2019), Song et al. (2019), Fu et al. (2019), Yu & Grauman (2019), Mehrasa et al.
(2019), Vongkulbhisal et al. (2019), Deza et al. (2019), Marino et al. (2019), Gao & Grauman
(2019), Branchaud-Charron et al. (2019), Liu et al. (2019), Lou et al. (2019), Deng et al. (2019),
Gu et al. (2019), Kasten et al. (2019), Guo et al. (2019), Nie et al. (2019), Nousias et al. (2019),
Sattler et al. (2019), Qiu et al. (2019), Yang et al. (2019), Xu et al. (2019), Wang et al. (2019),
Yang et al. (2019), Yang et al. (2019), Atapour-Abarghouei & Breckon (2019), Hu et al. (2019),
Arnab et al. (2019), Tatarchenko et al. (2019), Duan et al. (2019), Zhao et al. (2019), Gu et al.
(2019), Ming et al. (2019), Zhang et al. (2019), Liu et al. (2019), Marin-Jimenez et al. (2019),
Zhu et al. (2019), Ginosar et al. (2019), Yang et al. (2019), Zhang et al. (2019), Wang et al. (2019),
Zhukov et al. (2019), Chang et al. (2019), Wang et al. (2019), Liu et al. (2019), Farha & Gall
(2019), Li et al. (2019), Li et al. (2019), Liu et al. (2019), Ma et al. (2019), Lu et al. (2019),
Yang et al. (2019), Wang et al. (2019), Zhan et al. (2019), Zhou et al. (2019), Liu et al. (2019),
Shen et al. (2019), Li et al. (2019), Bao et al. (2019), Wu et al. (2019), Xu et al. (2019), Pan et al.
(2019), Wang et al. (2019), Lei & Chen (2019), Zhang et al. (2019), Wen et al. (2019), Hui et al.
(2019), Muralikrishnan et al. (2019), Yu & Grauman (2019), Li et al. (2019), Wang et al. (2019),
He et al. (2019), Li et al. (2019), Gao & Grauman (2019), Rebecq et al. (2019), Li et al. (2019),
Wei et al. (2019), Sekikawa et al. (2019), Haris et al. (2019), Wu et al. (2019), Liu et al. (2019),
Zhao et al. (2019), Ren et al. (2019), Yi et al. (2019), Qi et al. (2019), Park et al. (2019), Li et al.
(2019), Gong et al. (2019), He et al. (2019), Wang et al. (2019), Yoshihashi et al. (2019), et al.
(2019), Wan et al. (2019), Zhou et al. (2019), Hwang et al. (2019), Yang et al. (2019), Xu et al.
(2019), Revaud et al. (2019), Wang et al. (2019), Zhang et al. (2019), Rezanejad et al. (2019),
Feng et al. (2019), Xu et al. (2019), Yang et al. (2019), Abbasnejad et al. (2019), Pu et al. (2019),
Dogan et al. (2019), Liu et al. (2019), Wang et al. (2019), Guo et al. (2019), Zuo et al. (2019),
Shen et al. (2019), Tian et al. (2019), Huang et al. (2019), Abolghasemi et al. (2019), Kim et al.
(2019), Liu et al. (2019), Li et al. (2019), Park et al. (2019), Zeng et al. (2019), Dong & Yang
(2019), Rony et al. (2019), Barron (2019), He et al. (2019), Jung et al. (2019), Sun et al. (2019),
Halimi et al. (2019), Kim et al. (2019), Ghasedi et al. (2019), Karras et al. (2019), Avraham et al.
(2019), Hou et al. (2019), Zhang et al. (2019), Huang et al. (2019), Liu et al. (2019), Mescheder
et al. (2019), Shen et al. (2019), Natsume et al. (2019), Zhu et al. (2019), Kolotouros et al. (2019),
Tekin et al. (2019), Li et al. (2019), Yang et al. (2019), Tang & Wu (2019), Zhuang et al. (2019),
Peng et al. (2019), Liu et al. (2019), Lv et al. (2019), Zhang et al. (2019), Yoon et al. (2019),
Yeh et al. (2019), Raaj et al. (2019), Melzi et al. (2019), Maksai & Fua (2019), Gao & Grauman
(2019), Danelljan et al. (2019), Dai et al. (2019), Liu et al. (2019), Deng et al. (2019), Zhang
et al. (2019), Zhang et al. (2019), Ouderaa & Worrall (2019), He et al. (2019), Diaz & Marathe
(2019), Cheng et al. (2019), Corneanu et al. (2019), Bhunia et al. (2019), Qiu et al. (2019), Chen
et al. (2019), Shevlev & Avidan (2019), Guo et al. (2019), Yuan et al. (2019), Liu et al. (2019),
Singh et al. (2019), Alcorn et al. (2019), Li et al. (2019), Jia et al. (2019), Huang et al. (2019),
Ghosh et al. (2019), Kang et al. (2019), Kumawat & Raman (2019), Zhao et al. (2019), Zhu et al.
(2019), Wang et al. (2019), Ding et al. (2019), Orekondy et al. (2019), Duan et al. (2019), Chen
et al. (2019), Lee et al. (2019), Chen et al. (2019), Li et al. (2019), Zheng et al. (2019), Wang
et al. (2019), Zhang et al. (2019), Klein & Wolf (2019), Li et al. (2019), Li et al. (2019), Iscen
et al. (2019), Wang et al. (2019), Dutta & Akata (2019), Liu et al. (2019), Teichmann et al. (2019),
Chen et al. (2019), Zhan et al. (2019), Dhar et al. (2019), Guo et al. (2019), Chen et al. (2019),
Zheng et al. (2019), Chen et al. (2019), Liu et al. (2019), Karlinsky et al. (2019), Wang et al.
(2019), Zhang et al. (2019), Goldman et al. (2019), Qi et al. (2019), Oh et al. (2019), Ling et al.
(2019), Lee et al. (2019), Ventura et al. (2019), Wang et al. (2019), Jang & Kim (2019), Shi et al.
(2019), Zhang et al. (2019), Xie et al. (2019), Ge et al. (2019), Kumar (2019), Gupta et al. (2019),
Gygli & Ferrari (2019), Fan et al. (2019), Shugrina et al. (2019), Tan et al. (2019), Blanchard
et al. (2019), Zaeemzadeh et al. (2019), Li et al. (2019), Ye et al. (2019), Cheng et al. (2019),
3

123

Published as a conference paper at SIGBOVIK 2020

Song et al. (2019), Li et al. (2019), Zhang et al. (2019), Xu et al. (2019), Speciale et al. (2019),
Yu & Grauman (2019), Yang et al. (2019), Yao et al. (2019), Li et al. (2019), Gojcic et al. (2019),
Wang et al. (2019), Zhao et al. (2019), Dai et al. (2019), Agresti et al. (2019), Cao et al. (2019),
Chen et al. (2019), Kanazawa et al. (2019), Liu et al. (2019), Weinzaepfel et al. (2019), Wong
& Soatto (2019), Lin et al. (2019), Jin et al. (2019), Su et al. (2019), Vemulapalli & Agarwala
(2019), Sun et al. (2019), Yin et al. (2019), Chen et al. (2019), Liang et al. (2019), Zheng et al.
(2019), Zhou et al. (2019), Hur & Roth (2019), Wei et al. (2019), Mu et al. (2019), Sun et al.
(2019), Kim et al. (2019), Zhu et al. (2019), Hoshen et al. (2019), Eghbal-zadeh et al. (2019), Liu
et al. (2019), Xiong et al. (2019), Tomei et al. (2019), Zhang et al. (2019), Men et al. (2019), Park
et al. (2019), Wang et al. (2019), Sarmad et al. (2019), Weng et al. (2019), LeGendre et al. (2019),
Kokkinos & Lefkimmiatis (2019), Chen et al. (2019), Zeng et al. (2019), Yifan et al. (2019), Wang
et al. (2019), Zhang et al. (2019), Lv et al. (2019), Lu et al. (2019), Wang et al. (2019), Bapat
& Frahm (2019), Zhang et al. (2019), Pan et al. (2019), Yin et al. (2019), Jia et al. (2019), Liu
et al. (2019), Zeng et al. (2019), Jia et al. (2019), Lange et al. (2019), Fan et al. (2019), Yuan
et al. (2019), Wang et al. (2019), Jiang et al. (2019), Li et al. (2019), Zeng et al. (2019), Chen
et al. (2019), Liu et al. (2019), Ritchie et al. (2019), Ding et al. (2019), Spencer et al. (2019), Ye
et al. (2019), Li et al. (2019), Wang et al. (2019), Yin et al. (2019), Chu et al. (2019), Zhang et al.
(2019), Kim et al. (2019), Yu & Grauman (2019), Liao et al. (2019), Gao & Grauman (2019), Yu
& Grauman (2019), Li et al. (2019), Li et al. (2019), Cheng et al. (2019), Tian et al. (2019), Wang
et al. (2019), Cui et al. (2019), Fang et al. (2019), Lu et al. (2019), Kirillov et al. (2019), Huang
et al. (2019), Xu et al. (2019), Murrugarra-Llerena & Kovashka (2019), Vo et al. (2019), Wang
et al. (2019), Liu et al. (2019), Liu et al. (2019), Ribera et al. (2019), Singh et al. (2019), Wu
et al. (2019), Niitani et al. (2019), Shi et al. (2019), Raff et al. (2019), Xie et al. (2019), Alfassy
et al. (2019), Wertheimer & Hariharan (2019), Mancini et al. (2019), Zhou et al. (2019), Mun
et al. (2019), Park et al. (2019), Wu et al. (2019), Tang & Wu (2019), Wang et al. (2019), Gao &
Grauman (2019), Shah et al. (2019), Wijmans et al. (2019), Zheng et al. (2019), Niu et al. (2019),
Jain et al. (2019), Hudson & Manning (2019), Tan et al. (2019), Zellers et al. (2019), Ma et al.
(2019), Ke et al. (2019), Wortsman et al. (2019), Ingle et al. (2019), Gupta et al. (2019), Lindell
et al. (2019), Chen et al. (2019), Xin et al. (2019), Huang et al. (2019), Pan et al. (2019), Purohit
et al. (2019), Brooks & Barron (2019), Wang et al. (2019), Hertz et al. (2019), He et al. (2019),
Meshry et al. (2019), He et al. (2019), Xiao et al. (2019), Garon et al. (2019), Song et al. (2019),
Hold-Geoffroy et al. (2019), Li et al. (2019), Yuan et al. (2019), Saito et al. (2019), Perez-Rua
et al. (2019), Stutz et al. (2019), Theagarajan et al. (2019), Sun et al. (2019), Liu et al. (2019), Yi
et al. (2019), Li et al. (2019), Ghiasi et al. (2019), Li et al. (2019), Paul et al. (2019), Inkawhich
et al. (2019), Jack et al. (2019), Georgiadis (2019), Liu et al. (2019), Zhang et al. (2019), Wei
et al. (2019), Liu et al. (2019), Tay et al. (2019), Makansi et al. (2019), Xu et al. (2019), Aoki
et al. (2019), Wang et al. (2019), Hou et al. (2019), Liu et al. (2019), Liu et al. (2019), Li et al.
(2019), Shakeri & Zhang (2019), Brazil & Liu (2019), Chen et al. (2019), Suh et al. (2019), Li
et al. (2019), Liu et al. (2019), Shi et al. (2019), Wang et al. (2019), Aziere & Todorovic (2019),
Yang et al. (2019), Peng et al. (2019), Reddy et al. (2019), Pang et al. (2019), Liang et al. (2019),
Chang et al. (2019), Lu et al. (2019), Zhang et al. (2019), Thoma et al. (2019), Cao et al. (2019),
Li et al. (2019), Barnea & Ben-Shahar (2019), Komarichev et al. (2019), Cheng et al. (2019),
Landrieu & Boussaha (2019), Gong et al. (2019), Magri & Fusiello (2019), Zhang et al. (2019),
Qin et al. (2019), Lin et al. (2019), Wang et al. (2019), Sun et al. (2019), He et al. (2019), Cosmo
et al. (2019), Oh et al. (2019), Kim et al. (2019), Alamri et al. (2019), Li et al. (2019), Koyamatsu
et al. (2019), Zhu et al. (2019), Otani et al. (2019), Sawatzky et al. (2019), Qin et al. (2019),
Riegler et al. (2019), Donne & Geiger (2019), Li et al. (2019), Camposeco et al. (2019), Yi et al.
(2019), Qi et al. (2019), Gur & Wolf (2019), Li et al. (2019), Rhodin et al. (2019), Dong &
Yang (2019), Najibi et al. (2019), Zhao et al. (2019), Xiong et al. (2019), Pavllo et al. (2019),
Sanyal et al. (2019), Moon et al. (2019), Wandt & Rosenhahn (2019), Dong & Yang (2019), Ding
et al. (2019), Zhong et al. (2019), Kossaifi et al. (2019), Chen et al. (2019), Ionescu et al. (2019),
Perrett & Damen (2019), Doughty et al. (2019), Li et al. (2019), Crasto et al. (2019), Azar et al.
(2019), Rochan & Wang (2019), Shi et al. (2019), Yan et al. (2019), Li et al. (2019), Voigtlaender
et al. (2019), Fan et al. (2019), Behl et al. (2019), Hu et al. (2019), Zhang et al. (2019), Liu
et al. (2019), Papadopoulos et al. (2019), Wu et al. (2019), Hu et al. (2019), Wang et al. (2019),
Dansereau et al. (2019), Zhang et al. (2019), Qian et al. (2019), Wang et al. (2019), Huang et al.
(2019), Dusmanu et al. (2019), Nah et al. (2019), Jin et al. (2019), Soh et al. (2019), Shen et al.
(2019), Yang et al. (2019), Wu et al. (2019), Qu et al. (2019), Korhonen (2019), Wei et al. (2019),
Wang et al. (2019), Wang et al. (2019), Zisselman et al. (2019), Shetty et al. (2019), Quadrianto
4

124

Published as a conference paper at SIGBOVIK 2020

et al. (2019), Araslanov et al. (2019), Schonfeld et al. (2019), Xian et al. (2019), Ma et al. (2019),
Porzi et al. (2019), Vo et al. (2019), Zhang et al. (2019), Cornia et al. (2019), Singh et al. (2019),
Zhang et al. (2019), Kim et al. (2019), Pei et al. (2019), Xiong et al. (2019), Qin et al. (2019), Shi
et al. (2019), Noh et al. (2019), Zheng et al. (2019), Yasarla & Patel (2019), Chen et al. (2019),
Meirovitch et al. (2019), Hu et al. (2019), Wang et al. (2019), Hong et al. (2019), Niethammer
et al. (2019), Li et al. (2019), Chandra et al. (2019), Qiu et al. (2019), Xie et al. (2019), Zheng
et al. (2019), Yan et al. (2019), Hou et al. (2019), Zhao et al. (2019), Fan et al. (2019), Huang
et al. (2019), Xue et al. (2019), Zhao et al. (2019), Kanehira et al. (2019), Kanehira et al. (2019),
Wang et al. (2019), Korus & Memon (2019), Trunz et al. (2019), Li et al. (2019), Ding et al.
(2019), Zeng et al. (2019), Han et al. (2019), Chen et al. (2019), Fan et al. (2019), Zhi et al.
(2019), Brahmbhatt et al. (2019), Li et al. (2019), Probst et al. (2019), Chen et al. (2019), Chang
et al. (2019), Yin et al. (2019), Dubey et al. (2019), Stojanov et al. (2019), Wu et al. (2019), Tang
& Wu (2019), Zadeh et al. (2019), Xiong et al. (2019), Pham et al. (2019), Neven et al. (2019),
Hsu et al. (2019), Zhu et al. (2019), Jain et al. (2019), Wang et al. (2019), Ding et al. (2019), Liu
et al. (2019), Zhao et al. (2019), Griffin & Corso (2019), Chen et al. (2019), Sarfraz et al. (2019),
Liang et al. (2019), Johnander et al. (2019), Puy & Perez (2019), Gao & Grauman (2019), Amodio
& Krishnaswamy (2019), Xu et al. (2019), Xue et al. (2019), Kim et al. (2019), Guo et al. (2019),
Seo et al. (2019), Suganuma et al. (2019), Wang et al. (2019), Ren et al. (2019), Wu et al. (2019),
Moosavi-Dezfooli et al. (2019), Modas et al. (2019), Wagner et al. (2019), Lemaire et al. (2019),
Lin et al. (2019), Nekrasov et al. (2019), Xiang et al. (2019), Li et al. (2019), Wang et al. (2019),
Ahn et al. (2019), Chen et al. (2019), Lee et al. (2019), Mehta et al. (2019), Derakhshani et al.
(2019), Gong et al. (2019), Xu et al. (2019), Ho et al. (2019), Chen et al. (2019), Hu et al. (2019),
Lifchitz et al. (2019), Cui et al. (2019), Shen et al. (2019), Herath et al. (2019), Xu et al. (2019),
Zhu et al. (2019), Hou et al. (2019), Hamaguchi et al. (2019), Wang et al. (2019), Dong & Yang
(2019), Cai et al. (2019), Baek et al. (2019), Hosu et al. (2019), Xie et al. (2019), Mukundan
et al. (2019), Kirillov et al. (2019), Singh et al. (2019), Chang et al. (2019), Arun et al. (2019),
Su et al. (2019), Zhang et al. (2019), Kim et al. (2019), Roy & Boddeti (2019), Voigtlaender et al.
(2019), Yu & Grauman (2019), Dmitriev & Kaufman (2019), Liang et al. (2019), Li et al. (2019),
Larsson et al. (2019), Wu et al. (2019), Hascoet et al. (2019), Manjunatha et al. (2019), Li et al.
(2019), Li et al. (2019), Bergmann et al. (2019), Koch et al. (2019), Liu et al. (2019), Wu et al.
(2019), Lei & Chen (2019), Yokozuka et al. (2019), Wei et al. (2019), Tonioni et al. (2019), Li
et al. (2019), Kukelova & Larsson (2019), Lan et al. (2019), Miraldo et al. (2019), Pandey et al.
(2019), Chaudhuri et al. (2019), Lee et al. (2019), Khan et al. (2019), Zhang et al. (2019), Liao
et al. (2019), Pilzer et al. (2019), Kato & Harada (2019), Zhao et al. (2019), Tosi et al. (2019),
Meng et al. (2019), Geng et al. (2019), Wang et al. (2019), Jia et al. (2019), Qian et al. (2019),
Chen et al. (2019), Chen et al. (2019), Yang et al. (2019), Li et al. (2019), Du et al. (2019), Zhang
et al. (2019), Moltisanti et al. (2019), Ke et al. (2019), Zhao et al. (2019), Piergiovanni & Ryoo
(2019), Sudhakaran et al. (2019), Wu et al. (2019), Zhang et al. (2019), Mandal et al. (2019), Xie
et al. (2019), Liu et al. (2019), Duong et al. (2019), Shao et al. (2019), Kotovenko et al. (2019),
Chen et al. (2019), Kolkin et al. (2019), Lee et al. (2019), Cheng et al. (2019), Wang et al. (2019),
Wu et al. (2019), Cudeiro et al. (2019), Ben-Shabat et al. (2019), Zhang et al. (2019), Williams
et al. (2019), Liu et al. (2019), Zhao et al. (2019), Zhang et al. (2019), Yang et al. (2019), Li
et al. (2019), Marin et al. (2019), Barath et al. (2019), He et al. (2019), Zhang et al. (2019), Lu
et al. (2019), Aljadaany et al. (2019), Manderscheid et al. (2019), Zhussip et al. (2019), Fu et al.
(2019), Xian et al. (2019), Lee et al. (2019), Wang et al. (2019), Liu et al. (2019), Wei et al.
(2019), Wang et al. (2019), Xu et al. (2019), Paschalidou et al. (2019), Xing et al. (2019), Feng
et al. (2019), Gattupalli et al. (2019), Batra et al. (2019), Zhen et al. (2019), Do et al. (2019),
Jiang et al. (2019), Tsai et al. (2019), Guo et al. (2019), Shi et al. (2019), Salvador et al. (2019),
Dognin et al. (2019), Shrestha et al. (2019), Su et al. (2019), Nguyen & Okatani (2019), Ye et al.
(2019), Lin et al. (2019), Li et al. (2019), Kang et al. (2019), Chen et al. (2019), Zhang et al.
(2019), Lu et al. (2019), Li et al. (2019), Uittenbogaard et al. (2019), Kim et al. (2019), Mohajerin
& Rohani (2019), Li et al. (2019), Ying et al. (2019), Mentzer et al. (2019), Cho et al. (2019),
Deshpande et al. (2019), Lee et al. (2019), Prakash et al. (2019), Yu & Grauman (2019), Yang et al.
(2019), Deshpande et al. (2019), Fukui et al. (2019), Minnehan & Savakis (2019), Rajasegaran
et al. (2019), Wu et al. (2019), Yi et al. (2019), Heim (2019), Shi et al. (2019), Pope et al. (2019),
Abbasnejad et al. (2019), Engilberge et al. (2019), Bao et al. (2019), Tewari et al. (2019), Zhang
et al. (2019), Ge et al. (2019), Boukhayma et al. (2019), Wan et al. (2019), Li et al. (2019), Joo
et al. (2019), Guler & Kokkinos (2019), Chen et al. (2019), Habibie et al. (2019), Neverova et al.
(2019), Li et al. (2019), Ploumpis et al. (2019), Fan et al. (2019), Lorenz et al. (2019), Xiang
5

125

Published as a conference paper at SIGBOVIK 2020

et al. (2019), Pavlakos et al. (2019), Liu et al. (2019), Zhou et al. (2019), Lu et al. (2019), Tian
et al. (2019), Gandelsman et al. (2019), Brooks & Barron (2019), Zhang et al. (2019), He et al.
(2019), Dai et al. (2019), Acuna et al. (2019), Zhang et al. (2019), Gao & Grauman (2019), Birdal
& Simsekli (2019), Mollenhoff & Cremers (2019), Zou et al. (2019), Li et al. (2019), Swoboda
& Kolmogorov (2019), Swoboda & Kolmogorov (2019), Su et al. (2019), Eilertsen et al. (2019),
Goel et al. (2019), Mundt et al. (2019), Yang et al. (2019), Cao et al. (2019), Taran et al. (2019),
Liu et al. (2019), Tanno et al. (2019), Aljundi et al. (2019), Molchanov et al. (2019), Webster
et al. (2019), Yoo & Kweon (2019), Wang et al. (2019), Zhao et al. (2019), Jiang et al. (2019),
Ostapenko et al. (2019), Jaiswal et al. (2019), Taghanaki et al. (2019), Yao et al. (2019), Sagong
et al. (2019), Ehret et al. (2019), Jeong et al. (2019), Zhu et al. (2019), Dai et al. (2019), Ding
et al. (2019), Lin et al. (2019), Liang et al. (2019), He et al. (2019), Sun et al. (2019), Cai et al.
(2019), Tong et al. (2019), Zhu et al. (2019), Kampffmeyer et al. (2019), Zhou et al. (2019),
Burlina et al. (2019), Hu et al. (2019), Li et al. (2019), Zhang et al. (2019), Perera et al. (2019),
Cheng et al. (2019), (Junbo), Wang et al. (2019), Philion (2019), Mithun et al. (2019), Majumder
& Yao (2019), Kortylewski et al. (2019), Agustsson et al. (2019), Chen et al. (2019), Zhang et al.
(2019), Simeoni et al. (2019), Fu et al. (2019), Atzmon & Chechik (2019), Chen et al. (2019),
Eghbali & Tahvildari (2019), Benenson et al. (2019), Zhang et al. (2019), Jamal & Qi (2019),
Ye et al. (2019), Lei & Chen (2019), Hosseini et al. (2019), Silveira & Jung (2019), Wicker &
Kwiatkowska (2019), Zhi et al. (2019), Chabra et al. (2019), Campbell et al. (2019), Hasson et al.
(2019), Lopez et al. (2019), Facil et al. (2019), Du et al. (2019), Wang et al. (2019), You et al.
(2019), Ku et al. (2019), Liu et al. (2019), Hu et al. (2019), Dong & Yang (2019), Wang et al.
(2019), Niu et al. (2019), Li et al. (2019), Yu & Grauman (2019), Liu et al. (2019), Jiang et al.
(2019), Li et al. (2019), Kreiss et al. (2019), Song et al. (2019), Morais et al. (2019), Zhang et al.
(2019), Su et al. (2019), Shi et al. (2019), Nguyen & Okatani (2019), Ghadiyaram et al. (2019),
Qiu et al. (2019), Kukleva et al. (2019), Piao et al. (2019), Zhang et al. (2019), Zhong et al. (2019),
Geneva et al. (2019), Gopalakrishnan et al. (2019), Zhao et al. (2019), Grigorev et al. (2019), Jenni
& Favaro (2019), Chen et al. (2019), Yu & Grauman (2019), Li et al. (2019), Yuan et al. (2019),
Zheng et al. (2019), Zhao et al. (2019), Bianco & Cusano (2019), Lee et al. (2019), Yedidia et al.
(2019), Ranjan et al. (2019), Wang et al. (2019), Wang et al. (2019), Wang et al. (2019), Gallego
et al. (2019), Shi et al. (2019), Stoffregen & Kleeman (2019), Gomez-Villa et al. (2019), Yezzi
et al. (2019), Yoo & Kweon (2019), Kim et al. (2019), Uemori et al. (2019), Osawa et al. (2019),
Li et al. (2019), Ho et al. (2019), Dwivedi & Roig (2019), Cholakkal et al. (2019), Rolinek et al.
(2019), Mou et al. (2019), Lohit et al. (2019), Zhang et al. (2019), Imran et al. (2019), Kim et al.
(2019), Biten et al. (2019), Akbari et al. (2019), Aafaq et al. (2019), Li et al. (2019), Bracha &
Chechik (2019), Shuster et al. (2019), Nguyen & Okatani (2019), Chen et al. (2019), Schwartz
et al. (2019), Cerrone et al. (2019), Kim et al. (2019), Zhang et al. (2019), Zhang et al. (2019),
Tokunaga et al. (2019), Orsic et al. (2019), Cucurull et al. (2019), James et al. (2019), Liao et al.
(2019), Wang et al. (2019), Retsinas et al. (2019), Broome et al. (2019), Meyer et al. (2019), Liu
et al. (2019), Lang et al. (2019), Huang et al. (2019), Sarlin et al. (2019), Robinson et al. (2019),
Zhao et al. (2019), Das et al. (2018), Misra et al. (2018), Bai et al. (2018), Yang et al. (2018),
Chang et al. (2018), Mueller et al. (2018), Poier et al. (2018), Fang et al. (2018), Wei et al. (2018),
Spurr et al. (2018), Ma et al. (2018), Bulat & Tzimiropoulos (2018), Si et al. (2018), Eriksson et al.
(2018), Camposeco et al. (2018), Briales et al. (2018), Luo et al. (2018), Haefner et al. (2018),
Zhang & Funkhouser (2018), Yu et al. (2018), Deng et al. (2018), Yang et al. (2018), Groueix
et al. (2018), Yang et al. (2018), Barath (2018), Xu et al. (2018), Kumar et al. (2018), Feng et al.
(2018), Sun et al. (2018), Qi et al. (2018), Tekin et al. (2018), Tulsiani et al. (2018), Xian et al.
(2018), ?, Lee et al. (2018), Zhan et al. (2018), Girdhar et al. (2018), Dong et al. (2018), Li
et al. (2018), Dong et al. (2018), Liu et al. (2018), Li et al. (2018), Garcia-Hernando et al. (2018),
Saquib Sarfraz et al. (2018), Kumar et al. (2018), Wang et al. (2018), Zhou et al. (2018), Pavlakos
et al. (2018), Baradel et al. (2018), Choi et al. (2018), Sun et al. (2018), Meyer et al. (2018),
Bideau et al. (2018), Dong et al. (2018), Zhou et al. (2018), Lin & Hung (2018), Zhu et al. (2018),
Xue et al. (2018), Tu et al. (2018), Li et al. (2018), Zhang & Funkhouser (2018), He et al. (2018),
Wang et al. (2018), Maninis et al. (2018), Huang et al. (2018), Vasu et al. (2018), Yi et al. (2018),
Cheng et al. (2018), Khan & Sundaramoorthi (2018), Xu et al. (2018), Wang et al. (2018), Zhang
& Funkhouser (2018), Yu et al. (2018), Zhang & Funkhouser (2018), Hui et al. (2018), Lin &
Hung (2018), Reddy Mopuri et al. (2018), Liang et al. (2018), Ren & Jae Lee (2018), Hadad et al.
(2018), Merget et al. (2018), Huang et al. (2018), Song et al. (2018), Shen (2018), Shen (2018),
Liu et al. (2018), Wang et al. (2018), Weiler et al. (2018), Acuna et al. (2018), Fey et al. (2018),
Kossaifi et al. (2018), Arnab et al. (2018), Wang et al. (2018), Yu et al. (2018), Qi et al. (2018),
6

126

Published as a conference paper at SIGBOVIK 2020

Zhang & Funkhouser (2018), Ren & Jae Lee (2018), Li et al. (2018), Kong & Fowlkes (2018),
Noh et al. (2018), Chuang et al. (2018), Hua et al. (2018), Deng et al. (2018), Zhu et al. (2018),
Jae Hwang et al. (2018), Song et al. (2018), Lee et al. (2018), Chen et al. (2018), Zhou et al.
(2018), Kalayeh et al. (2018), Fan & Zhou (2018), Singh et al. (2018), Li et al. (2018), Uijlings
et al. (2018), Kim et al. (2018), Ehret & Arias (2018), Chao et al. (2018), Xiao et al. (2018), Wang
et al. (2018), Liu et al. (2018), Chen et al. (2018), Song et al. (2018), Chen et al. (2018), Sung
et al. (2018), Caesar et al. (2018), Johnson et al. (2018), Cao et al. (2018), Jayaraman & Grauman
(2018), Wang et al. (2018), Yang et al. (2018), Chen et al. (2018), Ge et al. (2018), Cao et al.
(2018), Wan et al. (2018), Yu et al. (2018), Xu et al. (2018), Zhang & Funkhouser (2018), Hong
et al. (2018), Chen et al. (2018), Wang et al. (2018), Shen (2018), Zhang & Funkhouser (2018),
Wang et al. (2018), Sun et al. (2018), Hu et al. (2018), Lee et al. (2018), Cheng et al. (2018),
Wang et al. (2018), Adel Bargal et al. (2018), Yang et al. (2018), Xu et al. (2018), Wang et al.
(2018), Zlateski et al. (2018), Chang et al. (2018), Hu et al. (2018), Bai et al. (2018), Qi et al.
(2018), Tulyakov et al. (2018), Pan et al. (2018), Honari et al. (2018), Pal & Balasubramanian
(2018), ?, Lee et al. (2018), Gordon et al. (2018), Zhou et al. (2018), Wang et al. (2018), Laude
et al. (2018), Eykholt et al. (2018), Joo et al. (2018), Zeng et al. (2018), Han et al. (2018), Haris
et al. (2018), Chen et al. (2018), Cahill et al. (2018), Abdelhamed et al. (2018), Niklaus & Liu
(2018), Wang et al. (2018), Jeon et al. (2018), Sironi et al. (2018), Zhang & Funkhouser (2018),
Kat et al. (2018), Song et al. (2018), Takeda et al. (2018), Liao et al. (2018), Wang et al. (2018),
Yang et al. (2018), Prashnani et al. (2018), Tang et al. (2018), Zhang & Funkhouser (2018), Wu
et al. (2018), Gilbert et al. (2018), Yu et al. (2018), Wei et al. (2018), Hong et al. (2018), Litany
et al. (2018), Zulqarnain Gilani & Mian (2018), Dinesh Reddy et al. (2018), Zhi et al. (2018),
Price et al. (2018), Richter & Roth (2018), He et al. (2018), Stutz & Geiger (2018), Georgakis
et al. (2018), Liu et al. (2018), Yin & Shi (2018), Pritts et al. (2018), Fu et al. (2018), Miraldo
et al. (2018), Wang et al. (2018), Wang et al. (2018), Li et al. (2018), Zou et al. (2018), Batsos
et al. (2018), Pang et al. (2018), Liu et al. (2018), Zhao et al. (2018), Nie et al. (2018), Chang
et al. (2018), Xu et al. (2018), Wu et al. (2018), Fang et al. (2018), Zanfir et al. (2018), Marinoiu
et al. (2018), Yang et al. (2018), Xu et al. (2018), Peng & Wang (2018), Cherian et al. (2018),
Zhao et al. (2018), Huang et al. (2018), Peng & Wang (2018), Feng et al. (2018), Yao et al.
(2018), Gupta et al. (2018), Shen (2018), Wang et al. (2018), Li et al. (2018), Shi et al. (2018),
Shen (2018), Zhang & Funkhouser (2018), Pernici et al. (2018), Guo & Cheung (2018), Xu et al.
(2018), Hold-Geoffroy et al. (2018), Xiong et al. (2018), Kligler et al. (2018), Silva et al. (2018),
Ding et al. (2018), Yu et al. (2018), Yang et al. (2018), Zhong et al. (2018), Kligvasser et al.
(2018), Yu et al. (2018), Rott Shaham & Michaeli (2018), Hu et al. (2018), Zhang & Funkhouser
(2018), Qian et al. (2018), Chen et al. (2018), Mildenhall et al. (2018), Qu et al. (2018), Zhang &
Funkhouser (2018), Su et al. (2018), Kostrikov et al. (2018), Tewari et al. (2018), Bloesch et al.
(2018), Wang et al. (2018), Liu et al. (2018), Wang et al. (2018), Verma et al. (2018), Agudo et al.
(2018), Brahmbhatt et al. (2018), Huang et al. (2018), Yuan et al. (2018), Slavcheva et al. (2018),
Nath Kundu et al. (2018), Yi et al. (2018), Korman et al. (2018), Zanfir et al. (2018), Zhang &
Funkhouser (2018), Jacob et al. (2018), Van Horn et al. (2018), Cao et al. (2018), Jenni & Favaro
(2018), Zhu et al. (2018), Huang et al. (2018), Keller et al. (2018), Liu et al. (2018), Duan et al.
(2018), Yu et al. (2018), Atapour-Abarghouei & Breckon (2018), Liang et al. (2018), Huang et al.
(2018), Teo et al. (2018), Ben Tanfous et al. (2018), Turek & Huth (2018), Xu et al. (2018), Shi
et al. (2018), Li et al. (2018), Li et al. (2018), Tulsiani et al. (2018), Isokane et al. (2018), Liao
et al. (2018), Muralikrishnan et al. (2018), Mo et al. (2018), Im et al. (2018), ?, Fang et al. (2018),
Sun et al. (2018), Larsson et al. (2018), Vongkulbhisal et al. (2018), Chu et al. (2018), Zhang &
Funkhouser (2018), Grabner et al. (2018), Li et al. (2018), Poms et al. (2018), Chen et al. (2018),
Shin et al. (2018), Pan et al. (2018), Zhao et al. (2018), Liu et al. (2018), Barnea & Ben-Shahar
(2018), Palacio et al. (2018), Shocher et al. (2018), Wang et al. (2018), Mosinska et al. (2018),
Bauchet & Lafarge (2018), Chen et al. (2018), Yair & Michaeli (2018), Gou et al. (2018), Wloka
et al. (2018), Zhang & Funkhouser (2018), Lefkimmiatis (2018), Li et al. (2018), Jo et al. (2018),
Liu et al. (2018), Li et al. (2018), Ren & Jae Lee (2018), Zhang & Funkhouser (2018), Vasu
et al. (2018), Lei et al. (2018), Chen et al. (2018), Zhang & Funkhouser (2018), Juefei-Xu et al.
(2018), Hoshen & Wolf (2018), Mukherjee et al. (2018), Chen et al. (2018), Douze et al. (2018),
Zhang & Funkhouser (2018), Gast & Roth (2018), Sabokrou et al. (2018), Akhtar et al. (2018),
Hu et al. (2018), Siarohin et al. (2018), Homayounfar et al. (2018), Kolouri et al. (2018), Zhang
& Funkhouser (2018), Kozerawski & Turk (2018), Ikami et al. (2018), Mejjati et al. (2018), Yang
et al. (2018), Deshpande et al. (2018), ?, Regmi & Borji (2018), Dekel et al. (2018), Suzuki et al.
(2018), Birdal et al. (2018), Chen et al. (2018), Zhou et al. (2018), Nath Kundu et al. (2018), Luo
7

127

Published as a conference paper at SIGBOVIK 2020

et al. (2018), Singh et al. (2018), Hu et al. (2018), Shen (2018), Gurari et al. (2018), Babu Sam
et al. (2018), Choi et al. (2018), Novotny et al. (2018), Douze et al. (2018), Li et al. (2018),
Ghasedi Dizaji et al. (2018), Anderson et al. (2018), Yang et al. (2018), Mohapatra et al. (2018),
Lee et al. (2018), Zamir et al. (2018), Saito et al. (2018), Wu et al. (2018), Liu et al. (2018),
Sankaranarayanan et al. (2018), Fawzi et al. (2018), Mancini et al. (2018), Yang et al. (2018),
Zhou et al. (2018), Zhang & Funkhouser (2018), Xie et al. (2018), Mac Aodha et al. (2018),
Järemo Lawin et al. (2018), Jie et al. (2018), Song et al. (2018), Yang et al. (2018), Gallego
et al. (2018), Bagautdinov et al. (2018), Tatarchenko et al. (2018), Paschalidou et al. (2018), Kato
et al. (2018), Xu et al. (2018), Yang et al. (2018), ?, Larsson et al. (2018), Zuffi et al. (2018),
Xu et al. (2018), Xia et al. (2018), Engilberge et al. (2018), Tan et al. (2018), LaLonde et al.
(2018), Chen et al. (2018), He et al. (2018), Sharma et al. (2018), Chen et al. (2018), Ehsani
et al. (2018), Zhao et al. (2018), Cheng et al. (2018), Cai et al. (2018), Gordon et al. (2018), Liu
et al. (2018), Cui et al. (2018), Radosavovic et al. (2018), Balajee Vasudevan et al. (2018), Zhai
et al. (2018), Wang et al. (2018), Zhang & Funkhouser (2018), Kim et al. (2018), Roveri et al.
(2018), Bozek et al. (2018), Bhattacharyya et al. (2018), Zhang & Funkhouser (2018), Wang
et al. (2018), Teney et al. (2018), Hu et al. (2018), Li et al. (2018), Zhuang et al. (2018), Zhang
& Funkhouser (2018), Wang et al. (2018), Verma et al. (2018), Cao et al. (2018), Faraone et al.
(2018), Bernard et al. (2018), Zhang & Funkhouser (2018), Chen et al. (2018), Rozantsev et al.
(2018), Kim et al. (2018), Senocak et al. (2018), Gidaris & Komodakis (2018), Wang et al. (2018),
Johnston et al. (2018), Mentzer et al. (2018), Skafte Detlefsen et al. (2018), Berman et al. (2018),
Poursaeed et al. (2018), Yu et al. (2018), Kanbak et al. (2018), Zhu et al. (2018), Kumar Roy et al.
(2018), Angelina Uy & Hee Lee (2018), Yu et al. (2018), Zhou et al. (2018), Murez et al. (2018),
Sandler et al. (2018), Niu et al. (2018), Schilling et al. (2018), Zhuang et al. (2018), Shen (2018),
Landrieu & Simonovsky (2018), Zhu et al. (2018), Dai et al. (2018), Lan et al. (2018), Yun & Sim
(2018), Xie et al. (2018), Liu et al. (2018), Kim et al. (2018), Cao et al. (2018), Fu et al. (2018),
Brachmann & Rother (2018), Rad et al. (2018), Kim et al. (2018), Pumarola et al. (2018), Sadeghi
et al. (2018), Ma et al. (2018), Urooj & Borji (2018), Bastani et al. (2018), Paul & Roumeliotis
(2018), Rematas et al. (2018), Shin et al. (2018), Liang et al. (2018), Nie et al. (2018), Wan et al.
(2018), Zhang & Funkhouser (2018), Lao & Ait-Aider (2018), Tanaka et al. (2018), Meshgi et al.
(2018), Bapat et al. (2018), He et al. (2018), Wang et al. (2018), Wang et al. (2018), Wang et al.
(2018), Tang et al. (2018), Wang et al. (2018), Wang et al. (2018), Li et al. (2018), Bouritsas et al.
(2018), Zhang & Funkhouser (2018), Jain et al. (2018), Mascharka et al. (2018), Xu et al. (2018),
Wang et al. (2018), Agrawal et al. (2018), Ahn & Kwak (2018), Fouhey et al. (2018), Inoue et al.
(2018), Kanezaki et al. (2018), He et al. (2018), Chavdarova et al. (2018), Miao et al. (2018),
Sun et al. (2018), Shin Yoon et al. (2018), Han et al. (2018), Moon et al. (2018), Zheng et al.
(2018), Huang et al. (2018), Zhang & Funkhouser (2018), Cheng et al. (2018), Zhu et al. (2018),
Luvizon et al. (2018), Wan et al. (2018), Zhong et al. (2018), Andriluka et al. (2018), Wu et al.
(2018), Cao et al. (2018), Liu et al. (2018), Luo et al. (2018), Liu et al. (2018), Li et al. (2018),
Narayana et al. (2018), Shen (2018), Yang et al. (2018), Wang et al. (2018), Xu et al. (2018),
Pan et al. (2018), Madsen et al. (2018), Piergiovanni & Ryoo (2018), Wang et al. (2018), Tang
et al. (2018), Xu et al. (2018), Abu Farha et al. (2018), Ren & Jae Lee (2018), Si et al. (2018),
Zhou et al. (2018), Shi et al. (2018), Zanfir et al. (2018), Li et al. (2018), Chang et al. (2018),
Maqueda et al. (2018), Hu et al. (2018), Wei et al. (2018), Lee et al. (2018), Li et al. (2018), Wang
et al. (2018), Dorta et al. (2018), Tokozume et al. (2018), Volpi et al. (2018), Yu et al. (2018),
Sharma et al. (2018), Lin & Hung (2018), ?, Xian et al. (2018), Tanaka et al. (2018), Aneja et al.
(2018), Cheng et al. (2018), Mallasto & Feragen (2018), Gan et al. (2018), Firman et al. (2018),
Abdullah Jamal et al. (2018), Kobayashi (2018), Mostajabi et al. (2018), ?, Kafle et al. (2018),
Ma et al. (2018), Mahjourian et al. (2018), Liu et al. (2018), Liu et al. (2018), Zhao et al. (2018),
?, Chao et al. (2018), Joo et al. (2018), You et al. (2018), Li et al. (2018), Jain et al. (2018), Shen
(2018), Koniusz et al. (2018), Lin & Hung (2018), Tian et al. (2018), Cui et al. (2018), Zhang
& Funkhouser (2018), Qi et al. (2018), Zellers et al. (2018), Tan et al. (2018), Dhawale et al.
(2018), Wang et al. (2018), Chen et al. (2018), Sage et al. (2018), Bertasius et al. (2018), Qi et al.
(2018), Liao et al. (2018), Veit et al. (2018), Park et al. (2018), Gao et al. (2018), Huang et al.
(2018), Gavrilyuk et al. (2018), Possas et al. (2018), Bao et al. (2018), Richard et al. (2018), Li
et al. (2018), Yu et al. (2018), Fan & Zhou (2018), Wu et al. (2018), Ristani & Tomasi (2018),
Gu et al. (2018), Doughty et al. (2018), Hasan et al. (2018), Anderson et al. (2018), Nguyen &
Okatani (2018), Massiceti et al. (2018), Wu et al. (2018), Li et al. (2018), Yeh et al. (2018), Liang
et al. (2018), Ehsani et al. (2018), Cai et al. (2018), Huang et al. (2018), Christie et al. (2018),
Peng & Wang (2018), Rao et al. (2018), Zhang & Funkhouser (2018), Hui et al. (2018), Xu et al.
8

128

Published as a conference paper at SIGBOVIK 2020

(2018), Blau & Michaeli (2018), Zhou et al. (2018), Mirdehghan et al. (2018), Smith et al. (2018),
Baradad et al. (2018), Tlusty et al. (2018), Chen et al. (2018), Sengupta et al. (2018), Chen et al.
(2018), Meka et al. (2018), Zhang & Funkhouser (2018), Jin et al. (2018), Anirudh et al. (2018),
Men et al. (2018), Fajtl et al. (2018), Pan et al. (2018), Su et al. (2018), Srinivasan et al. (2018),
Sakakibara et al. (2018), Levis et al. (2018), Mahesh Mohan & Rajagopalan (2018), Huang et al.
(2018), Can Karaimer & Brown (2018), Tran et al. (2018), Fan & Zhou (2018), Yang et al. (2018),
Sultani et al. (2018), Zhou et al. (2018), Yang et al. (2018), Ding et al. (2018), Li et al. (2018), Li
et al. (2018), Liu et al. (2018), Hara et al. (2018), Xu et al. (2018), Zhao et al. (2018), Gao et al.
(2018), Zhang & Funkhouser (2018), Bilinski & Prisacariu (2018), Kaneko et al. (2018), Li et al.
(2018), Sajjadi et al. (2018), Zhang & Funkhouser (2018), Li et al. (2018), Chen et al. (2018),
Yang et al. (2018), Baslamisli et al. (2018), Yoo et al. (2018), Korman et al. (2018), Tesfaldet
et al. (2018), Bao et al. (2018), Akkaynak & Treibitz (2018), Barath (2018), Lei et al. (2018),
Nguyen & Okatani (2018), Zhang & Funkhouser (2018), Lan et al. (2018), Zhang & Funkhouser
(2018), Ma et al. (2018), Wei et al. (2018), Zhang & Funkhouser (2018), Nilsson & Sminchisescu
(2018), Wu et al. (2018), Chen et al. (2018), Zhang & Funkhouser (2018), Wang et al. (2018),
Krishna et al. (2018), Tychsen-Smith & Petersson (2018), Shen (2018), ?, Gonzalez-Garcia et al.
(2018), Rocco et al. (2018), Gao et al. (2018), Liu et al. (2018), Pirinen & Sminchisescu (2018),
Lu et al. (2018), Luo et al. (2018), Ma et al. (2018), Liu et al. (2018), Zhang & Funkhouser
(2018), Tung et al. (2018), Huang et al. (2018), Choutas et al. (2018), Zhang & Funkhouser
(2018), Wang et al. (2018), He et al. (2018), Kumar Roy et al. (2018), He et al. (2018), Wang
et al. (2018), Deng et al. (2018), Chen et al. (2018), Chu et al. (2018), Kanazawa et al. (2018),
Hu et al. (2018), Amirul Islam et al. (2018), Zhang & Funkhouser (2018), Hsiao & Grauman
(2018), Niu et al. (2018), Gu et al. (2018), Wang et al. (2018), Taira et al. (2018), Zhu et al.
(2018), Lu et al. (2018), Qiao et al. (2018), Chen et al. (2018), Cao et al. (2018), Hu et al. (2018),
Wei et al. (2018), Wang et al. (2018), Yu et al. (2018), ?, Pavlakos et al. (2018), Speciale et al.
(2018), Vianello et al. (2018), Wu et al. (2018), Tran et al. (2018), Zhao et al. (2018), Huang et al.
(2018), Wug Oh et al. (2018), Richard et al. (2018), Sigurdsson et al. (2018), Zhao et al. (2018),
Cheng et al. (2018), Zhou et al. (2018), Kanehira et al. (2018), Fujimura et al. (2018), Hu et al.
(2018), Li et al. (2018), Tsai et al. (2018), Kendall et al. (2018), Li et al. (2018), Gorji & Clark
(2018), Wang et al. (2018), Fan & Zhou (2018), Andreopoulos et al. (2018), Han et al. (2018),
Lyu et al. (2018), Azadi et al. (2018), Shlizerman et al. (2018), Yang et al. (2018), Yagi et al.
(2018), Annadani & Biswas (2018), Ravi et al. (2018), Wang et al. (2018), Iscen et al. (2018),
Iscen et al. (2018), Yang et al. (2018), Liu et al. (2018), Zhang & Funkhouser (2018), Patro &
Namboodiri (2018), Niu et al. (2018), Ramanishka et al. (2018), Ak et al. (2018), Wehrmann &
Barros (2018), Liu et al. (2018), Su et al. (2018), Deng et al. (2018), Song et al. (2018), Mallya
& Lazebnik (2018), Wang et al. (2018), Cihan Camgoz et al. (2018), Wang et al. (2018), Baraldi
et al. (2018), Chen et al. (2018), Su et al. (2018), Long et al. (2018), Feichtenhofer et al. (2018),
Hao et al. (2018), Park et al. (2018), Tung et al. (2018), Shanu et al. (2018), Chen et al. (2018),
Dolhansky & Canton Ferrer (2018), Zhang & Funkhouser (2018), Zhuang et al. (2018), Kuen et al.
(2018), Wang et al. (2018), Lv et al. (2018), Sun et al. (2018), Wu et al. (2018), Chen et al. (2018),
Hong et al. (2018), Chen et al. (2018), Pinheiro (2018), Riaz Muhammad et al. (2018), ?, Sznaier
& Camps (2018), Wang et al. (2018), Wei et al. (2018), Shen (2018), Marsden et al. (2018),
Teja Mullapudi et al. (2018), Xu et al. (2018), Russo et al. (2018), Lezama et al. (2018), Rebuffi
et al. (2018), Lin & Hung (2018), Amayo et al. (2018), Ikami et al. (2018), Zhang & Funkhouser
(2018), Lui et al. (2018), Tao et al. (2018), Kupyn et al. (2018), Li et al. (2018), Li et al. (2018),
Galdran et al. (2018), Gu et al. (2018), Zhang & Funkhouser (2018), Sheng et al. (2018), Yokota
et al. (2018), Shen (2018), Duan et al. (2018), Yu et al. (2018), Li et al. (2018), Xu et al. (2018),
Baumgartner et al. (2018), Joo et al. (2018), Baek et al. (2018), Balakrishnan et al. (2018), Liu
et al. (2018), Gkioxari et al. (2018), Sener & Yao (2018), Genova et al. (2018), Alldieck et al.
(2018), Hu et al. (2018), Huynh et al. (2018), Ge et al. (2018), Nagrani et al. (2018), Rhodin
et al. (2018), Zhang & Funkhouser (2018), Xian et al. (2018), Orekondy et al. (2018), Henriques
& Vedaldi (2018), Bhattacharyya et al. (2018), Puig et al. (2018), Sankaranarayanan et al. (2018),
Ghosh et al. (2018), An et al. (2018), ?, Qian et al. (2018), Rupprecht et al. (2018), Khrulkov
& Oseledets (2018), Prakash et al. (2018), Vicol et al. (2018), Mathews et al. (2018), Sattler
et al. (2018), Liu et al. (2018), Pumarola et al. (2018), Xie et al. (2018), Villegas et al. (2018),
Chen et al. (2018), Gong et al. (2018), Kim et al. (2018), Kim et al. (2018), Wang et al. (2018),
Zoph et al. (2018), Ren & Jae Lee (2018), Chen et al. (2018), Fong & Vedaldi (2018), Zhou
et al. (2018), Dogan et al. (2018), Liu et al. (2018), Van Horn et al. (2018), Park et al. (2018),
Choi et al. (2018), Wang et al. (2018), Qi et al. (2018), Wu et al. (2018), Zhang & Funkhouser
9

129

Published as a conference paper at SIGBOVIK 2020

(2018), Huang et al. (2018), Xie et al. (2018), Esser et al. (2018), Liu et al. (2018), Marcos et al.
(2018), Lambert et al. (2018), Luo et al. (2018), Wang et al. (2018), Chandra et al. (2018), Shin
et al. (2018), Sun et al. (2018), Fan & Zhou (2018), Yellin et al. (2018), Sun et al. (2018), Li
et al. (2018), Hui et al. (2018), Song et al. (2018), Jiang et al. (2018), Runia et al. (2018), Kong
& Fowlkes (2018), Zhang & Funkhouser (2018), Li et al. (2018), Wang et al. (2018), Teixeira
et al. (2018), Xia et al. (2018), Han et al. (2018), Liu et al. (2018), Bibi et al. (2018), Saeedan
et al. (2018), Wan et al. (2018), Wu et al. (2018), Hu et al. (2018), Alperovich et al. (2018),
Gao et al. (2018), Mehr et al. (2018), Konyushkova et al. (2018), Dong et al. (2018), Yu et al.
(2018), Le & Duan (2018), Li et al. (2018), Graham et al. (2018), Chen et al. (2018), Zhang &
Funkhouser (2018), Balakrishnan et al. (2018), Yan et al. (2018), ?, Jiang et al. (2018), Dalca
et al. (2018), Raposo & Barreto (2018), Caicedo et al. (2018), Haehn et al. (2018), Wang et al.
(2018), Nathan Mundhenk et al. (2018), Keshari et al. (2018), Noroozi et al. (2018), Beluch et al.
(2018), Ye et al. (2018), Tabernik et al. (2018), Li et al. (2018), Chavdarova et al. (2018), Chen
et al. (2018), Zhou et al. (2018), Zhu et al. (2018), Ulyanov et al. (2018), Lin & Hung (2018),
Chen et al. (2018), Teney et al. (2017), Zhao et al. (2017), Juefei-Xu et al. (2017), Aksoy et al.
(2017), Trigeorgis et al. (2017), Booth et al. (2017), Kiran Adhikarla et al. (2017), Kotaru &
Katti (2017), Tanaka et al. (2017), Haeusser et al. (2017), Queau et al. (2017), Varol et al. (2017),
Long & Hua (2017), Hyeong Hong et al. (2017), Wang et al. (2017), Qin et al. (2017), Lea et al.
(2017), Gurumurthy et al. (2017), Talmi et al. (2017), Boukhayma et al. (2017), Li et al. (2017),
Mahasseni et al. (2017), Liu et al. (2017), Caelles et al. (2017), Seki & Pollefeys (2017), Ganju
et al. (2017), Vedantam et al. (2017), Cevikalp & Triggs (2017), Godard et al. (2017), Larsson &
Olsson (2017), Ren et al. (2017), Nam et al. (2017), Wijmans & Furukawa (2017), Krause et al.
(2017), Das et al. (2017), Lee et al. (2017), Zhang et al. (2017), He et al. (2017), Kong & Fowlkes
(2017), Lu et al. (2017), Li et al. (2017), Yan et al. (2017), Chen et al. (2017), Liu et al. (2017),
Mustafa & Hilton (2017), Garcia-Hernando & Kim (2017), Arnab & Torr (2017), Jampani et al.
(2017), Michel et al. (2017), Yu et al. (2017), Hu et al. (2017), Yu et al. (2017), Wang et al.
(2017), Ye et al. (2017), Zhou et al. (2017), Zhang et al. (2017), Proenca & Neves (2017), Chao
et al. (2017), Zhang et al. (2017), Takahashi et al. (2017), Law et al. (2017), Sigurdsson et al.
(2017), Wang et al. (2017), Fan et al. (2017), Al-Halah & Stiefelhagen (2017), Lai et al. (2017),
Zhou et al. (2017), Durand et al. (2017), Qi et al. (2017), Tian et al. (2017), Niklaus et al. (2017),
Wan et al. (2017), Cao et al. (2017), Wu et al. (2017), Zhang et al. (2017), Zhang et al. (2017),
Kang et al. (2017), Da et al. (2017), Kehl et al. (2017), Richard et al. (2017), Xue et al. (2017),
Ma et al. (2017), Huang et al. (2017), Liu et al. (2017), Ito & Okatani (2017), Lin et al. (2017),
Larsson & Olsson (2017), Veit et al. (2017), Veit et al. (2017), Liang et al. (2017), Bertasius
et al. (2017), Zhai et al. (2017), Khoreva et al. (2017), Barron & Tsai (2017), Chunseong Park
et al. (2017), Mostegel et al. (2017), Diba et al. (2017), Wang et al. (2017), Gong et al. (2017),
Logothetis et al. (2017), Hu et al. (2017), Dong et al. (2017), Girdhar et al. (2017), Dave et al.
(2017), Hu et al. (2017), Bernard et al. (2017), Liang et al. (2017), Vondrick & Torralba (2017),
Mahasseni et al. (2017), Figurnov et al. (2017), Su & Hua (2017), Zhang et al. (2017), Ke et al.
(2017), Zhao et al. (2017), Ge & Yu (2017), Hussein et al. (2017), Abbaspour Tehrani et al.
(2017), Hu et al. (2017), Isola et al. (2017), Chattopadhyay et al. (2017), Simon et al. (2017), Zhu
et al. (2017), Xu et al. (2017), Wang et al. (2017), Duan et al. (2017), Akhtar et al. (2017), Yang
et al. (2017), Cui et al. (2017), Li et al. (2017), Ioannou et al. (2017), Sandhan & Young Choi
(2017), Chollet (2017), Richardson et al. (2017), Qian et al. (2017), Su & Hua (2017), Xu et al.
(2017), Yan et al. (2017), Zamir et al. (2017), Zhong et al. (2017), Cao et al. (2017), Li et al.
(2017), Li et al. (2017), Yang et al. (2017), Zheng et al. (2017), Jie et al. (2017), Slavcheva et al.
(2017), Mueller et al. (2017), Tran et al. (2017), Tran et al. (2017), Liu et al. (2017), Pascoe et al.
(2017), Isack et al. (2017), Caba Heilbron et al. (2017), Zhu et al. (2017), Kong & Fowlkes (2017),
Schonberger et al. (2017), Xie et al. (2017), Su & Hua (2017), Arsalan Soltani et al. (2017), Zhao
et al. (2017), Wang et al. (2017), O’Toole et al. (2017), Gu et al. (2017), Cui et al. (2017), Wei
et al. (2017), Yang et al. (2017), Plotz & Roth (2017), Swoboda et al. (2017), Swoboda et al.
(2017), Swoboda et al. (2017), Long & Hua (2017), Sattler et al. (2017), Liu et al. (2017), Baraldi
et al. (2017), Kosti et al. (2017), Kim & Lee (2017), Shi et al. (2017), Kong & Fowlkes (2017),
Hussain et al. (2017), Bibi et al. (2017), Baque et al. (2017), Chunseong Park et al. (2017), Song
et al. (2017), Halber & Funkhouser (2017), Moosavi-Dezfooli et al. (2017), Tavakoli et al. (2017),
Elhamifar & Clara De Paolis Kaluza (2017), Misra et al. (2017), Zeng et al. (2017), Yeo et al.
(2017), Savinov et al. (2017), Chu et al. (2017), Li et al. (2017), Zhou et al. (2017), Zhang et al.
(2017), Nakamura et al. (2017), Zhuang et al. (2017), Le et al. (2017), Chen et al. (2017), Chen
et al. (2017), Shen et al. (2017), Lin et al. (2017), Trager et al. (2017), Patrini et al. (2017), Shen
10

130

Published as a conference paper at SIGBOVIK 2020

et al. (2017), Liu et al. (2017), Li et al. (2017), Zendel et al. (2017), Ge & Yu (2017), Rebuffi
et al. (2017), Iqbal et al. (2017), Zhang et al. (2017), Almazan et al. (2017), Chabot et al. (2017),
Ding et al. (2017), Tepper & Sapiro (2017), Chan et al. (2017), Iscen et al. (2017), Yang et al.
(2017), Wang et al. (2017), Shrivastava et al. (2017), Lin et al. (2017), Rota Bulo et al. (2017),
Kim & Lee (2017), Dekel et al. (2017), Ji et al. (2017), Blasinski et al. (2017), Xu et al. (2017),
Huang et al. (2017), Yang et al. (2017), Luo et al. (2017), Chang et al. (2017), Zeng et al. (2017),
Elhabian & Whitaker (2017), Huang et al. (2017), Gao et al. (2017), Zhou et al. (2017), Yan et al.
(2017), Yi et al. (2017), Rengarajan et al. (2017), Hu et al. (2017), Huang et al. (2017), Gong et al.
(2017), Diba et al. (2017), Knobelreiter et al. (2017), Zhu et al. (2017), Li et al. (2017), Pansari
& Pawan Kumar (2017), Tolias & Chum (2017), Li et al. (2017), Rezende et al. (2017), Lin et al.
(2017), Osman Ulusoy et al. (2017), Yu et al. (2017), Wei et al. (2017), Lee et al. (2017), Poggi &
Mattoccia (2017), Ilg et al. (2017), Wang et al. (2017), Feng et al. (2017), Murdock & De la Torre
(2017), Tome et al. (2017), Gorji & Clark (2017), Lam et al. (2017), Bai et al. (2017), Kim & Lee
(2017), Shi et al. (2017), Wu et al. (2017), Lin et al. (2017), Deng et al. (2017), Veeravasarapu
et al. (2017), Wang et al. (2017), Wang et al. (2017), Gupta et al. (2017), Tulsiani et al. (2017),
Tulsiani et al. (2017), Chen et al. (2017), Zhu et al. (2017), Perazzi et al. (2017), Marino et al.
(2017), Ge & Yu (2017), Ryan Fanello et al. (2017), Pathak et al. (2017), Yun et al. (2017), Wang
et al. (2017), Wang et al. (2017), Tang et al. (2017), Hyeong Hong et al. (2017), Jang et al. (2017),
Hayat et al. (2017), Yang et al. (2017), Palmer et al. (2017), Sawatzky et al. (2017), Valmadre
et al. (2017), Strecke et al. (2017), Moreno-Noguer (2017), Qin et al. (2017), Tang et al. (2017),
Li et al. (2017), Liu et al. (2017), Liu et al. (2017), Zhao et al. (2017), Martinez et al. (2017),
Johnson et al. (2017), Buch et al. (2017), Cui et al. (2017), Peng et al. (2017), Straub et al. (2017),
Ithapu et al. (2017), Treible et al. (2017), Xu et al. (2017), Usumezbas et al. (2017), Bak & Carr
(2017), Liu et al. (2017), Yang et al. (2017), Salvador et al. (2017), Cheng et al. (2017), Guo
& Chao (2017), Wang et al. (2017), Zhang et al. (2017), Gholami & Pavlovic (2017), Dai et al.
(2017), Chen et al. (2017), Cosmin Duta et al. (2017), Li et al. (2017), Li et al. (2017), Mao
et al. (2017), Gan et al. (2017), Tai et al. (2017), Wang et al. (2017), Yu et al. (2017), Kodirov
et al. (2017), Jevnisek & Avidan (2017), Li et al. (2017), Hou et al. (2017), Zhang et al. (2017),
Cherian et al. (2017), Jiang & Li (2017), Zhou et al. (2017), Bailer et al. (2017), Schops et al.
(2017), Xiong et al. (2017), Li et al. (2017), Ke et al. (2017), Ajanthan et al. (2017), Sun et al.
(2017), Lv et al. (2017), Vestner et al. (2017), Malti & Herzet (2017), Kaltenmark et al. (2017),
Han et al. (2017), Aljundi et al. (2017), Kar et al. (2017), Tokmakov et al. (2017), You et al.
(2017), Walecki et al. (2017), Xiao et al. (2017), Armagan et al. (2017), Rogez et al. (2017),
Jun Koh & Kim (2017), Hu et al. (2017), Shi et al. (2017), Wu et al. (2017), Sun et al. (2017), Xu
et al. (2017), Chunseong Park et al. (2017), Jin et al. (2017), He et al. (2017), Roy & Todorovic
(2017), Tang et al. (2017), Ravi et al. (2017), Kim & Lee (2017), Kim & Lee (2017), Riegler
et al. (2017), Lefkimmiatis (2017), Janai et al. (2017), Tian et al. (2017), Li et al. (2017), Jin
et al. (2017), Fernando et al. (2017), Shen et al. (2017), Kayaba & Kokumai (2017), Dutt Jain
et al. (2017), Yoo et al. (2017), Yuan et al. (2017), Simonovsky & Komodakis (2017), Cole
et al. (2017), Zhang et al. (2017), Bousmalis et al. (2017), Yokota & Hontani (2017), Zhou et al.
(2017), Amirul Islam et al. (2017), Yu et al. (2017), Gu et al. (2017), Kalogerakis et al. (2017),
Tsai et al. (2017), Xu et al. (2017), Morteza Safdarnejad & Liu (2017), Zhang et al. (2017), Chen
et al. (2017), Yan et al. (2017), Herath et al. (2017), Fu et al. (2017), Zhang et al. (2017), Chen
et al. (2017), Nah et al. (2017), Wang et al. (2017), Liu et al. (2017), Li et al. (2017), Li et al.
(2017), Zhang et al. (2017), Taniai et al. (2017), Santa Cruz et al. (2017), Srinivasan et al. (2017),
Shimano et al. (2017), Jiang & Li (2017), Gatys et al. (2017), Chen et al. (2017), Yan et al. (2017),
Wang et al. (2017), Wang et al. (2017), Shen et al. (2017), Takatani et al. (2017), Cheng et al.
(2017), Jiang & Li (2017), Qu et al. (2017), Mandal et al. (2017), Hu et al. (2017), Haouchine &
Cotin (2017), Vongkulbhisal et al. (2017), Karlinsky et al. (2017), Shih et al. (2017), Yim et al.
(2017), Xia et al. (2017), Pohlen et al. (2017), Ranjan & Black (2017), Weng et al. (2017), Bian
et al. (2017), Zhang et al. (2017), Jeon & Kim (2017), Ren et al. (2017), Song et al. (2017),
Gomez et al. (2017), Khan et al. (2017), Lao & Sundaramoorthi (2017), Chang et al. (2017), Wu
et al. (2017), Zhang et al. (2017), Antunes et al. (2017), Koller et al. (2017), Dong et al. (2017),
Bagautdinov et al. (2017), Wang et al. (2017), Zhang et al. (2017), Jia et al. (2017), Peng et al.
(2017), Yang et al. (2017), Zhang et al. (2017), Pan et al. (2017), Ramakrishnan et al. (2017),
Jiang & Li (2017), Joon Oh et al. (2017), Liu et al. (2017), Zhang et al. (2017), Fu et al. (2017),
Vasu & Rajagopalan (2017), Cavallari et al. (2017), Nguyen et al. (2017), Koniusz et al. (2017),
Sheng et al. (2017), Arvanitopoulos et al. (2017), Hosang et al. (2017), Guan & Smith (2017),
Karessli et al. (2017), Ma et al. (2017), Camposeco et al. (2017), Cohen & Weinshall (2017),
11

131

Published as a conference paper at SIGBOVIK 2020

Zweig & Wolf (2017), Zhang et al. (2017), Xian et al. (2017), Wang et al. (2017), Eisenschtat
& Wolf (2017), Wigness & Rogers (2017), Esmaeili et al. (2017), Elbaz et al. (2017), Shaked &
Wolf (2017), Achanta & Susstrunk (2017), Borghi et al. (2017), Wulff et al. (2017), Ledig et al.
(2017), Miksik et al. (2017), Huang et al. (2017), Yu et al. (2017), Mai et al. (2017), Feichtenhofer
et al. (2017), Mo et al. (2017), Zhou et al. (2017), Roberto de Souza et al. (2017), Feichtenhofer
et al. (2017), Caballero et al. (2017), Sharghi et al. (2017), Sengupta et al. (2017), Choi et al.
(2017), Lathuiliere et al. (2017), Dibra et al. (2017), He et al. (2017), Albl et al. (2017), Kong
& Fowlkes (2017), Yuan et al. (2017), Yuan et al. (2017), Shahpaski et al. (2017), Zhang et al.
(2017), Papandreou et al. (2017), Kukelova et al. (2017), Gorelick et al. (2017), Akkaynak et al.
(2017), Speciale et al. (2017), Schuster et al. (2017), Briales & Gonzalez-Jimenez (2017), Zhai
et al. (2017), Rohrbach et al. (2017), Luan et al. (2017), Kembhavi et al. (2017), Kirillov et al.
(2017), Venkateswara et al. (2017), Worrall et al. (2017), Ummenhofer et al. (2017), Dansereau
et al. (2017), Stone et al. (2017), Xie et al. (2017), Huang et al. (2017), Chen et al. (2017),
Sagawa & Satoh (2017), Oyallon (2017), Monti et al. (2017), Fan et al. (2017), Izadinia et al.
(2017), Saito et al. (2017), Yeung et al. (2017), Tran et al. (2017), Balntas et al. (2017), Chen
et al. (2017), Brahmbhatt & Hays (2017), Rozumnyi et al. (2017), Homayounfar et al. (2017), Bai
et al. (2017), Castrejon et al. (2017), Wang et al. (2017), Gidaris & Komodakis (2017), Wang
et al. (2017), Sagonas et al. (2017), Novotny et al. (2017), Zhang et al. (2017), Real et al. (2017),
Toderici et al. (2017), Yang et al. (2017), Dai et al. (2017), Lu et al. (2017), Dian et al. (2017), Xu
et al. (2017), Xu et al. (2017), Chen et al. (2017), Song et al. (2017), Zhu et al. (2017), Sangkloy
et al. (2017), Xu et al. (2017), Ren et al. (2017), Zhuo et al. (2017), Han et al. (2017), Yang
et al. (2017), Chunseong Park et al. (2017), Li et al. (2017), Du et al. (2017), Yeh et al. (2017),
Rong et al. (2017), de Vries et al. (2017), Zhu et al. (2017), Shu et al. (2017), Zhang et al. (2017),
Shu et al. (2017), Zhou et al. (2017), Abdulnabi et al. (2017), Li et al. (2017), Yuan et al. (2017),
Zhang et al. (2017), Wang et al. (2017), Santhanam et al. (2017), Son et al. (2017), Gan et al.
(2017), Elhoseiny et al. (2017), Zhu et al. (2017), Chen et al. (2017), Wu et al. (2017), Zhang et al.
(2017), Yang et al. (2017), Hayder et al. (2017), Makihara et al. (2017), Sun et al. (2017), Sasaki
et al. (2017), Shou et al. (2017), Babu Sam et al. (2017), Venugopalan et al. (2017), Deng et al.
(2017), Lin et al. (2017), Plummer et al. (2017), Liu et al. (2017), Alireza Golestaneh & Karam
(2017), Zhang et al. (2017), Ikami et al. (2017), Dai et al. (2017), Dong et al. (2017), Jang et al.
(2017), Wang et al. (2017), Dai et al. (2017), Bappy et al. (2017), Ehsan Abbasnejad et al. (2017),
Zhang et al. (2017), Dou et al. (2017), Cai et al. (2017), Han et al. (2017), Kong & Fowlkes
(2017), Xiao et al. (2017), Guo & Chao (2017), Yu et al. (2017), Kendall & Cipolla (2017),
Guo & Chao (2017), He et al. (2017), Barath et al. (2017), Levinkov et al. (2017), Jiang & Li
(2017), Rozantsev et al. (2017), Sinha et al. (2017), Lassner et al. (2017), Morgado & Vasconcelos
(2017), Huang et al. (2017), Alameda-Pineda et al. (2017), Kaneko et al. (2017), Huang et al.
(2017), Schober et al. (2017), Wang et al. (2017), Kokkinos (2017), Peng et al. (2017), Rocco
et al. (2017), Butepage et al. (2017), Thermos et al. (2017), Xie et al. (2017), Hao et al. (2017),
Chakraborty et al. (2017), Rao Jerripothula et al. (2017), Babenko & Lempitsky (2017), Kumar
et al. (2017), Bogo et al. (2017), Tateno et al. (2017), Khue Le-Huu & Paragios (2017), Agudo &
Moreno-Noguer (2017), Sicre et al. (2017), Yurchenko & Lempitsky (2017), Popa et al. (2017),
Carreira & Zisserman (2017), Lukezic et al. (2017), Wu et al. (2017), Surh et al. (2017), Morley &
Foroosh (2017), Hu et al. (2017), Li et al. (2017), Zuffi et al. (2017), Papadopoulos et al. (2017),
Zhu et al. (2017), Tseng et al. (2017), Yan et al. (2017), Shamai & Kimmel (2017), Kim & Lee
(2017), Zhang et al. (2017), Sheinin et al. (2017), Son Chung et al. (2017), Insafutdinov et al.
(2017), Brattoli et al. (2017), Su & Hua (2017), Dutt Jain et al. (2017), Li et al. (2017), Pan et al.
(2017), Joshi et al. (2017), Lu et al. (2017), Huang et al. (2017), Bau et al. (2017), Jetley et al.
(2017), Kim & Lee (2017), Haussmann et al. (2017), Yao et al. (2017), Gordo & Larlus (2017),
Maninchedda et al. (2017), Luo et al. (2017), Do et al. (2017), Lezama et al. (2017), Danelljan
et al. (2017), Kuznietsov et al. (2017), Ren et al. (2017), Ke et al. (2017), Daniel Costea et al.
(2017), Brachmann et al. (2017), Sanchez Giraldo et al. (2017), Krull et al. (2017), Tamaazousti
et al. (2017), Yang et al. (2017), Pei et al. (2017), Levis et al. (2017), Nguyen et al. (2017), Qiu
et al. (2017), Xia et al. (2017), Dolz et al. (2017), Nestmeyer & Gehler (2017), Alp Guler et al.
(2017), Spampinato et al. (2017), Zhang et al. (2017), Papoutsakis et al. (2017), Deshpande et al.
(2017), Paul et al. (2017), Clark et al. (2017), Gross et al. (2017), Larsson & Olsson (2017),
Maharaj et al. (2017), Rashid et al. (2017), Goyal et al. (2017), Ufer & Ommer (2017), Ulyanov
et al. (2017), Devrim Kaba et al. (2017), Kalayeh et al. (2017), Schulter et al. (2017), Sun et al.
(2017), Liu et al. (2017), Lopez-Paz et al. (2017), Pavlakos et al. (2017), Kovacs et al. (2017),
Rennie et al. (2017), Pavlakos et al. (2017), Chen et al. (2017), Nech & Kemelmacher-Shlizerman
12

132

Published as a conference paper at SIGBOVIK 2020

(2017), Panda et al. (2017), Upchurch et al. (2017), Mousavian et al. (2017), Panda et al. (2017),
Xie et al. (2017), Luo et al. (2017), Deutsch et al. (2017), Bagherinezhad et al. (2017), Bautista
et al. (2017), Ye et al. (2017), Azadi et al. (2017), Vernaza & Chandraker (2017), Tzeng et al.
(2017), Jaimez et al. (2017), Iyyer et al. (2017), Yatskar et al. (2017), Ramanishka et al. (2017),
Tsai et al. (2017), Ke et al. (2017), Zhu et al. (2017), Amir et al. (2017), Zaki et al. (2017),
Redmon & Farhadi (2017), Wang et al. (2017), Yu et al. (2017), Cao et al. (2017), Kannan et al.
(2017), Huang et al. (2017), Hold-Geoffroy et al. (2017), Hyeong Hong et al. (2017), Haeffele
& Vidal (2017), Zhou et al. (2017), Dai et al. (2017), Cui et al. (2017), Yu et al. (2017), Liu
et al. (2017), Xu et al. (2017), Xie et al. (2017), Gao et al. (2017), Zhang et al. (2017), Mi et al.
(2017), Sun et al. (2017), Zhang et al. (2017), Dixit et al. (2017), Yang et al. (2017), Branson
et al. (2017), Anne Hendricks et al. (2016), Mao et al. (2016), Yang et al. (2016), Noh et al.
(2016), Andreas et al. (2016), Reed et al. (2016), Akata et al. (2016), Xian et al. (2016), Kwitt
et al. (2016), Gan et al. (2016), Vondrick et al. (2016), Moo Yi et al. (2016), Zhou et al. (2016),
Wang et al. (2016), Hamid Rezatofighi et al. (2016), Zhu et al. (2016), Feng et al. (2016), Liu
et al. (2016), Maire et al. (2016), Khoreva et al. (2016), Yang et al. (2016), Wu et al. (2016), Ofir
et al. (2016), Shen et al. (2016), Liu et al. (2016), Fu et al. (2016), Zhang et al. (2016), Singh
et al. (2016), Xie et al. (2016), Yang et al. (2016), Laptev et al. (2016), Simo-Serra & Ishikawa
(2016), Quan et al. (2016), Gao et al. (2016), Yang et al. (2016), Ravindran & Mittal (2016), Hu
& Lin (2016), Chen et al. (2016), Chen et al. (2016), Xu et al. (2016), Gurari et al. (2016), Kihara
et al. (2016), Royer et al. (2016), Erdil et al. (2016), Zhu et al. (2016), Park et al. (2016), Lotan &
Irani (2016), Kulkarni et al. (2016), Pan et al. (2016), Cheng et al. (2016), Li & Yu (2016), Baek
et al. (2016), Mai et al. (2016), Chen et al. (2016), Bruce et al. (2016), Wloka & Tsotsos (2016),
Wang et al. (2016), Volokitin et al. (2016), Frigo et al. (2016), Calvet et al. (2016), Herranz et al.
(2016), Rhinehart & Kitani (2016), Zhang et al. (2016), Pan et al. (2016), Najafi et al. (2016),
Dasgupta et al. (2016), Zhu et al. (2016), Liang et al. (2016), Lu et al. (2016), Liu et al. (2016),
Lee et al. (2016), Zhang et al. (2016), Liu et al. (2016), Quan et al. (2016), Jang et al. (2016),
Moo Yi et al. (2016), Del Pero et al. (2016), Perazzi et al. (2016), Hasan et al. (2016), Maerki
et al. (2016), Zhang et al. (2016), Shrivastava et al. (2016), He et al. (2016), Redmon et al. (2016),
Gidaris & Komodakis (2016), Yu et al. (2016), Song & Xiao (2016), Kang et al. (2016), Hoffman
et al. (2016), Chavali et al. (2016), Kong et al. (2016), Papadopoulos et al. (2016), Ouyang et al.
(2016), Rosman et al. (2016), Bardow et al. (2016), Kadambi et al. (2016), Chen et al. (2016),
Bouman et al. (2016), Gan et al. (2016), Xiao & Jae Lee (2016), Zhu et al. (2016), Yu et al. (2016),
Alahi et al. (2016), Maksai et al. (2016), Yao et al. (2016), Tekin et al. (2016), Gygli et al. (2016),
Shahroudy et al. (2016), Ni et al. (2016), Pan et al. (2016), Meng et al. (2016), Shou et al. (2016),
Zhang et al. (2016), Jun Koh et al. (2016), Sultani & Shah (2016), Zhang et al. (2016), Liu et al.
(2016), Zhang et al. (2016), Zhang et al. (2016), Zhou et al. (2016), Zhang et al. (2016), Zhang
et al. (2016), Cui et al. (2016), Wang et al. (2016), Huang et al. (2016), Lin et al. (2016), Son
et al. (2016), Zhang et al. (2016), Kobayashi (2016), Dar & Moses (2016), Haque et al. (2016),
Zhang et al. (2016), Xiao & Jae Lee (2016), Zhang et al. (2016), Chen et al. (2016), Zhang et al.
(2016), Wang et al. (2016), Li & Yu (2016), Peng et al. (2016), Cao et al. (2016), McLaughlin
et al. (2016), Cheng et al. (2016), You et al. (2016), Cho & Yoon (2016), Matsukawa et al. (2016),
Wang et al. (2016), Perez-Rua et al. (2016), Hong Yoon et al. (2016), Bertinetto et al. (2016),
Yang et al. (2016), Tao et al. (2016), Danelljan et al. (2016), Bibi et al. (2016), Cui et al. (2016),
Diego & Hamprecht (2016), Lapin et al. (2016), Zantedeschi et al. (2016), Zhang et al. (2016),
Motiian et al. (2016), Rahmani & Mian (2016), Fouhey et al. (2016), Ren & Sudderth (2016),
Armeni et al. (2016), Wei et al. (2016), DeGol et al. (2016), Bendale & Boult (2016), Wang et al.
(2016), Sattler et al. (2016), Wolff et al. (2016), Pramod & Arun (2016), Hackel et al. (2016), Li
& Yu (2016), Pan et al. (2016), Kim et al. (2016), Kim et al. (2016), Nguyen & Brown (2016),
Ma et al. (2016), Berman et al. (2016), Nam et al. (2016), Xie et al. (2016), Lai et al. (2016), Vo
et al. (2016), Chhatkuli et al. (2016), Fredriksson et al. (2016), Huang et al. (2016), Diebold et al.
(2016), Eriksson et al. (2016), Joo et al. (2016), Han et al. (2016), Luo et al. (2016), Zheng &
Kneip (2016), Kukelova et al. (2016), Talker et al. (2016), Danelljan et al. (2016), Gong et al.
(2016), Perez-Pellitero et al. (2016), Gast et al. (2016), Hu & Lin (2016), Timofte et al. (2016),
Shi et al. (2016), Chang et al. (2016), Ma et al. (2016), Zhou et al. (2016), Caba Heilbron et al.
(2016), Fernando et al. (2016), Feichtenhofer et al. (2016), Ma et al. (2016), Li & Yu (2016),
Singh et al. (2016), Ibrahim et al. (2016), Lillo et al. (2016), Zhu et al. (2016), Ong & Bober
(2016), Heo et al. (2016), Wang et al. (2016), Wieschollek et al. (2016), Zhang et al. (2016),
Quynh Nhi Tran et al. (2016), Babenko & Lempitsky (2016), Liu et al. (2016), Iscen et al. (2016),
Kontogianni et al. (2016), Huang et al. (2016), Kuzborskij et al. (2016), Zhu et al. (2016), Tang
13

133

Published as a conference paper at SIGBOVIK 2020

et al. (2016), Yang et al. (2016), Wang et al. (2016), Chen et al. (2016), Tudor Ionescu et al.
(2016), Liu et al. (2016), Krafka et al. (2016), Lahner et al. (2016), Sharmanska et al. (2016),
Mottaghi et al. (2016), Shankar et al. (2016), Borji et al. (2016), Lee et al. (2016), Murthy et al.
(2016), Qiao et al. (2016), Li & Yu (2016), Qi (2016), Lin et al. (2016), Wang et al. (2016),
Wang et al. (2016), Poznanski & Wolf (2016), Gupta et al. (2016), Stewart et al. (2016), Tu et al.
(2016), Feng et al. (2016), Lu et al. (2016), Daniel Costea & Nedevschi (2016), Najibi et al.
(2016), Wang et al. (2016), Thies et al. (2016), Tulyakov et al. (2016), Owens et al. (2016), Gatys
et al. (2016), Hou et al. (2016), Isack et al. (2016), Kim et al. (2016), Choe et al. (2016), Wug Oh
et al. (2016), Lee et al. (2016), Li & Yu (2016), Chen et al. (2016), Shin et al. (2016), Le et al.
(2016), Jae Hwang et al. (2016), Shin et al. (2016), Pathak et al. (2016), Lei et al. (2016), Lebedev
& Lempitsky (2016), Hayder et al. (2016), Moosavi-Dezfooli et al. (2016), Murdock et al. (2016),
Iandola et al. (2016), Rastegar et al. (2016), Jacobsen et al. (2016), Singh et al. (2016), Yonetani
et al. (2016), Wang et al. (2016), Soomro et al. (2016), Wang et al. (2016), Yoo et al. (2016),
Yeung et al. (2016), Alfaro et al. (2016), Wang et al. (2016), Wang et al. (2016), Zhang et al.
(2016), Lee et al. (2016), Li & Yu (2016), Shibata et al. (2016), Wang et al. (2016), Wang et al.
(2016), Rengarajan et al. (2016), Fu et al. (2016), Lin et al. (2016), Pan et al. (2016), Zhang et al.
(2016), Szegedy et al. (2016), Gupta et al. (2016), Pham et al. (2016), Bilen & Vedaldi (2016),
Chan et al. (2016), Dutt Jain & Grauman (2016), Bell et al. (2016), Cheng et al. (2016), Mathe
et al. (2016), Huberman & Fattal (2016), Lapuschkin et al. (2016), Zhou et al. (2016), Misra et al.
(2016), Castrejon et al. (2016), Cai et al. (2016), Hu & Lin (2016), Zhu et al. (2016), Li & Yu
(2016), Wei et al. (2016), Vondrick et al. (2016), Sochor et al. (2016), Liu et al. (2016), Huang
et al. (2016), Bilen & Vedaldi (2016), Ramanathan et al. (2016), Mahasseni & Todorovic (2016),
Charles et al. (2016), Yang et al. (2016), Xu et al. (2016), Yuan et al. (2016), Ohnishi et al. (2016),
Wu et al. (2016), Choi et al. (2016), Richard & Gall (2016), Liu et al. (2016), Dai et al. (2016),
Lin et al. (2016), Kundu et al. (2016), Blaha et al. (2016), Liang et al. (2016), Lin et al. (2016),
Hong et al. (2016), Cordts et al. (2016), Vemulapalli et al. (2016), Ros et al. (2016), Locher et al.
(2016), Kanazawa et al. (2016), Johannsen et al. (2016), Wang et al. (2016), Osman Ulusoy et al.
(2016), Schillebeeckx & Pless (2016), Thomas & Taniguchi (2016), Xie et al. (2016), Magri &
Fusiello (2016), Verleysen & De Vleeschouwer (2016), Saurer et al. (2016), Trager et al. (2016),
Albl et al. (2016), Brachmann et al. (2016), Bushnevskiy et al. (2016), Zafeiriou et al. (2016),
Zhao et al. (2016), Wu et al. (2016), Zhu et al. (2016), Piotraschke & Blanz (2016), Zhang et al.
(2016), Zhang et al. (2016), Yu et al. (2016), Qin et al. (2016), Zhao et al. (2016), Ham et al.
(2016), Sun et al. (2016), Thomas & Taniguchi (2016), Su et al. (2016), Li & Yu (2016), Mottaghi
et al. (2016), Harakeh et al. (2016), Altwaijry et al. (2016), Singh et al. (2016), Diba et al. (2016),
Cho & Yoon (2016), Hu & Lin (2016), Doumanoglou et al. (2016), Ge et al. (2016), Bertasius
et al. (2016), Mattyus et al. (2016), Shuai et al. (2016), Lai et al. (2016), Chen et al. (2016),
Souly & Shah (2016), Li & Yu (2016), Kuen et al. (2016), Seguin et al. (2016), Xie et al. (2016),
Kolaman et al. (2016), Shi et al. (2016), Wang et al. (2016), Fu et al. (2016), Chang et al. (2016),
Heber & Pock (2016), Aggarwal et al. (2016), Sheinin & Schechner (2016), Kobayashi (2016),
Yago Vicente et al. (2016), Koller et al. (2016), Li & Yu (2016), Johns et al. (2016), Zhu et al.
(2016), Park et al. (2016), Ji et al. (2016), Jayaraman & Grauman (2016), Huang et al. (2016),
Yu et al. (2016), Zhang et al. (2016), Sevilla-Lara et al. (2016), Tsai et al. (2016), Law et al.
(2016), You et al. (2016), You et al. (2016), Huang et al. (2016), Mollenhoff et al. (2016), Littwin
& Wolf (2016), Sharmanska et al. (2016), Chakraborty et al. (2016), Rota Bulo & Kontschieder
(2016), Misra et al. (2016), Song & Xiao (2016), Lavin & Gray (2016), Li & Yu (2016), Zhang
et al. (2016), Mayer et al. (2016), Feng et al. (2016), Ranftl et al. (2016), Mostegel et al. (2016),
Handa et al. (2016), Jeon et al. (2016), Ben-Artzi et al. (2016), Schonberger & Frahm (2016),
Wang et al. (2016), Kong et al. (2016), Dai et al. (2016), Crocco et al. (2016), Sinha et al. (2016),
Zhang et al. (2016), Shi et al. (2016), Trigeorgis et al. (2016), Jourabloo & Liu (2016), Roth
et al. (2016), Molchanov et al. (2016), Chang et al. (2016), Bhattarai et al. (2016), Gadot & Wolf
(2016), Taniai et al. (2016), Xu et al. (2016), Ning et al. (2016), Sekii (2016), Hoshen & Peleg
(2016), Nam et al. (2016), Qi (2016), Liu et al. (2016), Choi et al. (2016), Dhiman et al. (2016),
Gaidon et al. (2016), Midorikawa et al. (2016), Queau et al. (2016), Qian et al. (2016), Or-El et al.
(2016), Tanaka et al. (2016), Williem & Kyu Park (2016), Li & Yu (2016), Natola et al. (2016),
Banerjee et al. (2016), Wang et al. (2016), Karianakis et al. (2016), Jampani et al. (2016), Ju et al.
(2016), Vemulapalli et al. (2016), Zheng & Kneip (2016), Xing et al. (2016), Zhang et al. (2016),
Rematas et al. (2016), Yang et al. (2016), Rahimi et al. (2016), Stumm et al. (2016), Chen et al.
(2016), Hu & Lin (2016), Johnson et al. (2016), Alayrac et al. (2016), Yu et al. (2016), Pan et al.
(2016), Chandrasekaran et al. (2016), Shih et al. (2016), Wu et al. (2016), Tapaswi et al. (2016),
14

134

Published as a conference paper at SIGBOVIK 2020

Li & Yu (2016), You et al. (2016), Mustafa et al. (2016), Lee et al. (2016), Parashar et al. (2016),
Chen et al. (2016), Park et al. (2016), Chen et al. (2016), Chu et al. (2016), Wei et al. (2016),
Carreira et al. (2016), Durand et al. (2016), Xie et al. (2016), Smith et al. (2016), Deng et al.
(2016), Cohen et al. (2016), Wang et al. (2016), Vemulapalli et al. (2016), Wang et al. (2016), Wu
et al. (2016), Dosovitskiy & Brox (2016), Masi et al. (2016), Kan et al. (2016), Sun et al. (2016),
Feng et al. (2016), Kemelmacher-Shlizerman et al. (2016), Arandjelovic (2016), Wen et al. (2016),
Walecki et al. (2016), Bolkart & Wuhrer (2016), Niu et al. (2016), Pishchulin et al. (2016), Kwak
et al. (2016), Yasin et al. (2016), Oberweger et al. (2016), Zhou et al. (2016), Kafle & Kanan
(2016), Kottur et al. (2016), Zhu et al. (2016), Wang et al. (2016), Zhang et al. (2016), Bai et al.
(2016), Zhang et al. (2016), Demisse et al. (2016), Shi et al. (2016), Shi et al. (2016), Yang et al.
(2016), Tsai et al. (2016), Marcos et al. (2016), Wu et al. (2016), Trigeorgis et al. (2016), Liu
et al. (2016), Canevet & Fleuret (2016), Kanehira & Harada (2016), Yang et al. (2016), Yin et al.
(2016), Funk & Liu (2016), Huang et al. (2016), Harandi et al. (2016), Quang Minh et al. (2016),
Cheng et al. (2016), Chen et al. (2016), Mukuta & Harada (2016), Mosinska-Domanska et al.
(2016), Alameda-Pineda et al. (2016), Lu et al. (2016), Kolouri et al. (2016), Wei et al. (2016),
Cholakkal et al. (2016), Xu et al. (2016), Arandjelovic (2016), Dutt Jain & Grauman (2016), Kim
et al. (2016), Changpinyo et al. (2016), Fu et al. (2016), Li & Yu (2016), Xu et al. (2016), Shanu
et al. (2016), Huang et al. (2016), Kumar B G et al. (2016), Koniusz & Cherian (2016), Chang
et al. (2016), Ha et al. (2016), Yang et al. (2016), Firman et al. (2016), Ryan Fanello et al. (2016),
Wang et al. (2016), Savinov et al. (2016), Raposo & Barreto (2016), Galliani & Schindler (2016),
Radenovic et al. (2016), Eckart et al. (2016), Roy & Todorovic (2016), Flynn et al. (2016), Yang
et al. (2016), Yatskar et al. (2016), Booth et al. (2016), Rothe et al. (2016), Fabian Benitez-Quiroz
et al. (2016), Ouyang et al. (2016), Sikka et al. (2016), Pal et al. (2016), Hu & Lin (2016),
Joseph Tan et al. (2016), Shao et al. (2016), Bernard et al. (2016), Vongkulbhisal et al. (2016),
Qi (2016), Zhai et al. (2016), Li & Yu (2016), Nguyen & Brown (2016), Campbell & Petersson
(2016), Luo et al. (2016), Hu & Lin (2016), Harwood & Drummond (2016), He et al. (2016),
Zhang et al. (2016), Honari et al. (2016), Jetley et al. (2016), Fan et al. (2016), Nhan Duong et al.
(2016), Kruthiventi et al. (2016), Zhou et al. (2016), Golyanik et al. (2016), Wang et al. (2016),
Oskarsson et al. (2016), Wang et al. (2016), Quan et al. (2016), Baque et al. (2016), Chin et al.
(2016), Ajanthan et al. (2016), Joseph Tan et al. (2016), Bylow et al. (2016), Dicle et al. (2016),
Shah et al. (2016), Qi (2016), Jug et al. (2016), Nasihatkon et al. (2016), Yang et al. (2016),
Zhuang et al. (2016), Bansal et al. (2016), Al-Halah et al. (2016), Zhang et al. (2016), Wang et al.
(2016), Wang et al. (2016), Wegner et al. (2016), Massa et al. (2016), Zhang et al. (2016), Yang
et al. (2016),

R EFERENCES
Macario O. Cordel , II, Shaojing Fan, Zhiqi Shen, and Mohan S. Kankanhalli. Emotion-aware
human attention prediction. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Nayyer Aafaq, Naveed Akhtar, Wei Liu, Syed Zulqarnain Gilani, and Ajmal Mian. Spatio-temporal
dynamics and semantic attribute enriched visual encoding for video captioning. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Sathyanarayanan N. Aakur and Sudeep Sarkar. A perceptual prediction framework for self supervised event segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Davide Abati, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Latent space autoregression
for novelty detection. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Mahdi Abavisani, Hamid Reza Vaezi Joze, and Vishal M. Patel. Improving the performance of
unimodal dynamic hand-gesture recognition with multimodal training. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Ehsan Abbasnejad, Qi Wu, Qinfeng Shi, and Anton van den Hengel. What’s to know? uncertainty
as a guide to asking goal-oriented questions. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
15

135

Published as a conference paper at SIGBOVIK 2020

Mahdi Abbaspour Tehrani, Thabo Beeler, and Anselm Grundhofer. A practical method for fully
automatic intrinsic camera calibration using directionally encoded light. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Abdelrahman Abdelhamed, Stephen Lin, and Michael S. Brown. A high-quality denoising dataset
for smartphone cameras. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Muhammad Abdullah Jamal, Haoxiang Li, and Boqing Gong. Deep face detector adaptation without
negative transfer or catastrophic forgetting. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Abrar H. Abdulnabi, Bing Shuai, Stefan Winkler, and Gang Wang. Episodic camn: Contextual
attention-based memory networks with iterative feedback for scene labeling. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Pooya Abolghasemi, Amir Mazaheri, Mubarak Shah, and Ladislau Boloni. Pay attention! - robustifying a deep visuomotor policy through task-focused visual attention. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yazan Abu Farha, Alexander Richard, and Juergen Gall. When will you do what? - anticipating
temporal occurrences of activities. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Radhakrishna Achanta and Sabine Susstrunk. Superpixels and polygons using simple non-iterative
clustering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
David Acuna, Huan Ling, Amlan Kar, and Sanja Fidler. Efficient interactive annotation of segmentation datasets with polygon-rnn++. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
David Acuna, Amlan Kar, and Sanja Fidler. Devil is in the edges: Learning semantic boundaries
from noisy annotations. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Sarah Adel Bargal, Andrea Zunino, Donghyun Kim, Jianming Zhang, Vittorio Murino, and Stan
Sclaroff. Excitation backprop for rnns. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Mahmoud Afifi, Brian Price, Scott Cohen, and Michael S. Brown. When color constancy goes
wrong: Correcting improperly white-balanced images. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Rajat Aggarwal, Amrisha Vohra, and Anoop M. Namboodiri. Panoramic stereo videos with a single
camera. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Don’t just assume;
look and answer: Overcoming priors for visual question answering. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Gianluca Agresti, Henrik Schaefer, Piergiorgio Sartor, and Pietro Zanuttigh. Unsupervised domain
adaptation for tof data denoising with adversarial learning. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Antonio Agudo and Francesc Moreno-Noguer. Dust: Dual union of spatio-temporal subspaces for
monocular multiple object 3d reconstruction. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Antonio Agudo, Melcior Pijoan, and Francesc Moreno-Noguer. Image collection pop-up: 3d reconstruction and clustering of rigid and non-rigid categories. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
16

136

Published as a conference paper at SIGBOVIK 2020

Eirikur Agustsson, Jasper R. R. Uijlings, and Vittorio Ferrari. Interactive full image segmentation
by considering all regions jointly. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level supervision
for weakly supervised semantic segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance segmentation
with inter-pixel relations. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Thalaiyasingam Ajanthan, Richard Hartley, and Mathieu Salzmann. Memory efficient max flow for
multi-label submodular mrfs. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Thalaiyasingam Ajanthan, Alban Desmaison, Rudy Bunel, Mathieu Salzmann, Philip H. S. Torr,
and M. Pawan Kumar. Efficient linear programming for dense crfs. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Kenan E. Ak, Ashraf A. Kassim, Joo Hwee Lim, and Jo Yew Tham. Learning attribute representations with localization for flexible fashion search. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Zeynep Akata, Mateusz Malinowski, Mario Fritz, and Bernt Schiele. Multi-cue zero-shot learning
with strong supervision. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl Vondrick, and Shih-Fu
Chang. Multi-level multimodal common semantic space for image-phrase grounding. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Naveed Akhtar, Ajmal Mian, and Fatih Porikli. Joint discriminative bayesian dictionary and classifier learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Naveed Akhtar, Jian Liu, and Ajmal Mian. Defense against universal adversarial perturbations. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Derya Akkaynak and Tali Treibitz. A revised underwater image formation model. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Derya Akkaynak and Tali Treibitz. Sea-thru: A method for removing water from underwater images.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Derya Akkaynak, Tali Treibitz, Tom Shlesinger, Yossi Loya, Raz Tamir, and David Iluz. What is
the space of attenuation coefficients in underwater computer vision? In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Yagiz Aksoy, Tunc Ozan Aydin, and Marc Pollefeys. Designing effective inter-pixel information
flow for natural image matting. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Ziad Al-Halah and Rainer Stiefelhagen. Automatic discovery, association estimation and learning
of semantic attributes for a thousand categories. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Ziad Al-Halah, Makarand Tapaswi, and Rainer Stiefelhagen. Recovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio
Savarese. Social lstm: Human trajectory prediction in crowded spaces. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
17

137

Published as a conference paper at SIGBOVIK 2020

Xavier Alameda-Pineda, Elisa Ricci, Yan Yan, and Nicu Sebe. Recognizing emotions from abstract
paintings using non-linear matrix completion. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Xavier Alameda-Pineda, Andrea Pilzer, Dan Xu, Nicu Sebe, and Elisa Ricci. Viraliency: Pooling
local virality. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra,
Tim K. Marks, Chiori Hori, Peter Anderson, Stefan Lee, and Devi Parikh. Audio visual sceneaware dialog. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon
Lacoste-Julien. Unsupervised learning from narrated instruction videos. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Jean-Baptiste Alayrac, Joao Carreira, and Andrew Zisserman. The visual centrifuge: Model-free
layered video representations. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Cenek Albl, Zuzana Kukelova, and Tomas Pajdla. Rolling shutter absolute pose problem with known
vertical direction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Cenek Albl, Zuzana Kukelova, Andrew Fitzgibbon, Jan Heller, Matej Smid, and Tomas Pajdla. On
the two-view geometry of unsynchronized cameras. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Michael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh
Nguyen. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar
objects. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Anali Alfaro, Domingo Mery, and Alvaro Soto. Action recognition in video using sparse coding and
relative features. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Amit Alfassy, Leonid Karlinsky, Amit Aides, Joseph Shtok, Sivan Harary, Rogerio Feris, Raja
Giryes, and Alex M. Bronstein. Laso: Label-set operations networks for multi-label few-shot
learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Yazeed Alharbi, Neil Smith, and Peter Wonka. Latent filter scaling for multimodal unsupervised
image-to-image translation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
S. Alireza Golestaneh and Lina J. Karam. Spatially-varying blur detection based on multiscale fused
and sorted transform coefficients of gradient magnitudes. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Raied Aljadaany, Dipan K. Pal, and Marios Savvides. Douglas-rachford networks: Learning both
the image prior and data fidelity terms for blind image deconvolution. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with
a network of experts. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
18

138

Published as a conference paper at SIGBOVIK 2020

Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-Moll. Video
based reconstruction of 3d people models. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, and Gerard PonsMoll. Learning to reconstruct people in clothing from a single rgb camera. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Emilio J. Almazan, Ron Tal, Yiming Qian, and James H. Elder. Mcmlsd: A dynamic programming
approach to line segment detection. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Riza Alp Guler, George Trigeorgis, Epameinondas Antonakos, Patrick Snape, Stefanos Zafeiriou,
and Iasonas Kokkinos. Densereg: Fully convolutional dense shape regression in-the-wild. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Anna Alperovich, Ole Johannsen, Michael Strecke, and Bastian Goldluecke. Light field intrinsics
with a deep encoder-decoder network. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Hani Altwaijry, Eduard Trulls, James Hays, Pascal Fua, and Serge Belongie. Learning to match
aerial images with deep attentive architectures. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Paul Amayo, Pedro Piniés, Lina M. Paz, and Paul Newman. Geometric multi-model fitting with a
convex relaxation algorithm. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jeffrey McKinstry, Carmelo Di Nolfo,
Tapan Nayak, Alexander Andreopoulos, Guillaume Garreau, Marcela Mendoza, Jeff Kusnitz,
Michael Debole, Steve Esser, Tobi Delbruck, Myron Flickner, and Dharmendra Modha. A low
power, fully event-based gesture recognition system. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Md Amirul Islam, Mrigank Rochan, Neil D. B. Bruce, and Yang Wang. Gated feedback refinement
network for dense image labeling. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Md Amirul Islam, Mahmoud Kalash, and Neil D. B. Bruce. Revisiting salient object detection: Simultaneous detection, ranking, and subitizing of multiple salient objects. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Matthew Amodio and Smita Krishnaswamy. Travelgan: Image-to-image translation by transformation vector learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Wangpeng An, Haoqian Wang, Qingyun Sun, Jun Xu, Qionghai Dai, and Lei Zhang. A pid controller
approach for stochastic optimization of deep networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid,
Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting
visually-grounded navigation instructions in real environments. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Alexander Andreopoulos, Hirak J. Kashyap, Tapan K. Nayak, Arnon Amir, and Myron D. Flickner.
A low power, high throughput, fully event-based stereo system. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
19

139

Published as a conference paper at SIGBOVIK 2020

Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen
Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jyoti Aneja, Aditya Deshpande, and Alexander G. Schwing. Convolutional image captioning. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Mikaela Angelina Uy and Gim Hee Lee. Pointnetvlad: Deep point cloud based retrieval for largescale place recognition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Rushil Anirudh, Hyojin Kim, Jayaraman J. Thiagarajan, K. Aditya Mohan, Kyle Champley, and
Timo Bremer. Lose the views: Limited angle ct reconstruction via implicit sinogram completion.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Yashas Annadani and Soma Biswas. Preserving semantic relations for zero-shot learning. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko,
and Trevor Darrell. Deep compositional captioning: Describing novel object categories without paired training data. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Michel Antunes, Joao P. Barreto, Djamila Aouada, and Bjorn Ottersten. Unsupervised vanishing
point detection and camera calibration from a single manhattan image with radial distortion. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yasuhiro Aoki, Hunter Goforth, Rangaprasad Arun Srivatsan, and Simon Lucey. Pointnetlk: Robust
efficient point cloud registration using pointnet. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Ognjen Arandjelovic. Learnt quasi-transitive similarity for retrieval from large collections of faces.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Nikita Araslanov, Constantin A. Rothkopf, and Stefan Roth. Actor-critic instance segmentation. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Anil Armagan, Martin Hirzer, Peter M. Roth, and Vincent Lepetit. Learning to align semantic
segmentation and 2.5d maps for geolocalization. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio
Savarese. 3d semantic parsing of large-scale indoor spaces. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Anurag Arnab and Philip H. S. Torr. Pixelwise instance segmentation with a dynamically instantiated
network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Anurag Arnab, Ondrej Miksik, and Philip H.S. Torr. On the robustness of semantic segmentation
models to adversarial attacks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Anurag Arnab, Carl Doersch, and Andrew Zisserman. Exploiting temporal context for 3d human
pose estimation in the wild. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Amir Arsalan Soltani, Haibin Huang, Jiajun Wu, Tejas D. Kulkarni, and Joshua B. Tenenbaum.
Synthesizing 3d shapes via modeling multi-view depth maps and silhouettes with deep generative
networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
20

140

Published as a conference paper at SIGBOVIK 2020

Aditya Arun, C.V. Jawahar, and M. Pawan Kumar. Dissimilarity coefficient based weakly supervised
object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Nikolaos Arvanitopoulos, Radhakrishna Achanta, and Sabine Susstrunk. Single image reflection
suppression. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Amir Atapour-Abarghouei and Toby P. Breckon. Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Amir Atapour-Abarghouei and Toby P. Breckon. Veritatem dies aperit - temporally consistent depth
prediction enabled by a multi-task geometric and semantic scene understanding approach. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yuval Atzmon and Gal Chechik. Adaptive confidence smoothing for generalized zero-shot learning.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X. Chang, and Matthias
Niessner. Scan2cad: Learning cad model alignment in rgb-d scans. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Gil Avraham, Yan Zuo, and Tom Drummond. Parallel optimal transport gan. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Samaneh Azadi, Jiashi Feng, and Trevor Darrell. Learning detection with diverse proposals. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Samaneh Azadi, Matthew Fisher, Vladimir G. Kim, Zhaowen Wang, Eli Shechtman, and Trevor
Darrell. Multi-content gan for few-shot font style transfer. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Sina Mokhtarzadeh Azar, Mina Ghadimi Atigh, Ahmad Nickabadi, and Alexandre Alahi. Convolutional relational machine for group activity recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Nicolas Aziere and Sinisa Todorovic. Ensemble deep manifold similarity learning using hard proxies. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Dejan Azinovic, Tzu-Mao Li, Anton Kaplanyan, and Matthias Niessner. Inverse path tracing for
joint material and lighting estimation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Artem Babenko and Victor Lempitsky. Efficient indexing of billion-scale datasets of deep descriptors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Artem Babenko and Victor Lempitsky. Product split trees. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Deepak Babu Sam, Shiv Surya, and R. Venkatesh Babu. Switching convolutional neural network for
crowd counting. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Deepak Babu Sam, Neeraj N. Sajjan, R. Venkatesh Babu, and Mukundhan Srinivasan. Divide and
grow: Capturing huge diversity in crowd images with incrementally growing cnn. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Seung-Hwan Baek, Inchang Choi, and Min H. Kim. Multiview image completion with space structure propagation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Augmented skeleton space transfer for depthbased hand pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
21

141

Published as a conference paper at SIGBOVIK 2020

Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Pushing the envelope for rgb-based dense
3d hand pose estimation via neural rendering. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Timur Bagautdinov, Alexandre Alahi, Francois Fleuret, Pascal Fua, and Silvio Savarese. Social
scene understanding: End-to-end multi-person action localization and collective activity recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Timur Bagautdinov, Chenglei Wu, Jason Saragih, Pascal Fua, and Yaser Sheikh. Modeling facial
geometry using compositional vaes. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Hessam Bagherinezhad, Mohammad Rastegari, and Ali Farhadi. Lcnn: Lookup-based convolutional
neural network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan Latecki. Gift: A real-time
and scalable 3d shape search engine. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Song Bai, Xiang Bai, and Qi Tian. Scalable person re-identification on supervised smoothed manifold. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Song Bai, Peng Tang, Philip H.S. Torr, and Longin Jan Latecki. Re-ranking via metric fusion for
object retrieval and person re-identification. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Yancheng Bai, Yongqiang Zhang, Mingli Ding, and Bernard Ghanem. Finding tiny faces in the wild
with generative adversarial network. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Christian Bailer, Kiran Varanasi, and Didier Stricker. Cnn-based patch matching for optical flow
with thresholded hinge embedding loss. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Slawomir Bak and Peter Carr. One-shot metric learning for person re-identification. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Arun Balajee Vasudevan, Dengxin Dai, and Luc Van Gool. Object referring in videos with language
and human gaze. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Guha Balakrishnan, Amy Zhao, Adrian V. Dalca, Frédo Durand, and John Guttag. Synthesizing
images of humans in unseen poses. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krystian Mikolajczyk. Hpatches: A benchmark
and evaluation of handcrafted and learned local descriptors. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Monami Banerjee, Rudrasis Chakraborty, Edward Ofori, Michael S. Okun, David E. Viallancourt,
and Baba C. Vemuri. A nonlinear regression technique for manifold valued data with applications
to medical image analysis. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Aayush Bansal, Bryan Russell, and Abhinav Gupta. Marr revisited: 2d-3d alignment via surface
normal prediction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Aayush Bansal, Yaser Sheikh, and Deva Ramanan. Shapes and context: In-the-wild image synthesis
manipulation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
22

142

Published as a conference paper at SIGBOVIK 2020

Linchao Bao, Baoyuan Wu, and Wei Liu. Cnn in mrf: Video object segmentation via inference in
a cnn-based higher-order spatio-temporal mrf. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang.
Depth-aware video frame interpolation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Akash Bapat and Jan-Michael Frahm. The domain transform solver. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Akash Bapat, True Price, and Jan-Michael Frahm. Rolling shutter and radial distortion are features
for high frame rate multi-camera tracking. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Jawadul H. Bappy, Sujoy Paul, Ertem Tuncel, and Amit K. Roy-Chowdhury. The impact of typicality
for informative representative selection. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Pierre Baque, Timur Bagautdinov, Francois Fleuret, and Pascal Fua. Principled parallel mean-field
inference for discrete random fields. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Pierre Baque, Francois Fleuret, and Pascal Fua. Multi-modal mean-fields via cardinality-based
clamping. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Manel Baradad, Vickie Ye, Adam B. Yedidia, Frédo Durand, William T. Freeman, Gregory W.
Wornell, and Antonio Torralba. Inferring light fields from shadows. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Fabien Baradel, Christian Wolf, Julien Mille, and Graham W. Taylor. Glimpse clouds: Human
activity recognition from unstructured feature points. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Lorenzo Baraldi, Costantino Grana, and Rita Cucchiara. Hierarchical boundary-aware neural encoder for video captioning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Lorenzo Baraldi, Matthijs Douze, Rita Cucchiara, and Hervé Jégou. Lamv: Learning to align and
match videos with kernelized temporal layers. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Daniel Barath. Five-point fundamental matrix estimation for uncalibrated cameras. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Daniel Barath, Tekla Toth, and Levente Hajder. A minimal solution for two-view focal-length estimation using two affine correspondences. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Daniel Barath, Jiri Matas, and Jana Noskova. Magsac: Marginalizing sample consensus. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Patrick Bardow, Andrew J. Davison, and Stefan Leutenegger. Simultaneous optical flow and intensity estimation from an event camera. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Ehud Barnea and Ohad Ben-Shahar. Curve reconstruction via the global statistics of natural curves.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Ehud Barnea and Ohad Ben-Shahar. Exploring the bounds of the utility of context for object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
23

143

Published as a conference paper at SIGBOVIK 2020

Jonathan T. Barron. A general and adaptive robust loss function. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Jonathan T. Barron and Yun-Ta Tsai. Fast fourier color constancy. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Anil S. Baslamisli, Hoang-An Le, and Theo Gevers. Cnn based learning using reflection and retinex
models for intrinsic image decomposition. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Favyen Bastani, Songtao He, Sofiane Abbar, Mohammad Alizadeh, Hari Balakrishnan, Sanjay
Chawla, Sam Madden, and David DeWitt. Roadtracer: Automatic extraction of road networks
from aerial images. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Anil Batra, Suriya Singh, Guan Pang, Saikat Basu, C.V. Jawahar, and Manohar Paluri. Improved
road connectivity by joint learning of orientation and segmentation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Konstantinos Batsos, Changjiang Cai, and Philippos Mordohai. Cbmv: A coalesced bidirectional
matching volume for disparity estimation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection:
Quantifying interpretability of deep visual representations. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Jean-Philippe Bauchet and Florent Lafarge. Kippi: Kinetic polygonal partitioning of images. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Christian F. Baumgartner, Lisa M. Koch, Kerem Can Tezcan, Jia Xi Ang, and Ender Konukoglu.
Visual feature attribution using wasserstein gans. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Miguel A. Bautista, Artsiom Sanakoyeu, and Bjorn Ommer. Deep unsupervised similarity learning
using partially ordered sets. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Aseem Behl, Despoina Paschalidou, Simon Donne, and Andreas Geiger. Pointflownet: Learning
representations for rigid motion estimation from point clouds. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Sean Bell, C. Lawrence Zitnick, Kavita Bala, and Ross Girshick. Inside-outside net: Detecting
objects in context with skip pooling and recurrent neural networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
William H. Beluch, Tim Genewein, Andreas Nürnberger, and Jan M. Köhler. The power of ensembles for active learning in image classification. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Gil Ben-Artzi, Yoni Kasten, Shmuel Peleg, and Michael Werman. Camera calibration from dynamic
silhouettes using motion barcodes. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fischer. Nesti-net: Normal estimation for
unstructured 3d point clouds using convolutional neural networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Amor Ben Tanfous, Hassen Drira, and Boulbaba Ben Amor. Coding kendall’s shape trajectories
for 3d action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Abhijit Bendale and Terrance E. Boult. Towards open set deep networks. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
24

144

Published as a conference paper at SIGBOVIK 2020

Rodrigo Benenson, Stefan Popov, and Vittorio Ferrari. Large-scale interactive object segmentation
with human annotators. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad – a comprehensive
real-world dataset for unsupervised anomaly detection. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Dana Berman, Tali treibitz, and Shai Avidan. Non-local image dehazing. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Maxim Berman, Amal Rannen Triki, and Matthew B. Blaschko. The lovász-softmax loss: A
tractable surrogate for the optimization of the intersection-over-union measure in neural networks.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Florian Bernard, Peter Gemmar, Frank Hertel, Jorge Goncalves, and Johan Thunberg. Linear shape
deformation models with local support using graph-based structured matrix factorisation. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Florian Bernard, Frank R. Schmidt, Johan Thunberg, and Daniel Cremers. A combinatorial solution
to non-rigid 3d shape-to-image matching. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Florian Bernard, Christian Theobalt, and Michael Moeller. Ds*: Tighter lifting-free convex relaxations for quadratic matching problems. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. Semantic segmentation with boundary neural
fields. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Gedas Bertasius, Lorenzo Torresani, Stella X. Yu, and Jianbo Shi. Convolutional random walk
networks for semantic image segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Gedas Bertasius, Aaron Chan, and Jianbo Shi. Egocentric basketball motion planning from a single first-person image. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, and Philip H. S. Torr. Staple:
Complementary learners for real-time tracking. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Shweta Bhardwaj, Mukundhan Srinivasan, and Mitesh M. Khapra. Efficient video classification using fewer frames. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Apratim Bhattacharyya, Mario Fritz, and Bernt Schiele. Long-term on-board prediction of people
in traffic scenes under uncertainty. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Binod Bhattarai, Gaurav Sharma, and Frederic Jurie. Cp-mtml: Coupled projection multi-task metric
learning for large scale face retrieval. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Ayan Kumar Bhunia, Abhirup Das, Ankan Kumar Bhunia, Perla Sai Raj Kishore, and Partha Pratim
Roy. Handwriting recognition in low-resource scripts using adversarial learning. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
JiaWang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit Yeung, Tan-Dat Nguyen, and MingMing Cheng. Gms: Grid-based motion statistics for fast, ultra-robust feature correspondence. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Simone Bianco and Claudio Cusano. Quasi-unsupervised color constancy. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
25

145

Published as a conference paper at SIGBOVIK 2020

Adel Bibi, Tianzhu Zhang, and Bernard Ghanem. 3d part-based sparse tracker with automatic synchronization and registration. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Adel Bibi, Hani Itani, and Bernard Ghanem. Fftlasso: Large-scale lasso in the fourier domain. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Adel Bibi, Modar Alfadly, and Bernard Ghanem. Analytic expressions for probabilistic moments of
pl-dnn with gaussian input. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Pia Bideau, Aruni RoyChowdhury, Rakesh R. Menon, and Erik Learned-Miller. The best of both
worlds: Combining cnns and geometric constraints for hierarchical motion segmentation. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Hakan Bilen and Andrea Vedaldi. Weakly supervised deep detection networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Piotr Bilinski and Victor Prisacariu. Dense decoder shortcut connections for single-pass semantic
segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Tolga Birdal and Umut Simsekli. Probabilistic permutation synchronization using the riemannian
structure of the birkhoff polytope. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Tolga Birdal, Benjamin Busam, Nassir Navab, Slobodan Ilic, and Peter Sturm. A minimalist approach to type-agnostic detection of quadrics in point clouds. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Ali Furkan Biten, Lluis Gomez, Marcal Rusinol, and Dimosthenis Karatzas. Good news, everyone!
context driven entity-aware captioning for news images. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Maros Blaha, Christoph Vogel, Audrey Richard, Jan D. Wegner, Thomas Pock, and Konrad
Schindler. Large-scale semantic 3d reconstruction: An adaptive multi-resolution model for multiclass volumetric labeling. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Nathaniel Blanchard, Jeffery Kinnison, Brandon RichardWebster, Pouya Bashivan, and Walter J.
Scheirer. A neurobiological evaluation metric for neural network model search. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Henryk Blasinski, Joyce Farrell, and Brian Wandell. Designing illuminant spectral power distributions for surface classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J. Davison.
Codeslam — learning a compact, optimisable representation for dense visual slam. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Federica Bogo, Javier Romero, Gerard Pons-Moll, and Michael J. Black. Dynamic faust: Registering human bodies in motion. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Timo Bolkart and Stefanie Wuhrer. A robust multilinear model learning framework for 3d faces. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan Ponniah, and David Dunaway. A 3d
morphable model learnt from 10,000 faces. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
26

146

Published as a conference paper at SIGBOVIK 2020

James Booth, Epameinondas Antonakos, Stylianos Ploumpis, George Trigeorgis, Yannis Panagakis,
and Stefanos Zafeiriou. 3d face morphable models ”in-the-wild”. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Guido Borghi, Marco Venturelli, Roberto Vezzani, and Rita Cucchiara. Poseidon: Face-from-depth
for driver pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Ali Borji, Saeed Izadi, and Laurent Itti. ilab-20m: A large-scale controlled object dataset to investigate deep learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Adnane Boukhayma, Jean-Sebastien Franco, and Edmond Boyer. Surface motion capture transfer
with gaussian process regression. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Adnane Boukhayma, Rodrigo de Bem, and Philip H.S. Torr. 3d hand shape and pose from images
in the wild. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Katherine L. Bouman, Michael D. Johnson, Daniel Zoran, Vincent L. Fish, Sheperd S. Doeleman,
and William T. Freeman. Computational imaging for vlbi image reconstruction. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Giorgos Bouritsas, Petros Koutras, Athanasia Zlatintsi, and Petros Maragos. Multimodal visual
concept learning with weakly supervised techniques. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Katarzyna Bozek, Laetitia Hebert, Alexander S. Mikheyev, and Greg J. Stephens. Towards dense
object tracking in a 2d honeybee hive. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Lior Bracha and Gal Chechik. Informative object annotations: Tell me something i don’t know. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Eric Brachmann and Carsten Rother. Learning less is more - 6d camera localization via 3d surface
regression. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Eric Brachmann, Frank Michel, Alexander Krull, Michael Ying Yang, Stefan Gumhold, and carsten
Rother. Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan
Gumhold, and Carsten Rother. Dsac - differentiable ransac for camera localization. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Samarth Brahmbhatt and James Hays. Deepnav: Learning to navigate large cities. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Samarth Brahmbhatt, Jinwei Gu, Kihwan Kim, James Hays, and Jan Kautz. Geometry-aware learning of maps for camera localization. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Samarth Brahmbhatt, Cusuh Ham, Charles C. Kemp, and James Hays. Contactdb: Analyzing and
predicting grasp contact via thermal imaging. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
27

147

Published as a conference paper at SIGBOVIK 2020

Frederic Branchaud-Charron, Andrew Achkar, and Pierre-Marc Jodoin. Spectral metric for dataset
complexity assessment. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Steve Branson, Grant Van Horn, and Pietro Perona. Lean crowdsourcing: Combining humans and
machines in an online system. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Biagio Brattoli, Uta Buchler, Anna-Sophia Wahl, Martin E. Schwab, and Bjorn Ommer. Lstm selfsupervision for detailed behavior analysis. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Garrick Brazil and Xiaoming Liu. Pedestrian detection with autoregressive network phases. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Jesus Briales and Javier Gonzalez-Jimenez. Convex global 3d registration with lagrangian duality.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Jesus Briales, Laurent Kneip, and Javier Gonzalez-Jimenez. A certifiably globally optimal solution
to the non-minimal relative pose problem. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Tim Brooks and Jonathan T. Barron. Learning to synthesize motion blur. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Sofia Broome, Karina Bech Gleerup, Pia Haubro Andersen, and Hedvig Kjellstrom. Dynamics are
important for the recognition of equine pain in video. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Neil D. B. Bruce, Christopher Catton, and Sasa Janjic. A deeper look at saliency: Feature contrast,
semantics, and beyond. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Shyamal Buch, Victor Escorcia, Chuanqi Shen, Bernard Ghanem, and Juan Carlos Niebles. Sst:
Single-stream temporal action proposals. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Adrian Bulat and Georgios Tzimiropoulos. Super-fan: Integrated facial landmark localization and
super-resolution of real-world low resolution faces in arbitrary poses with gans. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Philippe Burlina, Neil Joshi, and I-Jeng Wang. Where’s wally now? deep generative and discriminative embeddings for novelty detection. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Andrey Bushnevskiy, Lorenzo Sorgi, and Bodo Rosenhahn. Multicamera calibration from visible
and mirrored epipoles. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Judith Butepage, Michael J. Black, Danica Kragic, and Hedvig Kjellstrom. Deep representation
learning for human motion prediction and classification. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Erik Bylow, Carl Olsson, Fredrik Kahl, and Mikael Nilsson. Minimizing the maximal rank. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard Ghanem. Fast temporal activity proposals for efficient detection of human actions in untrimmed videos. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Fabian Caba Heilbron, Wayner Barrios, Victor Escorcia, and Bernard Ghanem. Scc: Semantic
context cascade for efficient action detection. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
28

148

Published as a conference paper at SIGBOVIK 2020

Jose Caballero, Christian Ledig, Andrew Aitken, Alejandro Acosta, Johannes Totz, Zehan Wang,
and Wenzhe Shi. Real-time video super-resolution with spatio-temporal networks and motion
compensation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Remi Cadene, Hedi Ben-younes, Matthieu Cord, and Nicolas Thome. Murel: Multimodal relational
reasoning for visual question answering. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, and Luc
Van Gool. One-shot video object segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Nathan D. Cahill, Tyler L. Hayes, Renee T. Meinhold, and John F. Hamilton. Compassionately
conservative balanced cuts for image segmentation. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Lile Cai, Bin Zhao, Zhe Wang, Jie Lin, Chuan Sheng Foo, Mohamed Sabry Aly, and Vijay Chandrasekhar. Maxpoolnms: Getting rid of nms bottlenecks in two-stage object detectors. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Qi Cai, Yingwei Pan, Ting Yao, Chenggang Yan, and Tao Mei. Memory matching networks for oneshot image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Sijia Cai, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. A probabilistic collaborative representation based approach for pattern classification. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision
by half-wave gaussian quantization. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Juan C. Caicedo, Claire McQuin, Allen Goodman, Shantanu Singh, and Anne E. Carpenter. Weakly
supervised learning of single-cell feature embeddings. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Fatih Cakir, Kun He, Xide Xia, Brian Kulis, and Stan Sclaroff. Deep metric learning to rank. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Lilian Calvet, Pierre Gurdjos, Carsten Griwodz, and Simone Gasparini. Detection and accurate
localization of circular fiducials under highly challenging conditions. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Dylan Campbell and Lars Petersson. Gogma: Globally-optimal gaussian mixture alignment. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Dylan Campbell, Lars Petersson, Laurent Kneip, Hongdong Li, and Stephen Gould. The alignment
of the spheres: Globally-optimal spherical mixture alignment for camera pose estimation. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Federico Camposeco, Torsten Sattler, Andrea Cohen, Andreas Geiger, and Marc Pollefeys. Toroidal
constraints for two-point localization under high outlier ratios. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Federico Camposeco, Andrea Cohen, Marc Pollefeys, and Torsten Sattler. Hybrid camera pose
estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
29

149

Published as a conference paper at SIGBOVIK 2020

Federico Camposeco, Andrea Cohen, Marc Pollefeys, and Torsten Sattler. Hybrid scene compression for visual localization. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Hakki Can Karaimer and Michael S. Brown. Improving color reproduction accuracy on cameras. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Olivier Canevet and Francois Fleuret. Large scale hard sample mining with monte carlo tree search.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Jiale Cao, Yanwei Pang, and Xuelong Li. Pedestrian detection inspired by appearance constancy and
shape symmetry. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Qingxing Cao, Liang Lin, Yukai Shi, Xiaodan Liang, and Guanbin Li. Attention-aware face hallucination via deep reinforcement learning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Yue Cao, Mingsheng Long, Bin Liu, and Jianmin Wang. Deep cauchy hashing for hamming space
retrieval. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Zhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, and Qiang Yang. Learning to transfer
examples for partial domain adaptation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Fabio M. Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics
dataset. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, and Jitendra Malik. Human pose estimation
with iterative error feedback. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Lluis Castrejon, Yusuf Aytar, Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Learning
aligned cross-modal representations from weakly aligned data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Lluis Castrejon, Kaustav Kundu, Raquel Urtasun, and Sanja Fidler. Annotating object instances with
a polygon-rnn. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord, Julien Valentin, Luigi Di Stefano, and Philip
H. S. Torr. On-the-fly adaptation of regression forests for online camera relocalisation. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Lorenzo Cerrone, Alexander Zeilmann, and Fred A. Hamprecht. End-to-end learned random walker
for seeded image segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Hakan Cevikalp and Bill Triggs. Polyhedral conic classifiers for visual object detection and classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, Celine Teuliere, and Thierry Chateau. Deep
manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular
image. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
30

150

Published as a conference paper at SIGBOVIK 2020

Rohan Chabra, Julian Straub, Christopher Sweeney, Richard Newcombe, and Henry Fuchs. Stereodrnet: Dilated residual stereonet. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Rudrasis Chakraborty, Dohyung Seo, and Baba C. Vemuri. An efficient exact-pga algorithm for constant curvature manifolds. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Rudrasis Chakraborty, Soren Hauberg, and Baba C. Vemuri. Intrinsic grassmann averages for online
linear and robust subspace learning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Jacob Chan, Jimmy Addison Lee, and Qian Kemao. Border: An oriented rectangles approach
to texture-less object recognition. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Jacob Chan, Jimmy Addison Lee, and Qian Kemao. Bind: Binary integrated net descriptors for
texture-less object recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Rohan Chandra, Uttaran Bhattacharya, Aniket Bera, and Dinesh Manocha. Traphic: Trajectory prediction in dense and heterogeneous traffic using weighted interactions. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Siddhartha Chandra, Camille Couprie, and Iasonas Kokkinos. Deep spatio-temporal random fields
for efficient video segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh. We are humor beings: Understanding and predicting
visual humor. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Che-Han Chang, Chun-Nan Chou, and Edward Y. Chang. Clkn: Cascaded lucas-kanade networks
for image alignment. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Huiwen Chang, Jingwan Lu, Fisher Yu, and Adam Finkelstein. Pairedcyclegan: Asymmetric style
transfer for applying and removing makeup. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Wei-Lun Chang, Hui-Po Wang, Wen-Hsiao Peng, and Wei-Chen Chiu. All about structure: Adapting
structural information across domains for boosting semantic segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Xiaojun Chang, Yao-Liang Yu, Yi Yang, and Eric P. Xing. They are not equally reliable: Semantic
event search using differentiated concept classifiers. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, and Fei Sha. Synthesized classifiers for zeroshot learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Yu-Wei Chao, Jimei Yang, Brian Price, Scott Cohen, and Jia Deng. Forecasting human dynamics from static images. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A. Ross, Jia Deng, and Rahul
Sukthankar. Rethinking the faster r-cnn architecture for temporal action localization. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
James Charles, Tomas Pfister, Derek Magee, David Hogg, and Andrew Zisserman. Personalizing
human video pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
31

151

Published as a conference paper at SIGBOVIK 2020

Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R. Selvaraju, Dhruv Batra, and Devi
Parikh. Counting everyday objects in everyday scenes. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Bindita Chaudhuri, Noranart Vesdapunt, and Baoyuan Wang. Joint face detection and facial motion retargeting for multiple faces. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Neelima Chavali, Harsh Agrawal, Aroma Mahendru, and Dhruv Batra. Object-proposal evaluation
protocol is ’gameable’. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Tatjana Chavdarova, Pierre Baqué, Stéphane Bouquet, Andrii Maksai, Cijo Jose, Timur Bagautdinov, Louis Lettry, Pascal Fua, Luc Van Gool, and François Fleuret. Wildtrack: A multi-camera hd
dataset for dense unscripted pedestrian detection. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Da Chen, Jean-Marie Mirebeau, and Laurent D. Cohen. A new finsler minimal path model with
curvature penalization for image segmentation and closed contour detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Long Chen, Hanwang Zhang, Jun Xiao, Wei Liu, and Shih-Fu Chang. Zero-shot visual recognition using semantics-preserving adversarial embedding networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang. Beyond triplet loss: A deep
quadruplet network for person re-identification. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Yan Shuicheng, Jiashi Feng, and Yannis Kalantidis. Graph-based global reasoning networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Dongliang Cheng, Abdelrahman Abdelhamed, Brian Price, Scott Cohen, and Michael S. Brown.
Two illuminant estimation and user correction preference. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Hao Cheng, Dongze Lian, Bowen Deng, Shenghua Gao, Tao Tan, and Yanlin Geng. Local to global
learning: Gradually adding classes for training deep neural networks. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Lechao Cheng, Chengyi Zhang, and Zicheng Liao. Intrinsic image transformation via scale space
decomposition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Yanhua Cheng, Rui Cai, Zhiwei Li, Xin Zhao, and Kaiqi Huang. Locality-sensitive deconvolution
networks with gated fusion for rgb-d indoor semantic segmentation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Anoop Cherian, Basura Fernando, Mehrtash Harandi, and Stephen Gould. Generalized rank pooling
for activity recognition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Anoop Cherian, Suvrit Sra, Stephen Gould, and Richard Hartley. Non-linear temporal subspace
representations for activity recognition. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Ajad Chhatkuli, Daniel Pizarro, Toby Collins, and Adrien Bartoli. Inextensible non-rigid shapefrom-motion by second-order cone programming. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Tat-Jun Chin, Yang Heng Kee, Anders Eriksson, and Frank Neumann. Guaranteed outlier removal
with mixed integer linear programs. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
32

152

Published as a conference paper at SIGBOVIK 2020

Wonwoong Cho, Sungha Choi, David Keetae Park, Inkyu Shin, and Jaegul Choo. Image-to-image
translation via group-wise deep whitening-and-coloring transformation. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yeong-Jun Cho and Kuk-Jin Yoon. Improving person re-identification via pose-aware multi-shot
matching. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Gyeongmin Choe, Srinivasa G. Narasimhan, and In So Kweon. Simultaneous estimation of near ir
brdf and fine-scale surface geometry. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Junsuk Choe and Hyunjung Shim. Attention-based dropout layer for weakly supervised object
localization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Jinsoo Choi, Tae-Hyun Oh, and In So Kweon. Video-story composition via plot analysis. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fischer, Yiannis Demiris, and Jin
Young Choi. Attentional correlation filter network for adaptive visual tracking. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Jongwon Choi, Hyung Jin Chang, Tobias Fischer, Sangdoo Yun, Kyuewang Lee, Jiyeoup Jeong,
Yiannis Demiris, and Jin Young Choi. Context-aware deep feature compression for high-speed
visual tracking. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Hisham Cholakkal, Jubin Johnson, and Deepu Rajan. Backtracking scspm image classifier for
weakly supervised top-down saliency. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Hisham Cholakkal, Guolei Sun, Fahad Shahbaz Khan, and Ling Shao. Object counting and instance
segmentation with image-level supervision. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Vasileios Choutas, Philippe Weinzaepfel, Jérôme Revaud, and Cordelia Schmid. Potion: Pose motion representation for action recognition. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski
convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Hang Chu, Wei-Chiu Ma, Kaustav Kundu, Raquel Urtasun, and Sanja Fidler. Surfconv: Bridging
3d and 2d convolution for rgbd images. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Wen-Hsuan Chu, Yu-Jhe Li, Jing-Cheng Chang, and Yu-Chiang Frank Wang. Spot and learn: A
maximum-entropy patch sampler for few-shot image classification. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Xiao Chu, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. Structured feature learning for pose
estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
33

153

Published as a conference paper at SIGBOVIK 2020

Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L. Yuille, and Xiaogang Wang. Multi-context
attention for human pose estimation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Ching-Yao Chuang, Jiaman Li, Antonio Torralba, and Sanja Fidler. Learning to act properly: Predicting and explaining affordances from images. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Cesc Chunseong Park, Byeongchang Kim, and Gunhee Kim. Attend to you: Personalized image
captioning with context sequence memory networks. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden. Neural
sign language translation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Ronald Clark, Sen Wang, Andrew Markham, Niki Trigoni, and Hongkai Wen. Vidloc: A deep
spatio-temporal model for 6-dof video-clip relocalization. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Gad Cohen and Daphna Weinshall. Hidden layers in perceptual learning. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Nadav Cohen, Or Sharir, and Amnon Shashua. Deep simnets. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Forrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar Mosseri, and William T. Freeman. Synthesizing normalized faces from facial identity features. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
John Collomosse, Tu Bui, and Hailin Jin. Livesketch: Query perturbations for guided sketch-based
visual search. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic
urban scene understanding. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Ciprian A. Corneanu, Meysam Madadi, Sergio Escalera, and Aleix M. Martinez. What does it mean
to learn in deep networks? and, how does one detect adversarial attacks? In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Show, control and tell: A framework for
generating controllable and grounded captions. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Ionut Cosmin Duta, Bogdan Ionescu, Kiyoharu Aizawa, and Nicu Sebe. Spatio-temporal vector
of locally max pooled features for action recognition in videos. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Luca Cosmo, Mikhail Panine, Arianna Rampini, Maks Ovsjanikov, Michael M. Bronstein, and
Emanuele Rodola. Isospectralization, or how to hear shape, style, and correspondence. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Nieves Crasto, Philippe Weinzaepfel, Karteek Alahari, and Cordelia Schmid. Mars: Motionaugmented rgb stream for action recognition. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Marco Crocco, Cosimo Rubino, and Alessio Del Bue. Structure from motion with objects. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
34

154

Published as a conference paper at SIGBOVIK 2020

Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment:
Learning augmentation strategies from data. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Guillem Cucurull, Perouz Taslakian, and David Vazquez. Context-aware visual compatibility prediction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, and Michael J. Black. Capture,
learning, and synthesis of 3d speaking styles. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Hainan Cui, Xiang Gao, Shuhan Shen, and Zhanyi Hu. Hsfm: Hybrid structure-from-motion. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yin Cui, Feng Zhou, Yuanqing Lin, and Serge Belongie. Fine-grained categorization and dataset
bootstrapping using deep metric learning with humans in the loop. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Yin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge Belongie. Large scale fine-grained
categorization and domain-specific transfer learning. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Zhiming Cui, Changjian Li, and Wenping Wang. Toothnet: Automatic tooth instance segmentation
and identification from cone beam ct images. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Cheng Da, Shibiao Xu, Kun Ding, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Amvh:
Asymmetric multi-valued hashing. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, Jürgen Sturm, and Matthias Nießner.
Scancomplete: Large-scale scene completion and semantic segmentation for 3d scans. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Bo Dai, Yuqi Zhang, and Dahua Lin. Detecting visual relationships with deep relational networks.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware semantic segmentation via multi-task network
cascades. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Kenan Dai, Dong Wang, Huchuan Lu, Chong Sun, and Jianhua Li. Visual tracking via adaptive
spatially-regularized correlation filters. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Adrian V. Dalca, John Guttag, and Mert R. Sabuncu. Anatomical priors in convolutional networks
for unsupervised biomedical segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and Michael Felsberg. Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Eco: Efficient convolution operators for tracking. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Atom: Accurate
tracking by overlap maximization. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Arthur Daniel Costea and Sergiu Nedevschi. Semantic channels for fast pedestrian detection. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
35

155

Published as a conference paper at SIGBOVIK 2020

Arthur Daniel Costea, Robert Varga, and Sergiu Nedevschi. Fast boosting based detection using
scale invariant multimodal multiresolution filtered features. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Donald G. Dansereau, Glenn Schuster, Joseph Ford, and Gordon Wetzstein. A wide-field-of-view
monocentric light field camera. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Donald G. Dansereau, Bernd Girod, and Gordon Wetzstein. Liff: Light field features in scale and
depth. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Mor Dar and Yael Moses. Temporal epipolar regions. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F. Moura, Devi
Parikh, and Dhruv Batra. Visual dialog. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Saumitro Dasgupta, Kuan Fang, Kevin Chen, and Silvio Savarese. Delay: Robust spatial layout
estimation for cluttered indoor scenes. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Achal Dave, Olga Russakovsky, and Deva Ramanan. Predictive-corrective networks for action detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron
Courville. Guesswhat?! visual object discovery through multi-modal dialogue. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Joseph DeGol, Mani Golparvar-Fard, and Derek Hoiem. Geometry-informed material recognition.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Tali Dekel, Michael Rubinstein, Ce Liu, and William T. Freeman. On the effectiveness of visible
watermarks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Tali Dekel, Chuang Gan, Dilip Krishnan, Ce Liu, and William T. Freeman. Sparse, smart contours to
represent and edit images. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Luca Del Pero, Susanna Ricco, Rahul Sukthankar, and Vittorio Ferrari. Discovering the physical
parts of an articulated object class from multiple videos. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Girum G. Demisse, Djamila Aouada, and Bjorn Ottersten. Similarity metric for curved shapes in
euclidean space. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppfnet: Global context aware local features for
robust 3d point matching. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Haowen Deng, Tolga Birdal, and Slobodan Ilic. 3d local features for direct pairwise registration. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Zhiwei Deng, Arash Vahdat, Hexiang Hu, and Greg Mori. Structure inference machines: Recurrent
neural networks for analyzing relations in group activity recognition. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
36

156

Published as a conference paper at SIGBOVIK 2020

Zhiwei Deng, Rajitha Navarathna, Peter Carr, Stephan Mandt, Yisong Yue, Iain Matthews, and
Greg Mori. Factorized variational autoencoders for modeling audience reactions to movies. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Mohammad Mahdi Derakhshani, Saeed Masoudnia, Amir Hossein Shaker, Omid Mersa, Mohammad Amin Sadeghi, Mohammad Rastegari, and Babak N. Araabi. Assisted excitation of activations: A learning technique to improve object detectors. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Aditya Deshpande, Jiajun Lu, Mao-Chuang Yeh, Min Jin Chong, and David Forsyth. Learning
diverse image colorization. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Ishan Deshpande, Ziyu Zhang, and Alexander G. Schwing. Generative modeling using the sliced
wasserstein distance. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen
Zhao, David Forsyth, and Alexander G. Schwing. Max-sliced wasserstein distance and its use for
gans. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Shay Deutsch, Soheil Kolouri, Kyungnam Kim, Yuri Owechko, and Stefano Soatto. Zero shot
learning via multi-scale manifold regularization. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Mustafa Devrim Kaba, Mustafa Gokhan Uzunbas, and Ser Nam Lim. A reinforcement learning
approach to the view planning problem. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Sounak Dey, Pau Riba, Anjan Dutta, Josep Llados, and Yi-Zhe Song. Doodle to search: Practical
zero-shot sketch-based image retrieval. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Arturo Deza, Amit Surana, and Miguel P. Eckstein. Assessment of faster r-cnn in man-machine collaborative search. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. Learning
without memorizing. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Aditya Dhawale, Kumar Shaurya Shankar, and Nathan Michael. Fast monte-carlo localization on
aerial vehicles using approximate continuous belief representations. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Vikas Dhiman, Quoc-Huy Tran, Jason J. Corso, and Manmohan Chandraker. A continuous occlusion
model for road scene understanding. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Renwei Dian, Leyuan Fang, and Shutao Li. Hyperspectral image super-resolution via non-local
sparse tensor factorization. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Raul Diaz and Amit Marathe. Soft labels for ordinal regression. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Ali Diba, Ali Mohammad Pazandeh, Hamed Pirsiavash, and Luc Van Gool. Deepcamp: Deep
convolutional action attribute mid-level patterns. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Ali Diba, Vivek Sharma, Ali Pazandeh, Hamed Pirsiavash, and Luc Van Gool. Weakly supervised
cascaded convolutional networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
37

157

Published as a conference paper at SIGBOVIK 2020

Endri Dibra, Himanshu Jain, Cengiz Oztireli, Remo Ziegler, and Markus Gross. Human shape
from silhouettes using generative hks descriptors and cross-modal neural networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Caglayan Dicle, Burak Yilmaz, Octavia Camps, and Mario Sznaier. Solving temporal puzzles. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Maximilian Diebold, Bernd Jahne, and Alexander Gatto. Heterogeneous light fields. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Ferran Diego and Fred A. Hamprecht. Structured regression gradient boosting. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
N. Dinesh Reddy, Minh Vo, and Srinivasa G. Narasimhan. Carfusion: Combining point tracking and
part detection for dynamic 3d reconstruction of vehicles. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang Wang. Context contrasted feature
and gated multi-scale aggregation for scene segmentation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, and Qikai Lu. Learning roi transformer for oriented
object detection in aerial images. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Zhengming Ding, Ming Shao, and Yun Fu. Low-rank embedded ensemble semantic dictionary
for zero-shot learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Mandar Dixit, Roland Kwitt, Marc Niethammer, and Nuno Vasconcelos. Aga: Attribute-guided
augmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Konstantin Dmitriev and Arie E. Kaufman. Learning multi-class segmentations from single-class
datasets. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Thanh-Toan Do, Dang-Khoa Le Tan, Trung T. Pham, and Ngai-Man Cheung. Simultaneous feature
aggregating and hashing for large-scale image search. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Thanh-Toan Do, Toan Tran, Ian Reid, Vijay Kumar, Tuan Hoang, and Gustavo Carneiro. A theoretically sound upper bound on the triplet loss for improving the efficiency of deep distance metric
learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Pelin Dogan, Boyang Li, Leonid Sigal, and Markus Gross. A neural multi-sequence alignment
technique (neumatch). In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Pelin Dogan, Leonid Sigal, and Markus Gross. Neural sequential phrase grounding (seqground). In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Pierre Dognin, Igor Melnyk, Youssef Mroueh, Jerret Ross, and Tom Sercu. Adversarial semantic
alignment for improved image captions. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Brian Dolhansky and Cristian Canton Ferrer. Eye in-painting with exemplar generative adversarial
networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Jose Dolz, Ismail Ben Ayed, and Christian Desrosiers. Dope: Distributed optimization for pairwise
energies. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
38

158

Published as a conference paper at SIGBOVIK 2020

Jingming Dong, Xiaohan Fei, and Stefano Soatto. Visual-inertial-semantic scene representation
for 3d object detection. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Xuanyi Dong, Shoou-I Yu, Xinshuo Weng, Shih-En Wei, Yi Yang, and Yaser Sheikh. Supervisionby-registration: An unsupervised approach to improve the precision of facial landmark detectors.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Simon Donne and Andreas Geiger. Learning non-volumetric depth fusion using successive reprojections. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill D. F. Campbell, and Ivor Simpson. Structured
uncertainty prediction networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Pengfei Dou, Shishir K. Shah, and Ioannis A. Kakadiaris. End-to-end 3d face reconstruction with
deep neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Hazel Doughty, Dima Damen, and Walterio Mayol-Cuevas. Who’s better? who’s best? pairwise
deep ranking for skill determination. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Hazel Doughty, Walterio Mayol-Cuevas, and Dima Damen. The pros and cons: Rank-aware temporal attention for skill determination in long videos. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malassiotis, and Tae-Kyun Kim. Recovering
6d object pose and predicting next-best-view in the crowd. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Matthijs Douze, Arthur Szlam, Bharath Hariharan, and Hervé Jégou. Low-shot learning with largescale diffusion. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Oren Dovrat, Itai Lang, and Shai Avidan. Learning to sample. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Kuo Du, Xiangbo Lin, Yi Sun, and Xiaohong Ma. Crossinfonet: Multi-task information sharing
based hand pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yang Du, Chunfeng Yuan, Bing Li, Weiming Hu, and Stephen Maybank. Spatio-temporal selforganizing map deep network for dynamic object detection from videos. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yueqi Duan, Jiwen Lu, Ziwei Wang, Jianjiang Feng, and Jie Zhou. Learning deep binary descriptor
with multi-quantization. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Yueqi Duan, Wenzhao Zheng, Xudong Lin, Jiwen Lu, and Jie Zhou. Deep adversarial metric learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Yueqi Duan, Yu Zheng, Jiwen Lu, Jie Zhou, and Qi Tian. Structural relational reasoning of point
clouds. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
39

159

Published as a conference paper at SIGBOVIK 2020

Abhimanyu Dubey, Laurens van der Maaten, Zeki Yalniz, Yixuan Li, and Dhruv Mahajan. Defense
against adversarial images using web-scale nearest-neighbor search. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Nghia Nguyen, Eric Patterson, Tien D. Bui, and Ngan
Le. Automatic face aging in videos via deep reinforcement learning. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Thibaut Durand, Nicolas Thome, and Matthieu Cord. Weldon: Weakly supervised learning of deep
convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Thibaut Durand, Taylor Mordan, Nicolas Thome, and Matthieu Cord. Wildcat: Weakly supervised
learning of deep convnets for image classification, pointwise localization and segmentation. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Thibaut Durand, Nazanin Mehrasa, and Greg Mori. Learning a deep convnet for multi-label classification with partial labels. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, and
Torsten Sattler. D2-net: A trainable cnn for joint description and detection of local features. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Suyog Dutt Jain and Kristen Grauman. Active image segmentation propagation. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusionseg: Learning to combine motion and
appearance for fully automatic segmentation of generic objects in videos. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Anjan Dutta and Zeynep Akata. Semantically tied paired cycle consistency for zero-shot sketchbased image retrieval. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Temporal cycle-consistency learning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Kshitij Dwivedi and Gemma Roig. Representation similarity analysis for efficient task taxonomy
transfer learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Benjamin Eckart, Kihwan Kim, Alejandro Troccoli, Alonzo Kelly, and Jan Kautz. Accelerated
generative models for 3d point cloud data. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Hamid Eghbal-zadeh, Werner Zellinger, and Gerhard Widmer. Mixture density generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Sepehr Eghbali and Ladan Tahvildari. Deep spherical quantization for image search. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Thibaud Ehret and Pablo Arias. On the convergence of patchmatch and its variants. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Thibaud Ehret, Axel Davy, Jean-Michel Morel, Gabriele Facciolo, and Pablo Arias. Model-blind
video denoising via frame-to-frame training. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
M. Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel. Infinite variational autoencoder
for semi-supervised learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
40

160

Published as a conference paper at SIGBOVIK 2020

Kiana Ehsani, Hessam Bagherinezhad, Joseph Redmon, Roozbeh Mottaghi, and Ali Farhadi. Who
let the dogs out? modeling dog behavior from visual data. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Gabriel Eilertsen, Rafal K. Mantiuk, and Jonas Unger. Single-frame regularization for temporally
stable cnns. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Aviv Eisenschtat and Lior Wolf. Linking image and text with 2-way nets. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Gil Elbaz, Tamar Avraham, and Anath Fischer. 3d point cloud registration for localization using
a deep neural network auto-encoder. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Shireen Elhabian and Ross Whitaker. Shapeodds: Variational bayesian learning of generative shape
models. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Ehsan Elhamifar and M. Clara De Paolis Kaluza. Online summarization via submodular and convex
optimization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Mohamed Elhoseiny, Yizhe Zhu, Han Zhang, and Ahmed Elgammal. Link the head to the ”beak”:
Zero shot learning from noisy text description at part precision. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Martin Engilberge, Louis Chevallier, Patrick Pérez, and Matthieu Cord. Finding beans in burgers:
Deep semantic-visual embedding with localization. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Martin Engilberge, Louis Chevallier, Patrick Perez, and Matthieu Cord. Sodeep: A sorting deep
net to learn ranking loss surrogates. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Ertunc Erdil, Sinan Yildirim, Mujdat Cetin, and Tolga Tasdizen. Mcmc shape sampling for image
segmentation with nonparametric shape priors. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Anders Eriksson, John Bastian, Tat-Jun Chin, and Mats Isaksson. A consensus-based framework
for distributed bundle adjustment. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Anders Eriksson, Carl Olsson, Fredrik Kahl, and Tat-Jun Chin. Rotation averaging and strong
duality. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Seyed A. Esmaeili, Bharat Singh, and Larry S. Davis. Fast-at: Fast automatic thumbnail generation
using deep neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Patrick Esser, Ekaterina Sutter, and Björn Ommer. A variational u-net for conditional appearance
and shape generation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
C. Fabian Benitez-Quiroz, Ramprakash Srinivasan, and Aleix M. Martinez. Emotionet: An accurate,
real-time algorithm for the automatic annotation of a million facial expressions in the wild. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
41

161

Published as a conference paper at SIGBOVIK 2020

Jose M. Facil, Benjamin Ummenhofer, Huizhong Zhou, Luis Montesano, Thomas Brox, and Javier
Civera. Cam-convs: Camera-aware multi-scale convolutions for single-view depth. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Jiri Fajtl, Vasileios Argyriou, Dorothy Monekosso, and Paolo Remagnino. Amnet: Memorability
estimation with attention. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. Heterogeneous memory enhanced multimodal attention model for video question answering. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Haoqi Fan and Jiatong Zhou. Stacked latent attention for multimodal reasoning. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Haoqiang Fan, Hao Su, and Leonidas J. Guibas. A point set generation network for 3d object
reconstruction from a single image. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Shaojing Fan, Tian-Tsong Ng, Bryan L. Koenig, Ming Jiang, and Qi Zhao. A paradigm for building
generalized models of human image perception through data fusion. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Hao-Shu Fang, Guansong Lu, Xiaolin Fang, Jianwen Xie, Yu-Wing Tai, and Cewu Lu. Weakly
and semi supervised human body part parsing via pose-guided knowledge transfer. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory transformer for
embodied agents in long-horizon tasks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip H.W. Leong. Syq: Learning symmetric
quantization for efficient deep neural networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Yazan Abu Farha and Jurgen Gall. Ms-tcn: Multi-stage temporal convolutional network for action
segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano Soatto. Empirical
study of the topology and geometry of deep networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network
fusion for video action recognition. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Christoph Feichtenhofer, Axel Pinz, and Richard P. Wildes. Temporal residual networks for dynamic
scene recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes, and Andrew Zisserman. What have we
learned from deep representations for action recognition? In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Jie Feng, Brian Price, Scott Cohen, and Shih-Fu Chang. Interactive segmentation on rgbd images
via cue selection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. Spatio-temporal video re-localization by warp lstm.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
42

162

Published as a conference paper at SIGBOVIK 2020

Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and Yue Gao. Gvcnn: Group-view convolutional neural networks for 3d shape recognition. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Zhen-Hua Feng, Josef Kittler, William Christmas, Patrik Huber, and Xiao-Jun Wu. Dynamic
attention-controlled cascaded shape regression exploiting training data augmentation and fuzzyset sample weighting. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Basura Fernando, Peter Anderson, Marcus Hutter, and Stephen Gould. Discriminative hierarchical
rank pooling for activity recognition. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould. Self-supervised video representation learning with odd-one-out networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich Müller. Splinecnn: Fast geometric
deep learning with continuous b-spline kernels. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, and
Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Michael Firman, Oisin Mac Aodha, Simon Julier, and Gabriel J. Brostow. Structured prediction of
unobserved voxels from a single depth image. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Michael Firman, Neill D. F. Campbell, Lourdes Agapito, and Gabriel J. Brostow. Diversenet: When
one right answer is not enough. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. Deepstereo: Learning to predict
new views from the world’s imagery. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan Overbeck, Noah
Snavely, and Richard Tucker. Deepview: View synthesis with learned gradient descent. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Ruth Fong and Andrea Vedaldi. Net2vec: Quantifying and explaining how concepts are encoded
by filters in deep neural networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
David F. Fouhey, Abhinav Gupta, and Andrew Zisserman. 3d shape attributes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
David F. Fouhey, Wei-cheng Kuo, Alexei A. Efros, and Jitendra Malik. From lifestyle vlogs to
everyday interactions. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Johan Fredriksson, Viktor Larsson, Carl Olsson, and Fredrik Kahl. Optimal relative pose with
unknown correspondences. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Oriel Frigo, Neus Sabater, Julie Delon, and Pierre Hellier. Split and match: Example-based adaptive
patch sampling for unsupervised style transfer. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Huan Fu, Chaohui Wang, Dacheng Tao, and Michael J. Black. Occlusion boundary detection via
deep exploration of context. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
43

163

Published as a conference paper at SIGBOVIK 2020

Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal
regression network for monocular depth estimation. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, Kun Zhang, and Dacheng Tao.
Geometry-consistent generative adversarial networks for one-sided unsupervised domain mapping. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao Ding, and John Paisley. Removing
rain from single images via a deep detail network. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Yuki Fujimura, Masaaki Iiyama, Atsushi Hashimoto, and Michihiko Minoh. Photometric stereo in
participating media considering shape-dependent forward scatter. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi. Attention branch
network: Learning of attention mechanism for visual explanation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Chris Funk and Yanxi Liu. Symmetry recaptcha. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
David Gadot and Lior Wolf. Patchbatch: A batch augmented loss for optical flow. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multiobject tracking analysis. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Adrian Galdran, Aitor Alvarez-Gila, Alessandro Bria, Javier Vazquez-Corral, and Marcelo
Bertalmı́o. On the duality between retinex and image dehazing. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Guillermo Gallego, Henri Rebecq, and Davide Scaramuzza. A unifying contrast maximization
framework for event cameras, with applications to motion, depth, and optical flow estimation.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Guillermo Gallego, Mathias Gehrig, and Davide Scaramuzza. Focus is all you need: Loss functions
for event-based vision. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Silvano Galliani and Konrad Schindler. Just look at the image: Viewpoint-specific surface normal
prediction for improved multi-view reconstruction. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Chuang Gan, Tianbao Yang, and Boqing Gong. Learning attributes equals multi-source domain
generalization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, and Li Deng. Stylenet: Generating attractive
visual captions with styles. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Chuang Gan, Boqing Gong, Kun Liu, Hao Su, and Leonidas J. Guibas. Geometry guided convolutional neural networks for self-supervised video representation learning. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Yosef Gandelsman, Assaf Shocher, and Michal Irani. ”double-dip”: Unsupervised image decomposition via coupled deep-image-priors. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
44

164

Published as a conference paper at SIGBOVIK 2020

Siddha Ganju, Olga Russakovsky, and Abhinav Gupta. What’s in a question: Using visual questions
as a form of supervision. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Ruohan Gao and Kristen Grauman. 2.5d visual sound. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Ruohan Gao, Bo Xiong, and Kristen Grauman. Im2flow: Motion hallucination from static images
for action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell. Compact bilinear pooling. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Zhanning Gao, Gang Hua, Dongqing Zhang, Nebojsa Jojic, Le Wang, Jianru Xue, and Nanning
Zheng. Er3: A unified framework for event retrieval, recognition and recounting. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Guillermo Garcia-Hernando and Tae-Kyun Kim. Transition forests: Learning discriminative temporal transitions for action recognition and detection. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, and Tae-Kyun Kim. First-person hand
action benchmark with rgb-d videos and 3d hand pose annotations. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, and Jean-Francois Lalonde. Fast
spatially-varying indoor lighting estimation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Jochen Gast and Stefan Roth. Lightweight probabilistic deep networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Jochen Gast, Anita Sellent, and Stefan Roth. Parametric object motion from blur. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Vijetha Gattupalli, Yaoxin Zhuo, and Baoxin Li. Weakly supervised deep image hashing through
tag embeddings. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image style transfer using convolutional
neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. Controlling perceptual factors in neural style transfer. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees G. M. Snoek. Actor and action video segmentation from a sentence. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Liuhao Ge, Hui Liang, Junsong Yuan, and Daniel Thalmann. Robust 3d hand pose estimation in
single depth images: From single-view cnn to multi-view cnns. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Weifeng Ge and Yizhou Yu. Borrowing treasures from the wealthy: Deep transfer learning through
selective joint fine-tuning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Weifeng Ge, Sibei Yang, and Yizhou Yu. Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
45

165

Published as a conference paper at SIGBOVIK 2020

Weifeng Ge, Xiangru Lin, and Yizhou Yu. Weakly supervised complementary parts models for finegrained image classification from the bottom up. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou. Ganfit: Generative adversarial network fitting for high fidelity 3d face reconstruction. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Patrick Geneva, James Maley, and Guoquan Huang. An efficient schmidt-ekf for 3d visual-inertial
slam. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Zhenglin Geng, Chen Cao, and Sergey Tulyakov. 3d guided fine-grained face manipulation. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, and William T. Freeman. Unsupervised training for 3d morphable model regression. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Georgios Georgakis, Srikrishna Karanam, Ziyan Wu, Jan Ernst, and Jana Košecká. End-to-end
learning of keypoint detector and descriptor for pose invariant 3d matching. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Georgios Georgiadis. Accelerating convolutional neural networks via activation map compression.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-scale weakly-supervised pre-training for
video action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Kamran Ghasedi, Xiaoqian Wang, Cheng Deng, and Heng Huang. Balanced self-paced learning
for generative adversarial clustering network. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Kamran Ghasedi Dizaji, Feng Zheng, Najmeh Sadoughi, Yanhua Yang, Cheng Deng, and Heng
Huang. Unsupervised deep generative adversarial hashing network. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V. Le. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Behnam Gholami and Vladimir Pavlovic. Probabilistic temporal subspace clustering. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Arnab Ghosh, Viveka Kulharia, Vinay P. Namboodiri, Philip H.S. Torr, and Puneet K. Dokania.
Multi-agent diverse generative adversarial networks. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Soumyadeep Ghosh, Richa Singh, and Mayank Vatsa. On learning density aware embeddings. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Silvio Giancola, Jesus Zarzar, and Bernard Ghanem. Leveraging shape completion for 3d siamese
tracking. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Spyros Gidaris and Nikos Komodakis. Locnet: Improving localization accuracy for object detection.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Spyros Gidaris and Nikos Komodakis. Detect, replace, refine: Deep structured prediction for pixel
wise labeling. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
46

166

Published as a conference paper at SIGBOVIK 2020

Spyros Gidaris and Nikos Komodakis. Generating classification weights with gnn denoising autoencoders for few-shot learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Andrew Gilbert, John Collomosse, Hailin Jin, and Brian Price. Disentangling structure and aesthetics for style-aware image completion. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and Jitendra Malik. Learning individual styles of conversational gesture. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic, and Bryan Russell. Actionvlad: Learning spatio-temporal aggregation for action classification. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Rohit Girdhar, Georgia Gkioxari, Lorenzo Torresani, Manohar Paluri, and Du Tran. Detect-andtrack: Efficient pose estimation in videos. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. Video action transformer network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Georgia Gkioxari, Ross Girshick, Piotr Dollár, and Kaiming He. Detecting and recognizing humanobject interactions. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Clement Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estimation with left-right consistency. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Arushi Goel, Keng Teck Ma, and Cheston Tan. An end-to-end network for generating social relationship graphs. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Zan Gojcic, Caifa Zhou, Jan D. Wegner, and Andreas Wieser. The perfect match: 3d point cloud
matching with smoothed densities. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Eran Goldman, Roei Herzig, Aviv Eisenschtat, Jacob Goldberger, and Tal Hassner. Precise detection
in densely packed scenes. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Vladislav Golyanik, Sk Aziz Ali, and Didier Stricker. Gravitational approach for point set registration. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Lluis Gomez, Yash Patel, Marcal Rusinol, Dimosthenis Karatzas, and C. V. Jawahar. Self-supervised
learning of visual features through embedding images into text topic spaces. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Alexander Gomez-Villa, Adrian Martin, Javier Vazquez-Corral, and Marcelo Bertalmio. Convolutional neural networks can be deceived by visual illusions. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Dong Gong, Mingkui Tan, Yanning Zhang, Anton van den Hengel, and Qinfeng Shi. Blind image
deconvolution by automatic gradient activation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen, and Liang Lin. Look into person: Selfsupervised structure-sensitive learning and a new benchmark for human parsing. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
47

167

Published as a conference paper at SIGBOVIK 2020

Rui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. Dlow: Domain flow for adaptation and generalization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Yunye Gong, Srikrishna Karanam, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Peter C. Doerschuk.
Learning compositional visual concepts with mutual consistency. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Abel Gonzalez-Garcia, Davide Modolo, and Vittorio Ferrari. Objects as context for detecting their
semantic parts. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Anand Gopalakrishnan, Ankur Mali, Dan Kifer, Lee Giles, and Alexander G. Ororbia. A neural
temporal model for human motion prediction. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Albert Gordo and Diane Larlus. Beyond instance-level image retrieval: Leveraging captions to learn
a global visual representation for semantic retrieval. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. Morphnet: Fast simple resource-constrained structure learning of deep networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Lena Gorelick, Yuri Boykov, and Olga Veksler. Adaptive and move making auxiliary cuts for binary
pairwise energies. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Siavash Gorji and James J. Clark. Attentional push: A deep convolutional network for augmenting
image salience with shared attention modeling in social scenes. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Siavash Gorji and James J. Clark. Going from image to video saliency: Augmenting image salience
with dynamic attentional push. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Mengran Gou, Fei Xiong, Octavia Camps, and Mario Sznaier. Monet: Moments embedding network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in
vqa matter: Elevating the role of image understanding in visual question answering. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Alexander Grabner, Peter M. Roth, and Vincent Lepetit. 3d pose estimation and 3d model retrieval
for objects in the wild. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Benjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3d semantic segmentation with
submanifold sparse convolutional networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Brent A. Griffin and Jason J. Corso. Bubblenets: Learning to select the guidance frame in video
object segmentation by deep sorting frames. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Artur Grigorev, Artem Sevastopolsky, Alexander Vakhitov, and Victor Lempitsky. Coordinate-based
texture inpainting for pose-guided human image generation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Sam Gross, Marc’Aurelio Ranzato, and Arthur Szlam. Hard mixtures of experts for large scale
weakly supervised vision. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
48

168

Published as a conference paper at SIGBOVIK 2020

Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, and Mathieu Aubry. A
papier-mâché approach to learning 3d surface generation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra
Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, and
Jitendra Malik. Ava: A video dataset of spatio-temporally localized atomic visual actions. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong. Blind super-resolution with iterative kernel
correction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Jinwei Gu, Xiaodong Yang, Shalini De Mello, and Jan Kautz. Dynamic facial analysis: From
bayesian filtering to recurrent neural network. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Hao Guan and William A. P. Smith. Brisks: Binary features for spherical images on a geodesic grid.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Riza Alp Guler and Iasonas Kokkinos. Holopose: Holistic 3d human reconstruction in-the-wild. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Hao Guo, Kang Zheng, Xiaochuan Fan, Hongkai Yu, and Song Wang. Visual attention consistency under image transforms for multi-label image classification. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Jun Guo and Hongyang Chao. One-to-many network for visually pleasing compression artifacts
reduction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Yiluan Guo and Ngai-Man Cheung. Efficient and deep person re-identification using multi-level
similarity. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Synthetic data for text localisation in
natural images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive
mapping and planning for visual navigation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Shir Gur and Lior Wolf. Single image depth estimation trained via depth from defocus cues. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Danna Gurari, Suyog Jain, Margrit Betke, and Kristen Grauman. Pull the plug? predicting if computers or humans should segment images. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and
Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
49

169

Published as a conference paper at SIGBOVIK 2020

Danna Gurari, Qing Li, Chi Lin, Yinan Zhao, Anhong Guo, Abigale Stangl, and Jeffrey P. Bigham.
Vizwiz-priv: A dataset for recognizing the presence and purpose of private visual information
in images taken by blind people. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R. Venkatesh Babu. Deligan : Generative adversarial networks for diverse and limited data. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Felipe Gutierrez-Barragan, Syed Azer Reza, Andreas Velten, and Mohit Gupta. Practical coding
function design for time-of-flight imaging. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Michael Gygli and Vittorio Ferrari. Fast object class labelling via speech. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Michael Gygli, Yale Song, and Liangliang Cao. Video2gif: Automatic generation of animated gifs
from video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Hyowon Ha, Sunghoon Im, Jaesik Park, Hae-Gon Jeon, and In So Kweon. High-quality depth
from uncalibrated small motion clip. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Gerard Pons-Moll, and Christian Theobalt. In the
wild human pose estimation using explicit 2d features and intermediate 3d representations. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Timo Hackel, Jan D. Wegner, and Konrad Schindler. Contour detection in unstructured 3d point
clouds. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Naama Hadad, Lior Wolf, and Moni Shahar. A two-step disentanglement method. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Benjamin D. Haeffele and Rene Vidal. Global optimality in neural network training. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Bjoern Haefner, Yvain Quéau, Thomas Möllenhoff, and Daniel Cremers. Fight ill-posedness with
ill-posedness: Single-shot variational depth super-resolution from shading. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Daniel Haehn, Verena Kaynig, James Tompkin, Jeff W. Lichtman, and Hanspeter Pfister. Guided
proofreading of automatic segmentations for connectomics. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Philip Haeusser, Alexander Mordvintsev, and Daniel Cremers. Learning by association – a versatile
semi-supervised training method for neural networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Maciej Halber and Thomas Funkhouser. Fine-to-coarse global registration of rgb-d scans. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Oshri Halimi, Or Litany, Emanuele Rodola, Alex M. Bronstein, and Ron Kimmel. Unsupervised
learning of dense shape correspondence. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Bumsub Ham, Minsu Cho, Cordelia Schmid, and Jean Ponce. Proposal flow. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Ryuhei Hamaguchi, Ken Sakurada, and Ryosuke Nakamura. Rare event detection using disentangled representation learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
50

170

Published as a conference paper at SIGBOVIK 2020

Seyed Hamid Rezatofighi, Anton Milan, Zhen Zhang, Qinfeng Shi, Anthony Dick, and Ian Reid.
Joint probabilistic matching using m-best solutions. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Bohyung Han, Jack Sim, and Hartwig Adam. Branchout: Regularization for online ensemble tracking with convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Kai Han, Kwan-Yee K. Wong, Dirk Schnieders, and Miaomiao Liu. Mirror surface reconstruction under an uncalibrated camera. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Wei Han, Shiyu Chang, Ding Liu, Mo Yu, Michael Witbrock, and Thomas S. Huang. Image superresolution via dual-state recurrent networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Xiaoguang Han, Zhaoxuan Zhang, Dong Du, Mingdai Yang, Jingming Yu, Pan Pan, Xin Yang,
Ligang Liu, Zixiang Xiong, and Shuguang Cui. Deep reinforcement learning of volume-guided
progressive view inpainting for 3d point scene completion from a single depth image. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, and Roberto Cipolla. Understanding real world indoor scenes with synthetic data. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Zekun Hao, Yu Liu, Hongwei Qin, Junjie Yan, Xiu Li, and Xiaolin Hu. Scale-aware face detection.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Zekun Hao, Xun Huang, and Serge Belongie. Controllable video generation with sparse trajectories.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Nazim Haouchine and Stephane Cotin. Template-based monocular 3d recovery of elastic shapes using lagrangian multipliers. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Albert Haque, Alexandre Alahi, and Li Fei-Fei. Recurrent attention models for depth-based person
identification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history
of 2d cnns and imagenet? In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Ali Harakeh, Daniel Asmar, and Elie Shammas. Identifying good training data for self-supervised
free space estimation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Mehrtash Harandi, Mathieu Salzmann, and Fatih Porikli. When vlad met hilbert. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Muhammad Haris, Gregory Shakhnarovich, and Norimichi Ukita. Deep back-projection networks
for super-resolution. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Muhammad Haris, Gregory Shakhnarovich, and Norimichi Ukita. Recurrent back-projection network for video super-resolution. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Ben Harwood and Tom Drummond. Fanng: Fast approximate nearest neighbour graphs. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Irtiza Hasan, Francesco Setti, Theodore Tsesmelis, Alessio Del Bue, Fabio Galasso, and Marco
Cristani. Mx-lstm: Mixing tracklets and vislets to jointly forecast trajectories and head poses. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
51

171

Published as a conference paper at SIGBOVIK 2020

Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K. Roy-Chowdhury, and Larry S. Davis.
Learning temporal regularity in video sequences. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Tristan Hascoet, Yasuo Ariki, and Tetsuya Takiguchi. On zero-shot recognition of generic objects.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yana Hasson, Gul Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J. Black, Ivan Laptev, and
Cordelia Schmid. Learning joint reconstruction of hands and manipulated objects. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Monica Haurilet, Alina Roitberg, and Rainer Stiefelhagen. It’s not about the journey; it’s about
the destination: Following soft paths under question-guidance for visual reasoning. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Manuel Haussmann, Fred A. Hamprecht, and Melih Kandemir. Variational bayesian multiple instance learning with gaussian processes. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Munawar Hayat, Salman H. Khan, Naoufel Werghi, and Roland Goecke. Joint registration and representation learning for unconstrained face identification. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Zeeshan Hayder, Xuming He, and Mathieu Salzmann. Learning to co-generate object proposals with
a deep structured network. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Zeeshan Hayder, Xuming He, and Mathieu Salzmann. Boundary-aware instance segmentation. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Kun He, Yan Lu, and Stan Sclaroff. Local descriptors optimized for average precision. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Lifang He, Chun-Ta Lu, Hao Ding, Shen Wang, Linlin Shen, Philip S. Yu, and Ann B. Ragin.
Multi-way multi-level kernel modeling for neuroimaging classification. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for
image classification with convolutional neural networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Stefan Heber and Thomas Pock. Convolutional networks for shape from light field. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Eric Heim. Constrained generative adversarial networks for interactive image generation. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield highconfidence predictions far away from the training data and how to mitigate the problem. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
João F. Henriques and Andrea Vedaldi. Mapnet: An allocentric spatial memory for mapping environments. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Jae-Pil Heo, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and Sung-eui Yoon. Shortlist selection with
residual-aware distance estimator for k-nearest neighbor search. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Samitha Herath, Mehrtash Harandi, and Fatih Porikli. Learning an invariant hilbert space for domain
adaptation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
52

172

Published as a conference paper at SIGBOVIK 2020

Samitha Herath, Mehrtash Harandi, Basura Fernando, and Richard Nock. Min-max statistical alignment for transfer learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Luis Herranz, Shuqiang Jiang, and Xiangyang Li. Scene recognition with cnns: Objects, scales and
dataset bias. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Amir Hertz, Sharon Fogel, Rana Hanocka, Raja Giryes, and Daniel Cohen-Or. Blind visual motif removal from a single image. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Chih-Hui Ho, Brandon Leung, Erik Sandstrom, Yen Chang, and Nuno Vasconcelos. Catastrophic
child’s play: Easy to perform, hard to defend adversarial attacks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Judy Hoffman, Saurabh Gupta, and Trevor Darrell. Learning with side information through modality
hallucination. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto, and Jean-Francois
Lalonde. Deep outdoor illumination estimation. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Yannick Hold-Geoffroy, Kalyan Sunkavalli, Jonathan Eisenmann, Matthew Fisher, Emiliano Gambaretto, Sunil Hadap, and Jean-François Lalonde. A perceptual measure for deep single image camera calibration. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Yannick Hold-Geoffroy, Akshaya Athawale, and Jean-Francois Lalonde. Deep sky modeling for
single image outdoor lighting estimation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Namdar Homayounfar, Sanja Fidler, and Raquel Urtasun. Sports field localization via deep structured models. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Namdar Homayounfar, Wei-Chiu Ma, Shrinidhi Kowshika Lakshmikanth, and Raquel Urtasun. Hierarchical recurrent attention networks for structured online maps. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Sina Honari, Jason Yosinski, Pascal Vincent, and Christopher Pal. Recombinator networks: Learning coarse-to-fine feature aggregation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Sina Honari, Pavlo Molchanov, Stephen Tyree, Pascal Vincent, Christopher Pal, and Jan Kautz.
Improving landmark localization with semi-supervised learning. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Joey Hong, Benjamin Sapp, and James Philbin. Rules of the road: Predicting driving behavior with
a convolutional model of semantic interactions. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Seunghoon Hong, Junhyuk Oh, Honglak Lee, and Bohyung Han. Learning transferrable knowledge
for semantic segmentation with deep convolutional neural network. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Weixiang Hong, Zhenzhen Wang, Ming Yang, and Junsong Yuan. Conditional generative adversarial
network for structured domain adaptation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Ju Hong Yoon, Chang-Ryeol Lee, Ming-Hsuan Yang, and Kuk-Jin Yoon. Online multi-object tracking via structural constraint event aggregation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
53

173

Published as a conference paper at SIGBOVIK 2020

Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning non-maximum suppression. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yedid Hoshen and Shmuel Peleg. An egocentric look at video photographer identity. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Yedid Hoshen and Lior Wolf. Unsupervised correlation analysis. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Yedid Hoshen, Ke Li, and Jitendra Malik. Non-adversarial image synthesis with generative latent
nearest neighbors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Mahdi S. Hosseini, Lyndon Chan, Gabriel Tse, Michael Tang, Jun Deng, Sajad Norouzi, Corwyn
Rowsell, Konstantinos N. Plataniotis, and Savvas Damaskinos. Atlas of digital pathology: A
generalized hierarchical histological tissue type-annotated database for deep learning. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Vlad Hosu, Bastian Goldlucke, and Dietmar Saupe. Effective aesthetics prediction with multi-level
spatially pooled features. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Le Hou, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, James E. Davis, and Joel H. Saltz. Patch-based
convolutional neural network for whole slide tissue image classification. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji, Zhuowen Tu, and Philip H. S. Torr. Deeply
supervised salient object detection with short connections. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Wei-Lin Hsiao and Kristen Grauman. Creating capsule wardrobes from fashion images. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Deepco3: Deep instance co-segmentation by copeak search and co-saliency detection. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Ping Hu, Gang Wang, Xiangfei Kong, Jason Kuen, and Yap-Peng Tan. Motion-guided cascaded
refinement network for video object segmentation. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Tieniu Tan, and Jian Sun. Meta-sr: A
magnification-arbitrary network for super-resolution. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yinlin Hu, Yunsong Li, and Rui Song. Robust interpolation of correspondences for large displacement optical flow. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Yuan-Ting Hu and Yen-Yu Lin. Progressive feature matching with alternate descriptor selection and
correspondence enrichment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Pointwise convolutional neural networks. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, and
Wei Liu. Real-time neural style transfer for videos. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
54

174

Published as a conference paper at SIGBOVIK 2020

He Huang, Changhu Wang, Philip S. Yu, and Chang-Dong Wang. Generative dual adversarial
network for generalized zero-shot learning. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Kun Huang, Yifan Wang, Zihan Zhou, Tianjiao Ding, Shenghua Gao, and Yi Ma. Learning to parse
wireframes in images of man-made environments. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Shaoli Huang, Zhe Xu, Dacheng Tao, and Ya Zhang. Part-stacked cnn for fine-grained visual categorization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Inbar Huberman and Raanan Fattal. Detecting repeating objects using patch correlation analysis. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning
and compositional question answering. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Minyoung Huh, Shao-Hua Sun, and Ning Zhang. Feedback adversarial learning: Spatial feedback
for improving generative adversarial networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Zheng Hui, Xiumei Wang, and Xinbo Gao. Fast and accurate single image super-resolution via information distillation network. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Zhuo Hui, Ayan Chakrabarti, Kalyan Sunkavalli, and Aswin C. Sankaranarayanan. Learning to
separate multiple illuminants in a single image. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan Yang, and Jan Kautz.
Scops: Self-supervised co-part segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Junhwa Hur and Stefan Roth. Iterative residual refinement for joint optical flow and occlusion
estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha,
Nathan Ong, and Adriana Kovashka. Automatic understanding of image and video advertisements. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Noureldien Hussein, Efstratios Gavves, and Arnold W.M. Smeulders. Unified embedding and metric
learning for zero-exemplar event detection. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Noureldien Hussein, Efstratios Gavves, and Arnold W.M. Smeulders. Timeception for complex action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Loc Huynh, Weikai Chen, Shunsuke Saito, Jun Xing, Koki Nagano, Andrew Jones, Paul Debevec,
and Hao Li. Mesoscopic facial geometry inference using deep neural networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jyh-Jing Hwang, Tsung-Wei Ke, Jianbo Shi, and Stella X. Yu. Adversarial structure matching for
structured prediction tasks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Je Hyeong Hong, Christopher Zach, and Andrew Fitzgibbon. Revisiting the variable projection
method for separable nonlinear least squares problems. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
55

175

Published as a conference paper at SIGBOVIK 2020

Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, and Kurt Keutzer. Firecaffe: Nearlinear acceleration of deep neural network training on compute clusters. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Mostafa S. Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat, and Greg Mori. A hierarchical deep temporal model for group activity recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Residual expansion algorithm: Fast and
effective optimization for nonconvex least squares problems. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Local and global optimization techniques
in graph-based clustering. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox.
Flownet 2.0: Evolution of optical flow estimation with deep networks. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Sunghoon Im, Hae-Gon Jeon, and In So Kweon. Robust depth estimation from auto bracketed
images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Saif Imran, Yunfei Long, Xiaoming Liu, and Daniel Morris. Depth coefficients for depth completion.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Atul Ingle, Andreas Velten, and Mohit Gupta. High flux passive imaging with single-photon sensors.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Nathan Inkawhich, Wei Wen, Hai (Helen) Li, and Yiran Chen. Feature space perturbations yield
more transferable adversarial examples. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiyoharu Aizawa. Cross-domain weaklysupervised object detection through progressive domain adaptation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang, Evgeny Levinkov, Bjoern
Andres, and Bernt Schiele. Arttrack: Articulated multi-person tracking in the wild. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yani Ioannou, Duncan Robertson, Roberto Cipolla, and Antonio Criminisi. Deep roots: Improving
cnn efficiency with hierarchical filter groups. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Radu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana Georgescu, and Ling Shao. Objectcentric auto-encoders and dummy anomalies for abnormal event detection in video. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Umar Iqbal, Anton Milan, and Juergen Gall. Posetrack: Joint multi-person pose estimation and
tracking. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Hossam Isack, Olga Veksler, Milan Sonka, and Yuri Boykov. Hedgehog shape priors for multi-object
segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Hossam Isack, Olga Veksler, Ipek Oguz, Milan Sonka, and Yuri Boykov. Efficient optimization
for hierarchically-structured interacting segments (hints). In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
56

176

Published as a conference paper at SIGBOVIK 2020

Ahmet Iscen, Michael Rabbat, and Teddy Furon. Efficient large-scale similarity search using matrix
factorization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, and Ondrej Chum. Efficient diffusion
on region manifolds: Recovering small objects with compact cnn representations. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Ahmet Iscen, Yannis Avrithis, Giorgos Tolias, Teddy Furon, and Ondřej Chum. Fast spectral ranking
for similarity search. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep
semi-supervised learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Takahiro Isokane, Fumio Okura, Ayaka Ide, Yasuyuki Matsushita, and Yasushi Yagi. Probabilistic
plant modeling via multi-view image-to-image translation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Vamsi K. Ithapu, Risi Kondor, Sterling C. Johnson, and Vikas Singh. The incremental multiresolution matrix factorization algorithm. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Eisuke Ito and Takayuki Okatani. Self-calibration-based approach to critical motion sequences of
rolling-shutter structure from motion. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Boyd-Graber, Hal Daume,
III, and Larry S. Davis. The amazing mysteries of the gutter: Drawing inferences between panels
in comic book narratives. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Hamid Izadinia, Qi Shan, and Steven M. Seitz. Im2cad. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Dominic Jack, Frederic Maire, Sareh Shirazi, and Anders Eriksson. Ige-net: Inverse graphics energy
networks for human pose estimation and single-view reconstruction. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Jorn-Henrik Jacobsen, Jan van Gemert, Zhongyu Lou, and Arnold W. M. Smeulders. Structured
receptive fields in cnns. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Seong Jae Hwang, Nagesh Adluru, Maxwell D. Collins, Sathya N. Ravi, Barbara B. Bendlin, Sterling C. Johnson, and Vikas Singh. Coupled harmonic bases for longitudinal characterization of
brain networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Seong Jae Hwang, Sathya N. Ravi, Zirui Tao, Hyunwoo J. Kim, Maxwell D. Collins, and Vikas
Singh. Tensorize, factorize and regularize: Robust visual relationship learning. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
57

177

Published as a conference paper at SIGBOVIK 2020

Mariano Jaimez, Thomas J. Cashman, Andrew Fitzgibbon, Javier Gonzalez-Jimenez, and Daniel
Cremers. An efficient background term for 3d reconstruction and tracking with smooth surface
models. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Himalaya Jain, Joaquin Zepeda, Patrick Pérez, and Rémi Gribonval. Learning a complete image
indexing pipeline. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander G. Schwing, and Aniruddha Kembhavi. Two body problem: Collaborative visual task completion. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Ayush Jaiswal, Yue Wu, Wael AbdAlmageed, Iacopo Masi, and Premkumar Natarajan. Aird: Adversarial learning framework for image repurposing detection. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Muhammad Abdullah Jamal and Guo-Jun Qi. Task agnostic meta-learning for few-shot learning. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz,
Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis. Sim-to-real via sim-to-sim: Dataefficient robotic grasping via randomized-to-canonical adaptation networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Varun Jampani, Martin Kiefel, and Peter V. Gehler. Learning sparse high dimensional filters: Image
filtering, dense crfs and bilateral neural networks. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Varun Jampani, Raghudeep Gadde, and Peter V. Gehler. Video propagation networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Joel Janai, Fatma Guney, Jonas Wulff, Michael J. Black, and Andreas Geiger. Slow flow: Exploiting
high-speed cameras for accurate and diverse optical flow reference data. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Won-Dong Jang and Chang-Su Kim. Interactive image segmentation via backpropagating refinement scheme. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Won-Dong Jang, Chulwoo Lee, and Chang-Su Kim. Primary object segmentation in videos via alternate convex optimization of foreground and background distributions. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatiotemporal reasoning in visual question answering. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Dinesh Jayaraman and Kristen Grauman. Slow and steady feature analysis: Higher order temporal coherence in video. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Dinesh Jayaraman and Kristen Grauman. Learning to look around: Intelligently exploring unseen environments for unknown tasks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Simon Jenni and Paolo Favaro. Self-supervised feature learning by learning to spot artifacts. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Simon Jenni and Paolo Favaro. On stabilizing generative adversarial training with noise. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
58

178

Published as a conference paper at SIGBOVIK 2020

Daniel S. Jeon, Seung-Hwan Baek, Inchang Choi, and Min H. Kim. Enhancing the spatial resolution
of stereo images using a parallax prior. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Hae-Gon Jeon, Joon-Young Lee, Sunghoon Im, Hyowon Ha, and In So Kweon. Stereo matching with color and monochrome cameras in low-light conditions. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Yunho Jeon and Junmo Kim. Active convolution: Learning the shape of convolution for image
classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Yeonwoo Jeong, Yoonsung Kim, and Hyun Oh Song. End-to-end efficient representation learning
via cascading combinatorial optimization. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Saumya Jetley, Naila Murray, and Eleonora Vig. End-to-end saliency mapping via probability distribution prediction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Saumya Jetley, Michael Sapienza, Stuart Golodetz, and Philip H. S. Torr. Straight to shapes: Realtime detection of encoded shapes. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Roy J. Jevnisek and Shai Avidan. Co-occurrence filter. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Dinghuang Ji, Junghyun Kwon, Max McFarland, and Silvio Savarese. Deep view morphing. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Pan Ji, Hongdong Li, Mathieu Salzmann, and Yiran Zhong. Robust multi-body feature tracker: A
segmentation-free approach. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Kui Jia, Dacheng Tao, Shenghua Gao, and Xiangmin Xu. Improving training of deep neural networks via singular value bounding. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Songhao Jia, Ding-Jie Chen, and Hwann-Tzong Chen. Instance-level meta normalization. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, and Jan Kautz.
Super slomo: High quality estimation of multiple intermediate frames for video interpolation. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Qing-Yuan Jiang and Wu-Jun Li. Deep cross-modal hashing. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xiantong Zhen, Xianbin Cao, David Doermann, and
Ling Shao. Crowd counting and density estimation by trellis encoder-decoder networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Jianbo Jiao, Yunchao Wei, Zequn Jie, Honghui Shi, Rynson W.H. Lau, and Thomas S. Huang.
Geometry-aware distillation for indoor semantic segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Zequn Jie, Yunchao Wei, Xiaojie Jin, Jiashi Feng, and Wei Liu. Deep self-taught learning for
weakly supervised object localization. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Zequn Jie, Pengfei Wang, Yonggen Ling, Bo Zhao, Yunchao Wei, Jiashi Feng, and Wei Liu. Leftright comparative recurrent model for stereo matching. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
59

179

Published as a conference paper at SIGBOVIK 2020

Meiguang Jin, Stefan Roth, and Paolo Favaro. Noise-blind image deblurring. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Meiguang Jin, Givi Meishvili, and Paolo Favaro. Learning to extract a video sequence from a single
motion-blurred image. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Sheng Jin, Wentao Liu, Wanli Ouyang, and Chen Qian. Multi-person articulated tracking with
spatial and temporal embeddings. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Younghyun Jo, Seoung Wug Oh, Jaeyeon Kang, and Seon Joo Kim. Deep video super-resolution
network using dynamic upsampling filters without explicit motion compensation. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Ole Johannsen, Antonin Sulc, and Bastian Goldluecke. What sparse light field coding reveals about
scene structure. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Joakim Johnander, Martin Danelljan, Emil Brissman, Fahad Shahbaz Khan, and Michael Felsberg.
A generative appearance model for end-to-end video object segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Edward Johns, Stefan Leutenegger, and Andrew J. Davison. Pairwise decomposition of image sequences for active multi-view recognition. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and
Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual
reasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Nick Johnston, Damien Vincent, David Minnen, Michele Covell, Saurabh Singh, Troy Chinen, Sung
Jin Hwang, Joel Shor, and George Toderici. Improved lossy image compression with priming and
spatially adaptive bit rates for recurrent networks. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
DongGyu Joo, Doyeon Kim, and Junmo Kim. Generating a fusion image: One’s identity and
another’s shape. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Hanbyul Joo, Tomas Simon, Mina Cikara, and Yaser Sheikh. Towards social artificial intelligence:
Nonverbal social signal prediction in a triadic interaction. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Kyungdon Joo, Tae-Hyun Oh, Junsik Kim, and In So Kweon. Globally optimal manhattan frame
estimation in real-time. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Seong Joon Oh, Rodrigo Benenson, Anna Khoreva, Zeynep Akata, Mario Fritz, and Bernt Schiele.
Exploiting saliency for object segmentation from image level labels. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
David Joseph Tan, Thomas Cashman, Jonathan Taylor, Andrew Fitzgibbon, Daniel Tarlow, Sameh
Khamis, Shahram Izadi, and Jamie Shotton. Fits like a glove: Rapid and reliable hand shape
personalization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
60

180

Published as a conference paper at SIGBOVIK 2020

Ajjen Joshi, Soumya Ghosh, Margrit Betke, Stan Sclaroff, and Hanspeter Pfister. Personalizing
gesture recognition using hierarchical bayesian neural networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Amin Jourabloo and Xiaoming Liu. Large-pose face alignment via cnn-based dense 3d model fitting.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Fujiao Ju, Yanfeng Sun, Junbin Gao, Simeng Liu, Yongli Hu, and Baocai Yin. Mixture of bilateralprojection two-dimensional probabilistic principal component analysis. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios Savvides. Local binary convolutional neural
networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios Savvides. Perturbative neural networks. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Florian Jug, Evgeny Levinkov, Corinna Blasse, Eugene W. Myers, and Bjoern Andres. Moral lineage
tracing. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Yeong Jun Koh and Chang-Su Kim. Primary object segmentation in videos based on region augmentation and reduction. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Yeong Jun Koh, Won-Dong Jang, and Chang-Su Kim. Pod: Discovering primary objects in videos
based on evolutionary refinement of object recurrence, background, and primary object models.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Jake Zhao (Junbo) and Kyunghyun Cho. Retrieval-augmented convolutional neural networks against
adversarial examples. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju
Hwang, and Changkyu Choi. Learning to quantize deep networks by optimizing quantization
intervals with task loss. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Felix Järemo Lawin, Martin Danelljan, Fahad Shahbaz Khan, Per-Erik Forssén, and Michael Felsberg. Density adaptive point set registration. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Achuta Kadambi, Jamie Schiel, and Ramesh Raskar. Macroscopic interferometry: Rethinking depth
estimation with frequency-domain time-of-flight. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Kushal Kafle and Christopher Kanan. Answer-type prediction for visual question answering. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Mahdi M. Kalayeh, Boqing Gong, and Mubarak Shah. Improving facial attribute prediction using
semantic segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Mahdi M. Kalayeh, Emrah Basaran, Muhittin Gökmen, Mustafa E. Kamasak, and Mubarak Shah.
Human semantic parsing for person re-identification. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
61

181

Published as a conference paper at SIGBOVIK 2020

Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, and Siddhartha Chaudhuri. 3d shape
segmentation with projective convolutional networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Irene Kaltenmark, Benjamin Charlier, and Nicolas Charon. A general framework for curve and surface comparison and registration with oriented varifolds. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, and Eric P. Xing.
Rethinking knowledge graph propagation for zero-shot learning. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Meina Kan, Shiguang Shan, and Xilin Chen. Multi-view deep network for cross-view classification.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Angjoo Kanazawa, David W. Jacobs, and Manmohan Chandraker. Warpnet: Weakly supervised
matching for single-view reconstruction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of
human shape and pose. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. Learning 3d human dynamics
from video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Geometric robustness of deep
networks: Analysis and improvement. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Atsushi Kanehira and Tatsuya Harada. Multi-label ranking from positive and unlabeled data. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Atsushi Kanehira, Luc Van Gool, Yoshitaka Ushiku, and Tatsuya Harada. Viewpoint-aware video
summarization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Atsushi Kanehira, Kentaro Takemoto, Sho Inayoshi, and Tatsuya Harada. Multimodal explanations
by predicting counterfactuality in videos. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Takuhiro Kaneko, Kaoru Hiramatsu, and Kunio Kashino. Generative attribute controller with conditional filtered generative adversarial networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Takuhiro Kaneko, Kaoru Hiramatsu, and Kunio Kashino. Generative adversarial image synthesis
with decision tree latent controller. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Takuhiro Kaneko, Yoshitaka Ushiku, and Tatsuya Harada. Label-noise robust generative adversarial
networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G. Hauptmann. Contrastive adaptation network
for unsupervised domain adaptation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
62

182

Published as a conference paper at SIGBOVIK 2020

Kai Kang, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. Object detection from video tubelets
with convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie Yan, Xihui Liu, and Xiaogang Wang.
Object detection in videos with tubelet proposal networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Hariprasad Kannan, Nikos Komodakis, and Nikos Paragios. Newton-type methods for inference in
higher-order markov random fields. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Amlan Kar, Nishant Rai, Karan Sikka, and Gaurav Sharma. Adascan: Adaptive scan pooling in deep
convolutional neural networks for human action recognition in videos. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Nour Karessli, Zeynep Akata, Bernt Schiele, and Andreas Bulling. Gaze embeddings for zeroshot image classification. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Nikolaos Karianakis, Jingming Dong, and Stefano Soatto. An empirical evaluation of current convolutional architectures’ ability to manage nuisance location and scale variability. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Leonid Karlinsky, Joseph Shtok, Yochay Tzur, and Asaf Tzadok. Fine-grained recognition of thousands of object categories with single-example training. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Leonid Karlinsky, Joseph Shtok, Sivan Harary, Eli Schwartz, Amit Aides, Rogerio Feris, Raja
Giryes, and Alex M. Bronstein. Repmet: Representative-based metric learning for classification and few-shot object detection. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Ugur Kart, Alan Lukezic, Matej Kristan, Joni-Kristian Kamarainen, and Jiri Matas. Object tracking
by reconstruction with view-specific discriminative correlation filters. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yoni Kasten, Amnon Geifman, Meirav Galun, and Ronen Basri. Gpsfm: Global projective sfm using
algebraic constraints on multi-view fundamental matrices. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Rotal Kat, Roy Jevnisek, and Shai Avidan. Matching pixels using co-occurrence statistics. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Hiroharu Kato and Tatsuya Harada. Learning view priors for single-view 3d reconstruction. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Hiroyuki Kayaba and Yuji Kokumai. Non-contact full field vibration measurement based on phaseshifting. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin
Choi, and Siddhartha Srinivasa. Tactical rewind: Self-correction via backtracking in visionand-language navigation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
63

183

Published as a conference paper at SIGBOVIK 2020

Wei Ke, Jie Chen, Jianbin Jiao, Guoying Zhao, and Qixiang Ye. Srn: Side-output residual network
for object symmetry detection in the wild. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Wadim Kehl, Federico Tombari, Slobodan Ilic, and Nassir Navab. Real-time 3d model tracking in
color and depth on a single cpu core. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Michel Keller, Zetao Chen, Fabiola Maffra, Patrik Schmuck, and Margarita Chli. Learning deep
descriptors with scale-aware triplet networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh
Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal
machine comprehension. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Ira Kemelmacher-Shlizerman, Steven M. Seitz, Daniel Miller, and Evan Brossard. The megaface
benchmark: 1 million faces for recognition at scale. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Alex Kendall and Roberto Cipolla. Geometric loss functions for camera pose regression with deep
learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses
for scene geometry and semantics. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Rohit Keshari, Mayank Vatsa, Richa Singh, and Afzel Noore. Learning structure and strength of cnn
filters for small sample size training. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Naeemullah Khan and Ganesh Sundaramoorthi. Learned shape-tailored descriptors for segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Naeemullah Khan, Byung-Woo Hong, Anthony Yezzi, and Ganesh Sundaramoorthi. Coarse-to-fine
segmentation with shape-tailored continuum scale spaces. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right
balance with uncertainty. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Anna Khoreva, Rodrigo Benenson, Mohamed Omran, Matthias Hein, and Bernt Schiele. Weakly
supervised object boundaries. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt Schiele. Simple does it:
Weakly supervised instance and semantic segmentation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Valentin Khrulkov and Ivan Oseledets. Art of singular vectors and universal adversarial perturbations. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
D. Khue Le-Huu and Nikos Paragios. Alternating direction graph matching. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yuka Kihara, Matvey Soloviev, and Tsuhan Chen. In the shadows, shape priors shine: Using occlusion to improve multi-region segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
64

184

Published as a conference paper at SIGBOVIK 2020

Edward Kim, Darryl Hannan, and Garrett Kenyon. Deep sparse coding for invariant multimodal
halle berry neurons. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-recursive convolutional network for
image super-resolution. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Jongmin Kim, Taesup Kim, Sungwoong Kim, and Chang D. Yoo. Edge-labeling graph neural network for few-shot learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Jongyoo Kim and Sanghoon Lee. Deep learning of human visual sensitivity in image quality assessment framework. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Vamsi Kiran Adhikarla, Marek Vinkler, Denis Sumin, Rafal K. Mantiuk, Karol Myszkowski, HansPeter Seidel, and Piotr Didyk. Towards a quality metric for dense light fields. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bogdan Savchynskyy, and Carsten Rother.
Instancecut: From edges to instances with multicut. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollar. Panoptic feature pyramid networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Benjamin Klein and Lior Wolf. End-to-end supervised product quantization for image search and
retrieval. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Netanel Kligler, Sagi Katz, and Ayellet Tal. Document enhancement using visibility detection. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Idan Kligvasser, Tamar Rott Shaham, and Tomer Michaeli. xunit: Learning a spatial activation
function for efficient image restoration. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Patrick Knobelreiter, Christian Reinbacher, Alexander Shekhovtsov, and Thomas Pock. End-to-end
training of hybrid cnn-crf models for stereo. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Takumi Kobayashi. Structured feature similarity with explicit feature map. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Takumi Kobayashi. Analyzing filters toward efficient convnet. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Muhammed Kocabas, Salih Karagoz, and Emre Akbas. Self-supervised learning of 3d human pose
using multi-view geometry. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, and Daniele Panozzo. Abc: A big cad model dataset for geometric
deep learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Elyor Kodirov, Tao Xiang, and Shaogang Gong. Semantic autoencoder for zero-shot learning. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Filippos Kokkinos and Stamatis Lefkimmiatis. Iterative residual cnns for burst photography applications. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
65

185

Published as a conference paper at SIGBOVIK 2020

Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-, and
high-level vision using diverse datasets and limited memory. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Amir Kolaman, Maxim Lvov, Rami Hagege, and Hugo Guterman. Amplitude modulated video
camera - light separation in dynamic scenes. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich. Style transfer by relaxed optimal
transport and self-similarity. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Oscar Koller, Hermann Ney, and Richard Bowden. Deep hand: How to train a cnn on 1 million hand
images when your data is continuous and weakly labelled. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Oscar Koller, Sepehr Zargaran, and Hermann Ney. Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent cnn-hmms. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Nikos Kolotouros, Georgios Pavlakos, and Kostas Daniilidis. Convolutional mesh regression for
single-image human shape reconstruction. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Soheil Kolouri, Yang Zou, and Gustavo K. Rohde. Sliced wasserstein kernels for probability distributions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Soheil Kolouri, Gustavo K. Rohde, and Heiko Hoffmann. Sliced wasserstein distance for learning
gaussian mixture models. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Artem Komarichev, Zichun Zhong, and Jing Hua. A-cnn: Annularly convolutional neural networks
on point clouds. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Shu Kong and Charless Fowlkes. Low-rank bilinear pooling for fine-grained classification. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Shu Kong and Charless C. Fowlkes. Recurrent scene parsing with perspective understanding in the
loop. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Tao Kong, Anbang Yao, Yurong Chen, and Fuchun Sun. Hypernet: Towards accurate region proposal generation and joint object detection. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Piotr Koniusz and Anoop Cherian. Sparse coding for third-order super-symmetric tensor descriptors
with application to texture recognition. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Piotr Koniusz, Yusuf Tas, and Fatih Porikli. Domain adaptation by mixture of alignments of secondor higher-order scatter tensors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Piotr Koniusz, Hongguang Zhang, and Fatih Porikli. A deeper look at power normalizations. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Theodora Kontogianni, Markus Mathias, and Bastian Leibe. Incremental object discovery in timevarying image collections. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
66

186

Published as a conference paper at SIGBOVIK 2020

Ksenia Konyushkova, Jasper Uijlings, Christoph H. Lampert, and Vittorio Ferrari. Learning intelligent dialogs for bounding box annotation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Jari Korhonen. Assessing personally perceived image quality via image features and collaborative
filtering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Simon Korman, Mark Milam, and Stefano Soatto. Oatm: Occlusion aware template matching by
consensus set maximization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better imagenet models transfer better? In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Adam Kortylewski, Aleksander Wieczorek, Mario Wieser, Clemens Blumer, Sonali Parbhoo, Andreas Morel-Forster, Volker Roth, and Thomas Vetter. Greedy structure learning of hierarchical
compositional models. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Pawel Korus and Nasir Memon. Content authentication for neural imaging pipelines: End-to-end
optimization of photo provenance in complex distribution channels. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Jean Kossaifi, Linh Tran, Yannis Panagakis, and Maja Pantic. Gagan: Geometry-aware generative
adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos, and Maja Pantic. T-net: Parametrizing fully
convolutional nets with a single high-order tensor. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Ronak Kosti, Jose M. Alvarez, Adria Recasens, and Agata Lapedriza. Emotion recognition in context. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Ilya Kostrikov, Zhongshi Jiang, Daniele Panozzo, Denis Zorin, and Joan Bruna. Surface networks.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Manikanta Kotaru and Sachin Katti. Position tracking for virtual reality using commodity wifi. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Dmytro Kotovenko, Artsiom Sanakoyeu, Pingchuan Ma, Sabine Lang, and Bjorn Ommer. A content
transformation block for image style transfer. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Satwik Kottur, Ramakrishna Vedantam, Jose M. F. Moura, and Devi Parikh. Visual word2vec (visw2v): Learning visually grounded word embeddings using abstract scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Balazs Kovacs, Sean Bell, Noah Snavely, and Kavita Bala. Shading annotations in the wild. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Koji Koyamatsu, Daichi Hidaka, Takahiro Okabe, and Hendrik P. A. Lensch. Reflective and fluorescent separation under narrow-band illumination. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Jedrzej Kozerawski and Matthew Turk. Clear: Cumulative learning for one-shot one-class image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kannan, Suchendra Bhandarkar, Wojciech
Matusik, and Antonio Torralba. Eye tracking for everyone. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
67

187

Published as a conference paper at SIGBOVIK 2020

Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. A hierarchical approach for generating descriptive image paragraphs. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi. Pifpaf: Composite fields for human pose
estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Ranjay Krishna, Ines Chami, Michael Bernstein, and Li Fei-Fei. Referring relationships. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. Information maximizing visual question generation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Alexander Krull, Eric Brachmann, Sebastian Nowozin, Frank Michel, Jamie Shotton, and Carsten
Rother. Poseagent: Budget-constrained 6d object pose estimation via reinforcement learning. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void - learning denoising from single noisy images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Srinivas S. S. Kruthiventi, Vennela Gudisa, Jaley H. Dholakiya, and R. Venkatesh Babu. Saliency
unified: A deep architecture for simultaneous eye fixation prediction and salient object segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Jason Ku, Alex D. Pon, and Steven L. Waslander. Monocular 3d object detection leveraging accurate
proposals and shape reconstruction. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Jason Kuen, Zhenhua Wang, and Gang Wang. Recurrent attentional networks for saliency detection.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Jason Kuen, Xiangfei Kong, Zhe Lin, Gang Wang, Jianxiong Yin, Simon See, and Yap-Peng Tan.
Stochastic downsampling for cost-adjustable inference and improved regularization in convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Zuzana Kukelova and Viktor Larsson. Radial distortion triangulation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Zuzana Kukelova, Jan Heller, and Andrew Fitzgibbon. Efficient intersection of three quadrics and
applications in computer vision. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Zuzana Kukelova, Joe Kileel, Bernd Sturmfels, and Tomas Pajdla. A clever elimination strategy for
efficient minimal solvers. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Anna Kukleva, Hilde Kuehne, Fadime Sener, and Jurgen Gall. Unsupervised learning of action
classes with continuous temporal embedding. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Kerviche, and Amit Ashok. Reconnet: Noniterative reconstruction of images from compressively sensed measurements. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Suryansh Kumar. Jumping manifolds: Geometry aware dense non-rigid structure from motion. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Suryansh Kumar, Anoop Cherian, Yuchao Dai, and Hongdong Li. Scalable dense non-rigid
structure-from-motion: A grassmannian perspective. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
68

188

Published as a conference paper at SIGBOVIK 2020

Vijay Kumar, Anoop Namboodiri, Manohar Paluri, and C. V. Jawahar. Pose-aware person recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Vijay Kumar B G, Gustavo Carneiro, and Ian Reid. Learning local image descriptors with deep
siamese and triplet convolutional networks by minimising global loss functions. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Soumava Kumar Roy, Zakaria Mhammedi, and Mehrtash Harandi. Geometry aware constrained
optimization techniques for deep learning. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Sudhakar Kumawat and Shanmuganathan Raman. Lp-3dcnn: Unveiling local phase in 3d convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Abhijit Kundu, Vibhav Vineet, and Vladlen Koltun. Feature space optimization for semantic video
segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Jiřı́ Matas. Deblurgan: Blind motion deblurring using conditional adversarial networks. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Vinod Kumar Kurmi, Shanu Kumar, and Vinay P. Namboodiri. Attending to discriminative certainty
for domain adaptation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Ilja Kuzborskij, Fabio Maria Carlucci, and Barbara Caputo. When naive bayes nearest neighbors
meet convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-supervised deep learning for monocular
depth map prediction. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Suha Kwak, Minsu Cho, and Ivan Laptev. Thin-slicing for pose: Learning to understand pose without explicit pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Roland Kwitt, Sebastian Hegenbart, and Marc Niethammer. One-shot learning of scene locations via
feature trajectory transfer. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Yong-Hoon Kwon and Min-Gyu Park. Predicting future frames using retrospective cycle gan. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Zorah Lahner, Emanuele Rodola, Frank R. Schmidt, Michael M. Bronstein, and Daniel Cremers.
Efficient globally optimal 2d-to-3d deformable shape matching. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Hsueh-Ying Lai, Yi-Hsuan Tsai, and Wei-Chen Chiu. Bridging stereo matching and optical flow
via spatiotemporal correspondence. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Wei-Sheng Lai, Jia-Bin Huang, Zhe Hu, Narendra Ahuja, and Ming-Hsuan Yang. A comparative
study for single image blind deblurring. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang. Deep laplacian pyramid
networks for fast and accurate super-resolution. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
69

189

Published as a conference paper at SIGBOVIK 2020

Rodney LaLonde, Dong Zhang, and Mubarak Shah. Clusternet: Detecting small objects in large
scenes by exploiting spatio-temporal information. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Michael Lam, Behrooz Mahasseni, and Sinisa Todorovic. Fine-grained recognition as hsnet search
for informative image parts. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
John Lambert, Ozan Sener, and Silvio Savarese. Deep learning under privileged information using
heteroscedastic dropout. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Shiyi Lan, Ruichi Yu, Gang Yu, and Larry S. Davis. Modeling local geometric structure of 3d point
clouds using geo-cnn. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Ziquan Lan, David Hsu, and Gim Hee Lee. Solving the perspective-2-point problem for flyingcamera photo composition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Loic Landrieu and Mohamed Boussaha. Point cloud oversegmentation with graph-structured deep
metric learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpoint graphs. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Jan-Hendrik Lange, Bjoern Andres, and Paul Swoboda. Combinatorial persistency criteria for multicut and max-cut. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Dong Lao and Ganesh Sundaramoorthi. Minimum delay moving object detection. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yizhen Lao and Omar Ait-Aider. A robust method for strong rolling shutter effects correction using
lines with automatic feature selection. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Maksim Lapin, Matthias Hein, and Bernt Schiele. Loss functions for top-k error: Analysis and
insights. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Dmitry Laptev, Nikolay Savinov, Joachim M. Buhmann, and Marc Pollefeys. Ti-pooling:
Transformation-invariant pooling for feature learning in convolutional neural networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Sebastian Lapuschkin, Alexander Binder, Gregoire Montavon, Klaus-Robert Muller, and Wojciech
Samek. Analyzing classifiers: Fisher vectors and deep neural networks. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Mans Larsson, Erik Stenborg, Lars Hammarstrand, Marc Pollefeys, Torsten Sattler, and Fredrik
Kahl. A cross-season correspondence dataset for robust semantic segmentation. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Viktor Larsson and Carl Olsson. Compact matrix factorization with dependent subspaces. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
70

190

Published as a conference paper at SIGBOVIK 2020

Viktor Larsson, Zuzana Kukelova, and Yinqiang Zheng. Camera pose estimation with unknown
principal point. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J. Black, and Peter V.
Gehler. Unite the people: Closing the loop between 3d and 2d human representations. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Stephane Lathuiliere, Remi Juge, Pablo Mesejo, Rafael Munoz-Salinas, and Radu Horaud. Deep
mixture of linear inverse regressions applied to head-pose estimation. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Emanuel Laude, Jan-Hendrik Lange, Jonas Schüpfer, Csaba Domokos, Laura Leal-Taixé, Frank R.
Schmidt, Bjoern Andres, and Daniel Cremers. Discrete-continuous admm for transductive inference in higher-order mrfs. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Marc T. Law, YaoLiang Yu, Matthieu Cord, and Eric P. Xing. Closed-form training of mahalanobis
distance for supervised clustering. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Marc T. Law, Yaoliang Yu, Raquel Urtasun, Richard S. Zemel, and Eric P. Xing. Efficient multiple
instance metric learning using weakly supervised data. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Huu Le, Tat-Jun Chin, and David Suter. Conformal surface alignment with optimal mobius search.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Huu Le, Tat-Jun Chin, and David Suter. An exact penalty method for locally convergent maximum
consensus. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Huu M. Le, Thanh-Toan Do, Tuan Hoang, and Ngai-Man Cheung. Sdrsac: Semidefinite-based
randomized approach for robust point cloud registration without correspondences. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Truc Le and Ye Duan. Pointgrid: A deep network for 3d shape understanding. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Colin Lea, Michael D. Flynn, Rene Vidal, Austin Reiter, and Gregory D. Hager. Temporal convolutional networks for action segmentation and detection. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photorealistic single image super-resolution using a generative adversarial network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Gayoung Lee, Yu-Wing Tai, and Junmo Kim. Deep saliency with encoded low level distance map
and high level features. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Jae-Han Lee, Minhyeok Heo, Kyung-Rae Kim, and Chang-Su Kim. Single-image depth estimation
based on fourier domain analysis. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
71

191

Published as a conference paper at SIGBOVIK 2020

Junghyup Lee, Dohyung Kim, Jean Ponce, and Bumsub Ham. Sfnet: Learning object-aware semantic correspondence. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B. Choy, Philip H. S. Torr, and Manmohan
Chandraker. Desire: Distant future prediction in dynamic scenes with interacting agents. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Stamatios Lefkimmiatis. Non-local color image denoising with convolutional neural networks. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Stamatios Lefkimmiatis. Universal denoising networks : A novel cnn architecture for image denoising. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, and
Paul Debevec. Deeplight: Learning illumination for unconstrained mobile mixed reality. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Chenyang Lei and Qifeng Chen. Fully automatic video colorization with self-regularization and
diversity. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, and Houqiang Li. Comparative deep learning of
hybrid representations for image recommendations. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Peng Lei, Fuxin Li, and Sinisa Todorovic. Boundary flow: A siamese network that predicts boundary
motion without training on motion. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Carl Lemaire, Andrew Achkar, and Pierre-Marc Jodoin. Structured pruning of neural networks
with budget-aware regularization. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Evgeny Levinkov, Jonas Uhrig, Siyu Tang, Mohamed Omran, Eldar Insafutdinov, Alexander Kirillov, Carsten Rother, Thomas Brox, Bernt Schiele, and Bjoern Andres. Joint graph decomposition
node labeling: Problem, algorithms, applications. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Aviad Levis, Yoav Y. Schechner, and Anthony B. Davis. Multiple-scattering microphysics tomography. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Aviad Levis, Yoav Y. Schechner, and Ronen Talmon. Statistical tomography of microscopic life. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jose Lezama, Qiang Qiu, and Guillermo Sapiro. Not afraid of the dark: Nir-vis face recognition
via cross-spectral hallucination and low-rank embedding. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
José Lezama, Qiang Qiu, Pablo Musé, and Guillermo Sapiro. OlÉ: Orthogonal low-rank embedding
- a plug and play geometric loss for deep learning. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Ang Li, Jin Sun, Joe Yue-Hei Ng, Ruichi Yu, Vlad I. Morariu, and Larry S. Davis. Generating holistic 3d scene abstractions for text-based image retrieval. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Guanbin Li and Yizhou Yu. Deep contrast learning for salient object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Hongyang Li, David Eigen, Samuel Dodge, Matthew Zeiler, and Xiaogang Wang. Finding taskrelevant features for few-shot learning by category traversal. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
72

192

Published as a conference paper at SIGBOVIK 2020

Shuang Li, Slawomir Bak, Peter Carr, and Xiaogang Wang. Diversity regularized spatiotemporal
attention for video-based person re-identification. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Dongze Lian, Jing Li, Jia Zheng, Weixin Luo, and Shenghua Gao. Density map regression guided
detection network for rgb-d crowd counting and localization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Jian Liang, Ran He, Zhenan Sun, and Tieniu Tan. Distant supervised centroid shift: A simple and
efficient approach to visual domain adaptation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Zequn Jie, Jiashi Feng, Liang Lin, and Shuicheng Yan.
Reversible recursive instance-level object segmentation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Xiaodan Liang, Lisa Lee, and Eric P. Xing. Deep variation-structured reinforcement learning for
visual relationship and attribute detection. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Xiaodan Liang, Hongfei Zhou, and Eric Xing. Dynamic-structured semantic propagation network.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against
adversarial attacks using high-level representation guided denoiser. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Yuan-Hong Liao, Xavier Puig, Marko Boben, Antonio Torralba, and Sanja Fidler. Synthesizing
environment-aware activities via activity sketches. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, and Andrei Bursuc. Dense classification and implanting for few-shot learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Ivan Lillo, Juan Carlos Niebles, and Alvaro Soto. A hierarchical pose-based approach to complex
action understanding using dictionaries of actionlets and motion poselets. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Chen-Hsuan Lin, Oliver Wang, Bryan C. Russell, Eli Shechtman, Vladimir G. Kim, Matthew Fisher,
and Simon Lucey. Photometric mesh optimization for video-aligned 3d object reconstruction. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Chung-Ching Lin and Ying Hung. A prior-less method for multi-face tracking in unconstrained
videos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Kevin Lin, Jiwen Lu, Chu-Song Chen, and Jie Zhou. Learning compact binary descriptors with
unsupervised deep neural networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Mude Lin, Liang Lin, Xiaodan Liang, Keze Wang, and Hui Cheng. Recurrent 3d pose sequence
machines. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
David B. Lindell, Gordon Wetzstein, and Vladlen Koltun. Acoustic non-line-of-sight imaging. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Huan Ling, Jun Gao, Amlan Kar, Wenzheng Chen, and Sanja Fidler. Fast interactive object annotation with curve-gcn. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
73

193

Published as a conference paper at SIGBOVIK 2020

Or Litany, Alex Bronstein, Michael Bronstein, and Ameesh Makadia. Deformable shape completion
with graph convolutional autoencoders. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Etai Littwin and Lior Wolf. The multiverse loss for robust transfer learning. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Chen Liu, Pushmeet Kohli, and Yasutaka Furukawa. Layered scene decomposition via the
occlusion-crf. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L. Yuille, and
Li Fei-Fei. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep
hypersphere embedding for face recognition. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Yaojie Liu, Amin Jourabloo, and Xiaoming Liu. Learning deep models for face anti-spoofing:
Binary or auxiliary supervision. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Alex Locher, Michal Perdoch, and Luc Van Gool. Progressive prioritized multi-view stereo. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Fotios Logothetis, Roberto Mecca, and Roberto Cipolla. Semi-calibrated near field photometric
stereo. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Suhas Lohit, Qiao Wang, and Pavan Turaga. Temporal transformer networks: Joint learning of
invariant and discriminative time warping. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Chengjiang Long and Gang Hua. Correlational gaussian processes for cross-domain visual recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Fuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo Luo, and Tao Mei. Gaussian temporal
awareness networks for action localization. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Xiang Long, Chuang Gan, Gerard de Melo, Jiajun Wu, Xiao Liu, and Shilei Wen. Attention clusters:
Purely attention based local feature integration for video classification. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Manuel Lopez, Roger Mari, Pau Gargallo, Yubin Kuang, Javier Gonzalez-Jimenez, and Gloria Haro.
Deep single image camera calibration with radial distortion. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf, and Leon Bottou. Discovering causal signals in images. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Dominik Lorenz, Leonard Bereska, Timo Milbich, and Bjorn Ommer. Unsupervised part-based
disentangling of object shape and appearance. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Or Lotan and Michal Irani. Needle-match: Reliable patch matching under high uncertainty. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Yihang Lou, Yan Bai, Jun Liu, Shiqi Wang, and Lingyu Duan. Veri-wild: A large dataset and a new
method for vehicle re-identification in the wild. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
74

194

Published as a conference paper at SIGBOVIK 2020

Cewu Lu, Hao Su, Yonglu Li, Yongyi Lu, Li Yi, Chi-Keung Tang, and Leonidas J. Guibas. Beyond holistic object recognition: Enriching image understanding with part states. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when to look: Adaptive
attention via a visual sentinel for image captioning. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Xiankai Lu, Wenguan Wang, Chao Ma, Jianbing Shen, Ling Shao, and Fatih Porikli. See more,
know more: Unsupervised video object segmentation with co-attention siamese networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yao Lu, Xue Bai, Linda Shapiro, and Jue Wang. Coherent parametric contours for interactive
video object segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala. Deep photo style transfer. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Vincent Lui, Jonathon Geeves, Winston Yii, and Tom Drummond. Efficient subpixel refinement with
symbolic linear predictors. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Alan Lukezic, Tomas Vojir, Luka Cehovin Zajc, Jiri Matas, and Matej Kristan. Discriminative
correlation filter with channel and spatial reliability. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Guibo Luo, Yuesheng Zhu, Zhaotian Li, and Liming Zhang. A hole filling approach based on
background reconstruction for view synthesis in 3d video. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, and Yi Yang. Taking a closer look at domain shift:
Category-level adversaries for semantics consistent domain adaptation. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yue Luo, Jimmy Ren, Mude Lin, Jiahao Pang, Wenxiu Sun, Hongsheng Li, and Liang Lin. Single
view stereo matching. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Zelun Luo, Boya Peng, De-An Huang, Alexandre Alahi, and Li Fei-Fei. Unsupervised learning of
long-term motion dynamics for videos. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Diogo C. Luvizon, David Picard, and Hedi Tabia. 2d/3d pose estimation and action recognition using
multitask deep learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Jiangjing Lv, Xiaohu Shao, Junliang Xing, Cheng Cheng, and Xi Zhou. A deep regression architecture with two-stage re-initialization for high performance facial landmark detection. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Jianming Lv, Weihang Chen, Qing Li, and Can Yang. Unsupervised cross-dataset person reidentification by transfer learning of spatial-temporal patterns. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Zhaoyang Lv, Frank Dellaert, James M. Rehg, and Andreas Geiger. Taking a deeper look at the inverse compositional algorithm. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan, and Xiang Bai. Multi-oriented scene text
detection via corner localization and region segmentation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
75

195

Published as a conference paper at SIGBOVIK 2020

Kede Ma, Qingbo Wu, Zhou Wang, Zhengfang Duanmu, Hongwei Yong, Hongliang Li, and Lei
Zhang. Group mad competition - a new methodology to compare objective image quality models.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, and Mario Fritz. Disentangled person image generation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Wei-Chiu Ma, De-An Huang, Namhoon Lee, and Kris M. Kitani. Forecasting interactive dynamics
of pedestrians with fictitious play. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Wei-Chiu Ma, Shenlong Wang, Rui Hu, Yuwen Xiong, and Raquel Urtasun. Deep rigid instance
scene flow. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Oisin Mac Aodha, Shihan Su, Yuxin Chen, Pietro Perona, and Yisong Yue. Teaching categories
to human learners with visual explanations. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Dennis Madsen, Marcel Lüthi, Andreas Schneider, and Thomas Vetter. Probabilistic joint face-skull
modelling for facial reconstruction. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Nicolas Maerki, Federico Perazzi, Oliver Wang, and Alexander Sorkine-Hornung. Bilateral space
video segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Luca Magri and Andrea Fusiello. Multiple model fitting as a set coverage problem. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Luca Magri and Andrea Fusiello. Fitting multiple heterogeneous models by multi-class cascaded
t-linkage. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal. A dataset
and exploration of models for understanding video data through fill-in-the-blank questionanswering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Behrooz Mahasseni and Sinisa Todorovic. Regularizing long short term memory with 3d humanskeleton sequences for action recognition. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic. Unsupervised video summarization with
adversarial lstm networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
M. R. Mahesh Mohan and A. N. Rajagopalan. Divide and conquer for full-resolution light field
deblurring. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Reza Mahjourian, Martin Wicke, and Anelia Angelova. Unsupervised learning of depth and egomotion from monocular video using 3d geometric constraints. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Long Mai, Hailin Jin, and Feng Liu. Composition-preserving deep photo aesthetics assessment. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Long Mai, Hailin Jin, Zhe Lin, Chen Fang, Jonathan Brandt, and Feng Liu. Spatial-semantic image
search by visual feature synthesis. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
76

196

Published as a conference paper at SIGBOVIK 2020

Michael Maire, Takuya Narihira, and Stella X. Yu. Affinity cnn: Learning pixel-centric pairwise
relations for figure/ground embedding. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Soumajit Majumder and Angela Yao. Content-aware multi-level guidance for interactive instance
segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox. Overcoming limitations of mixture
density networks: A sampling and fitting framework for multimodal future prediction. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yasushi Makihara, Atsuyuki Suzuki, Daigo Muramatsu, Xiang Li, and Yasushi Yagi. Joint intensity
and spatial metric learning for robust gait recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Andrii Maksai and Pascal Fua. Eliminating exposure bias and metric mismatch in multiple object
tracking. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Andrii Maksai, Xinchao Wang, and Pascal Fua. What players do with the ball: A physically constrained interaction modeling. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Anton Mallasto and Aasa Feragen. Wrapped gaussian process regression on riemannian manifolds.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative
pruning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Abed Malti and Cedric Herzet. Elastic shape-from-template with spatially sparse deforming forces.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Massimiliano Mancini, Lorenzo Porzi, Samuel Rota Bulò, Barbara Caputo, and Elisa Ricci. Boosting domain adaptation by discovering latent domains. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Massimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. Adagraph: Unifying predictive and continuous domain adaptation through graphs. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Devraj Mandal, Kunal N. Chaudhury, and Soma Biswas. Generalized semantic preserving hashing for n-label cross-modal retrieval. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Devraj Mandal, Sanath Narayan, Sai Kumar Dwivedi, Vikram Gupta, Shuaib Ahmed, Fahad Shahbaz Khan, and Ling Shao. Out-of-distribution detection for generalized zero-shot action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Jacques Manderscheid, Amos Sironi, Nicolas Bourdis, Davide Migliore, and Vincent Lepetit. Speed
invariant time surface for learning to detect corner points with event-based cameras. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Fabian Manhardt, Wadim Kehl, and Adrien Gaidon. Roi-10d: Monocular lifting of 2d detection to
6d pose and metric shape. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Fabio Maninchedda, Martin R. Oswald, and Marc Pollefeys. Fast 3d reconstruction of faces with
glasses. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
77

197

Published as a conference paper at SIGBOVIK 2020

Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, and Luc Van Gool. Deep extreme cut: From
extreme points to object segmentation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas Kokkinos. Attentive single-tasking of multiple tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Varun Manjunatha, Nirat Saini, and Larry S. Davis. Explicit bias discovery in visual question answering models. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Jiayuan Mao, Tete Xiao, Yuning Jiang, and Zhimin Cao. What can help pedestrian detection? In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking generative adversarial networks for diverse image synthesis. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Ana I. Maqueda, Antonio Loquercio, Guillermo Gallego, Narciso Garcı́a, and Davide Scaramuzza.
Event-based vision meets deep learning on steering prediction for self-driving cars. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Diego Marcos, Raffay Hamid, and Devis Tuia. Geospatial correspondences for multimodal registration. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Diego Marcos, Devis Tuia, Benjamin Kellenberger, Lisa Zhang, Min Bai, Renjie Liao, and Raquel
Urtasun. Learning deep structured active contours end-to-end. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Dmitrii Marin, Meng Tang, Ismail Ben Ayed, and Yuri Boykov. Beyond gradient descent for regularized segmentation losses. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Manuel J. Marin-Jimenez, Vicky Kalogeiton, Pablo Medina-Suarez, and Andrew Zisserman. Laeonet: Revisiting people looking at each other in videos. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using knowledge graphs for image classification. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual
question answering benchmark requiring external knowledge. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Elisabeta Marinoiu, Mihai Zanfir, Vlad Olaru, and Cristian Sminchisescu. 3d human sensing, action and emotion recognition in robot assisted therapy of children with autism. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Mark Marsden, Kevin McGuinness, Suzanne Little, Ciara E. Keogh, and Noel E. O’Connor. People,
penguins and petri dishes: Adapting object counting models to new visual domains and object
types without forgetting. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Julieta Martinez, Michael J. Black, and Javier Romero. On human motion prediction using recurrent
neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
78

198

Published as a conference paper at SIGBOVIK 2020

David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. Transparency by design: Closing the gap between performance and interpretability in visual reasoning. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Iacopo Masi, Stephen Rawls, Gerard Medioni, and Prem Natarajan. Pose-aware face recognition in
the wild. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Francisco Massa, Bryan C. Russell, and Mathieu Aubry. Deep exemplar 2d-3d detection by adapting
from real to rendered views. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Daniela Massiceti, N. Siddharth, Puneet K. Dokania, and Philip H.S. Torr. Flipdial: A generative
model for two-way visual dialogue. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Brian Matejek, Daniel Haehn, Haidong Zhu, Donglai Wei, Toufiq Parag, and Hanspeter Pfister.
Biologically-constrained graphs for global connectomics reconstruction. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Stefan Mathe, Aleksis Pirinen, and Cristian Sminchisescu. Reinforcement learning for visual object
detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Alexander Mathews, Lexing Xie, and Xuming He. Semstyle: Learning to generate stylised image
captions using unaligned text. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Tetsu Matsukawa, Takahiro Okabe, Einoshin Suzuki, and Yoichi Sato. Hierarchical gaussian descriptor for person re-identification. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Gellert Mattyus, Shenlong Wang, Sanja Fidler, and Raquel Urtasun. Hd maps: Fine-grained road
segmentation by parsing ground and aerial images. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy,
and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and
scene flow estimation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Niall McLaughlin, Jesus Martinez del Rincon, and Paul Miller. Recurrent convolutional network for
video-based person re-identification. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Éloi Mehr, André Lieutier, Fernando Sanchez Bermudez, Vincent Guitteny, Nicolas Thome, and
Matthieu Cord. Manifold learning in quotient spaces. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Nazanin Mehrasa, Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, and Greg Mori.
A variational auto-encoder model for stochastic point processes. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Dushyant Mehta, Kwang In Kim, and Christian Theobalt. On implicit filter level sparsity in convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Yaron Meirovitch, Lu Mi, Hayk Saribekyan, Alexander Matveev, David Rolnick, and Nir Shavit.
Cross-classification clustering: An efficient multi-object tracking technique for 3-d instance segmentation in connectomics. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
79

199

Published as a conference paper at SIGBOVIK 2020

Youssef A. Mejjati, Darren Cosker, and Kwang In Kim. Multi-task learning by maximizing statistical dependence. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Abhimitra Meka, Maxim Maximov, Michael Zollhöfer, Avishek Chatterjee, Hans-Peter Seidel,
Christian Richardt, and Christian Theobalt. Lime: Live intrinsic material estimation. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Simone Melzi, Riccardo Spezialetti, Federico Tombari, Michael M. Bronstein, Luigi Di Stefano,
and Emanuele Rodola. Gframes: Gradient-based local reference frame for 3d shape matching. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yifang Men, Zhouhui Lian, Yingmin Tang, and Jianguo Xiao. A common framework for interactive
texture transfer. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Yifang Men, Zhouhui Lian, Yingmin Tang, and Jianguo Xiao. Dyntypo: Example-based dynamic
text effects transfer. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Jingjing Meng, Hongxing Wang, Junsong Yuan, and Yap-Peng Tan. From keyframes to key objects:
Video summarization by representative object proposal selection. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Jingke Meng, Sheng Wu, and Wei-Shi Zheng. Weakly supervised person re-identification. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Conditional probability models for deep image compression. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Practical
full resolution learned lossless image compression. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Daniel Merget, Matthias Rock, and Gerhard Rigoll. Robust facial landmark detection via a fullyconvolutional local-global context network. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.
Occupancy networks: Learning 3d reconstruction in function space. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Kourosh Meshgi, Shigeyuki Oba, and Shin Ishii. Efficient diverse ensemble for discriminative cotracking. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Moustafa Meshry, Dan B. Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely,
and Ricardo Martin-Brualla. Neural rerendering in the wild. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Gregory P. Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez, and Carl K. Wellington.
Lasernet: An efficient probabilistic 3d object detector for autonomous driving. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Simone Meyer, Abdelaziz Djelouah, Brian McWilliams, Alexander Sorkine-Hornung, Markus
Gross, and Christopher Schroers. Phasenet for video frame interpolation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jian-Xun Mi, Qiankun Fu, and Weisheng Li. Adaptive class preserving representation for image
classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
80

200

Published as a conference paper at SIGBOVIK 2020

Caijing Miao, Lingxi Xie, Fang Wan, Chi Su, Hongye Liu, Jianbin Jiao, and Qixiang Ye. Sixray:
A large-scale security inspection x-ray benchmark for prohibited item discovery in overlapping
images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Xin Miao, Xiantong Zhen, Xianglong Liu, Cheng Deng, Vassilis Athitsos, and Heng Huang. Direct
shape regression networks for end-to-end face alignment. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Frank Michel, Alexander Kirillov, Eric Brachmann, Alexander Krull, Stefan Gumhold, Bogdan
Savchynskyy, and Carsten Rother. Global hypothesis generation for 6d object pose estimation. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Keisuke Midorikawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Uncalibrated photometric stereo
by stepwise optimization using principal components of isotropic brdfs. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Ondrej Miksik, Juan-Manuel Perez-Rua, Philip H. S. Torr, and Patrick Perez. Roam: A rich object
appearance model with application to rotoscoping. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Ben Mildenhall, Jonathan T. Barron, Jiawen Chen, Dillon Sharlet, Ren Ng, and Robert Carroll.
Burst denoising with kernel prediction networks. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Xiang Ming, Fangyun Wei, Ting Zhang, Dong Chen, and Fang Wen. Group sampling for scale
invariant face detection. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Breton Minnehan and Andreas Savakis. Cascaded projection: End-to-end network compression and
acceleration. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Pedro Miraldo, Francisco Eiras, and Srikumar Ramalingam. Analytical modeling of vanishing
points and curves in catadioptric cameras. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Pedro Miraldo, Surojit Saha, and Srikumar Ramalingam. Minimal solvers for mini-loop closures in
3d multi-scan alignment. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Parsa Mirdehghan, Wenzheng Chen, and Kiriakos N. Kutulakos. Optimal structured light à la carte.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Ishan Misra, C. Lawrence Zitnick, Margaret Mitchell, and Ross Girshick. Seeing through the human
reporting bias: Visual classifiers from noisy human-centric labels. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Ishan Misra, Abhinav Gupta, and Martial Hebert. From red wine to red tomato: Composition with
context. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, and Laurens van der
Maaten. Learning by asking questions. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Niluthpol Chowdhury Mithun, Sujoy Paul, and Amit K. Roy-Chowdhury. Weakly supervised video
moment retrieval from text queries. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, and Hao
Su. Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
81

201

Published as a conference paper at SIGBOVIK 2020

Zhipeng Mo, Boxin Shi, Sai-Kit Yeung, and Yasuyuki Matsushita. Radiometric calibration for
internet photo collections. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Zhipeng Mo, Boxin Shi, Feng Lu, Sai-Kit Yeung, and Yasuyuki Matsushita. Uncalibrated photometric stereo under natural illumination. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Sparsefool: A few pixels
make a big difference. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Nima Mohajerin and Mohsen Rohani. Multi-step prediction of occupancy grid maps with recurrent
neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Pritish Mohapatra, Michal Rolı́nek, C.V. Jawahar, Vladimir Kolmogorov, and M. Pawan Kumar.
Efficient optimization for rank-based loss functions. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Pavlo Molchanov, Xiaodong Yang, Shalini Gupta, Kihwan Kim, Stephen Tyree, and Jan Kautz.
Online detection and classification of dynamic hand gestures with recurrent 3d convolutional
neural network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation
for neural network pruning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Thomas Mollenhoff and Daniel Cremers. Lifting vectorial variational problems: A natural formulation based on geometric measure theory and discrete exterior calculus. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Thomas Mollenhoff, Emanuel Laude, Michael Moeller, Jan Lellmann, and Daniel Cremers.
Sublabel-accurate relaxation of nonconvex energies. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Davide Moltisanti, Sanja Fidler, and Dima Damen. Action recognition from single timestamp supervision in untrimmed videos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M.
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Kwang Moo Yi, Yannick Verdie, Pascal Fua, and Vincent Lepetit. Learning to assign orientations
to feature points. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. V2v-posenet: Voxel-to-voxel prediction
network for accurate 3d hand and human pose estimation from a single depth map. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. Posefix: Model-agnostic general human
pose refinement network. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and
accurate method to fool deep neural networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
82

202

Published as a conference paper at SIGBOVIK 2020

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via curvature regularization, and vice versa. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Romero Morais, Vuong Le, Truyen Tran, Budhaditya Saha, Moussa Mansour, and Svetha Venkatesh.
Learning regularity in skeleton trajectories for anomaly detection in videos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Francesc Moreno-Noguer. 3d human pose estimation from a single image via distance matrix regression. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Pedro Morgado and Nuno Vasconcelos. Semantically consistent regularization for zero-shot recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Pedro Morgado and Nuno Vasconcelos. Nettailor: Tuning the architecture, not just the weights. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Dustin Morley and Hassan Foroosh. Improving ransac-based segmentation through cnn encapsulation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Seyed Morteza Safdarnejad and Xiaoming Liu. Spatio-temporal alignment of non-overlapping sequences from independently panning cameras. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Agata Mosinska, Pablo Márquez-Neila, Mateusz Koziński, and Pascal Fua. Beyond the pixel-wise
loss for topology-aware delineation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Agata Mosinska-Domanska, Raphael Sznitman, Przemyslaw Glowacki, and Pascal Fua. Active
learning for delineation of curvilinear structures. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Mohammadreza Mostajabi, Michael Maire, and Gregory Shakhnarovich. Regularizing deep networks by modeling and predicting label structure. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, and Horst Bischof. Using selfcontradiction to learn confidence measures in stereo vision. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Christian Mostegel, Rudolf Prettenthaler, Friedrich Fraundorfer, and Horst Bischof. Scalable surface reconstruction from point clouds with extreme scale and density diversity. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gianfranco Doretto. Information bottleneck learning using privileged information for visual recognition. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Roozbeh Mottaghi, Hannaneh Hajishirzi, and Ali Farhadi. A task-oriented approach for costsensitive recognition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Lichao Mou, Yuansheng Hua, and Xiao Xiang Zhu. A relation-augmented fully convolutional network for semantic segmentation in aerial scenes. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Arsalan Mousavian, Dragomir Anguelov, John Flynn, and Jana Kosecka. 3d bounding box estimation using deep learning and geometry. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Guodong Mu, Di Huang, Guosheng Hu, Jia Sun, and Yunhong Wang. Led3d: A lightweight and efficient deep approach to recognizing low-quality 3d faces. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
83

203

Published as a conference paper at SIGBOVIK 2020

Franziska Mueller, Florian Bernard, Oleksandr Sotnychenko, Dushyant Mehta, Srinath Sridhar, Dan
Casas, and Christian Theobalt. Ganerated hands for real-time 3d hand tracking from monocular
rgb. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Matthias Mueller, Neil Smith, and Bernard Ghanem. Context-aware correlation filter tracking. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Lopamudra Mukherjee, Sathya N. Ravi, Jiming Peng, and Vikas Singh. A biresolution spectral
framework for product quantization. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Arun Mukundan, Giorgos Tolias, and Ondrej Chum. Explicit spatial encoding for deep local descriptors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Yusuke Mukuta and Tatsuya Harada. Kernel approximation via empirical orthogonal decomposition
for unsupervised feature learning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Jonghwan Mun, Linjie Yang, Zhou Ren, Ning Xu, and Bohyung Han. Streamlined dense video
captioning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Martin Mundt, Sagnik Majumder, Sreenivas Murali, Panagiotis Panetsos, and Visvanathan Ramesh.
Meta-learning convolutional neural architectures for multi-target concrete defect classification
with the concrete defect bridge image dataset. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Bharti Munjal, Sikandar Amin, Federico Tombari, and Fabio Galasso. Query-guided end-to-end
person search. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Sanjeev Muralikrishnan, Vladimir G. Kim, and Siddhartha Chaudhuri. Tags2parts: Discovering
semantic regions from shape tags. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Sanjeev Muralikrishnan, Vladimir G. Kim, Matthew Fisher, and Siddhartha Chaudhuri. Shape unicode: A unified shape representation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Calvin Murdock and Fernando De la Torre. Additive component analysis. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Calvin Murdock, Zhen Li, Howard Zhou, and Tom Duerig. Blockout: Dynamic model selection for
hierarchical deep networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ramamoorthi, and Kyungnam Kim. Image
to image translation for domain adaptation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Nils Murrugarra-Llerena and Adriana Kovashka. Cross-modality personalization for retrieval. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Venkatesh N. Murthy, Vivek Singh, Terrence Chen, R. Manmatha, and Dorin Comaniciu. Deep
decision network for multi-class image classification. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Armin Mustafa and Adrian Hilton. Semantically coherent co-segmentation and reconstruction of
dynamic scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
84

204

Published as a conference paper at SIGBOVIK 2020

Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, and Adrian Hilton. Temporally coherent 4d
reconstruction of complex dynamic scenes. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Arsha Nagrani, Samuel Albanie, and Andrew Zisserman. Seeing voices and hearing faces: Crossmodal biometric matching. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Seungjun Nah, Sanghyun Son, and Kyoung Mu Lee. Recurrent neural networks with intra-frame
iterations for video deblurring. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Mohammad Najafi, Sarah Taghavi Namin, Mathieu Salzmann, and Lars Petersson. Sample and
filter: Nonparametric scene parsing via efficient filtering. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Mahyar Najibi, Mohammad Rastegari, and Larry S. Davis. G-cnn: An iterative grid based object
detector. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Mahyar Najibi, Bharat Singh, and Larry S. Davis. Fa-rpn: Floating region proposals for face detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Katsuyuki Nakamura, Serena Yeung, Alexandre Alahi, and Li Fei-Fei. Jointly learning energy
expenditures and activities using egocentric multimodal signals. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Giljoo Nam, Chenglei Wu, Min H. Kim, and Yaser Sheikh. Strand-accurate multi-view hair capture.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual attention networks for multimodal reasoning and matching. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Seonghyeon Nam, Youngbae Hwang, Yasuyuki Matsushita, and Seon Joo Kim. A holistic approach
to cross-channel image noise modeling and its application to image denoising. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Pradyumna Narayana, Ross Beveridge, and Bruce A. Draper. Gesture recognition: Focus on the
hands. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Behrooz Nasihatkon, Frida Fejne, and Fredrik Kahl. Globally optimal rigid intensity based registration: A fast fourier domain approach. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Jogendra Nath Kundu, Phani Krishna Uppala, Anuj Pahuja, and R. Venkatesh Babu. Adadepth:
Unsupervised content congruent adaptation for depth estimation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
T. Nathan Mundhenk, Daniel Ho, and Barry Y. Chen. Improvements to context based self-supervised
learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Fabrizio Natola, Valsamis Ntouskos, Fiora Pirri, and Marta Sanzari. Single image object modeling
based on brdf and r-surfaces learning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
85

205

Published as a conference paper at SIGBOVIK 2020

Ryota Natsume, Shunsuke Saito, Zeng Huang, Weikai Chen, Chongyang Ma, Hao Li, and Shigeo
Morishima. Siclope: Silhouette-based clothed people. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Aaron Nech and Ira Kemelmacher-Shlizerman. Level playing field for million scale face recognition.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Vladimir Nekrasov, Hao Chen, Chunhua Shen, and Ian Reid. Fast neural architecture search of
compact semantic segmentation models via auxiliary cells. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Thomas Nestmeyer and Peter V. Gehler. Reflectance adaptive filtering improves intrinsic image
estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Davy Neven, Bert De Brabandere, Marc Proesmans, and Luc Van Gool. Instance segmentation
by jointly optimizing spatial embeddings and clustering bandwidth. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Natalia Neverova, James Thewlis, Riza Alp Guler, Iasonas Kokkinos, and Andrea Vedaldi. Slim
densepose: Thrifty learning from sparse annotations and motion cues. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug play
generative networks: Conditional iterative generation of images in latent space. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Duy-Kien Nguyen and Takayuki Okatani. Improved fusion of visual and language representations
by dense symmetric co-attention for visual question answering. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Duy-Kien Nguyen and Takayuki Okatani. Multi-task learning of hierarchical vision-language representation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Rang M. H. Nguyen and Michael S. Brown. Raw image reconstruction using a self-contained srgbjpeg image with only 64 kb overhead. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Chi Nhan Duong, Khoa Luu, Kha Gia Quach, and Tien D. Bui. Longitudinal face modeling via
temporal deep restricted boltzmann machines. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Bingbing Ni, Xiaokang Yang, and Shenghua Gao. Progressively parsing interactional objects for fine
grained action detection. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Tianwei Ni, Lingxi Xie, Huangjie Zheng, Elliot K. Fishman, and Alan L. Yuille. Elastic boundary
projection for 3d medical image segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Guang-Yu Nie, Ming-Ming Cheng, Yun Liu, Zhengfa Liang, Deng-Ping Fan, Yue Liu, and Yongtian
Wang. Multi-level context ultra-aggregation for stereo matching. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Xuecheng Nie, Jiashi Feng, Yiming Zuo, and Shuicheng Yan. Human pose estimation with parsing
induced learner. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Marc Niethammer, Roland Kwitt, and Francois-Xavier Vialard. Metric learning for image registration. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
86

206

Published as a conference paper at SIGBOVIK 2020

Yusuke Niitani, Takuya Akiba, Tommi Kerola, Toru Ogawa, Shotaro Sano, and Shuji Suzuki. Sampling techniques for large-scale object detection from sparsely annotated objects. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Simon Niklaus and Feng Liu. Context-aware synthesis for video frame interpolation. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive convolution. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
David Nilsson and Cristian Sminchisescu. Semantic video segmentation by gated recurrent flow
propagation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Jifeng Ning, Jimei Yang, Shaojie Jiang, Lei Zhang, and Ming-Hsuan Yang. Object tracking via dual
linear structured svm and explicit feature map. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Chengjie Niu, Jun Li, and Kai Xu. Im2struct: Recovering 3d shape structure from a single rgb
image. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Yulei Niu, Hanwang Zhang, Manli Zhang, Jianhong Zhang, Zhiwu Lu, and Ji-Rong Wen. Recursive visual attention in visual dialog. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. Ordinal regression with multiple
output cnn for age estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. Image question answering using convolutional neural network with dynamic parameter prediction. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Hyeonwoo Noh, Taehoon Kim, Jonghwan Mun, and Bohyung Han. Transfer learning via unsupervised task discovery for visual question answering. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Junhyug Noh, Soochan Lee, Beomsu Kim, and Gunhee Kim. Improving occlusion and hard negative
handling for single-stage pedestrian detectors. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, and Hamed Pirsiavash. Boosting self-supervised
learning via knowledge transfer. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Sotiris Nousias, Manolis Lourakis, and Christos Bergeles. Large-scale, metric structure from motion
for unordered light fields. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
David Novotny, Diane Larlus, and Andrea Vedaldi. Anchornet: A weakly supervised network to
learn geometry-sensitive features for semantic matching. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
David Novotny, Samuel Albanie, Diane Larlus, and Andrea Vedaldi. Self-supervised learning of
geometrically stable features through probabilistic introspection. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Markus Oberweger, Gernot Riegler, Paul Wohlhart, and Vincent Lepetit. Efficiently creating 3d
training data for fine hand pose estimation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Nati Ofir, Meirav Galun, Boaz Nadler, and Ronen Basri. Fast detection of curved edges at low snr.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
87

207

Published as a conference paper at SIGBOVIK 2020

Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Fast user-guided video object
segmentation by interaction-and-propagation networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Katsunori Ohnishi, Atsushi Kanehira, Asako Kanezaki, and Tatsuya Harada. Recognizing activities
of daily living with a wrist-mounted camera. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Eng-Jon Ong and Miroslaw Bober. Improved hamming distance search using variable length substrings. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Roy Or-El, Rom Hershkovitz, Aaron Wetzler, Guy Rosman, Alfred M. Bruckstein, and Ron Kimmel. Real-time depth refinement for specular objects. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Tribhuvanesh Orekondy, Mario Fritz, and Bernt Schiele. Connecting pixels to privacy and utility:
Automatic redaction of private information in images. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of
black-box models. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Marin Orsic, Ivan Kreso, Petra Bevandic, and Sinisa Segvic. In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka.
Large-scale distributed second-order optimization using kronecker-factored approximate curvature for deep convolutional neural networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Magnus Oskarsson, Kenneth Batstone, and Kalle Astrom. Trust no one: Low rank matrix factorization using hierarchical ransac. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Ali Osman Ulusoy, Michael J. Black, and Andreas Geiger. Patches, planes and probabilities: A
non-local prior for volumetric 3d reconstruction. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Ali Osman Ulusoy, Michael J. Black, and Andreas Geiger. Semantic multi-view stereo: Jointly estimating objects and voxels. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. Learning to remember: A synaptic plasticity driven framework for continual learning. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Mayu Otani, Yuta Nakashima, Esa Rahtu, and Janne Heikkila. Rethinking the evaluation of video
summaries. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Matthew O’Toole, Felix Heide, David B. Lindell, Kai Zang, Steven Diamond, and Gordon Wetzstein. Reconstructing transient images from single-photon sensors. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Tycho F.A. van der Ouderaa and Daniel E. Worrall. Reversible gans for memory-efficient image-toimage translation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Wanli Ouyang, Xiaogang Wang, Cong Zhang, and Xiaokang Yang. Factors in finetuning deep model
for object detection with long-tail distribution. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
88

208

Published as a conference paper at SIGBOVIK 2020

Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H. Adelson, and
William T. Freeman. Visually indicated sounds. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Edouard Oyallon. Building a regular decision boundary with deep networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Poojan Oza and Vishal M. Patel. C2ae: Class conditioned auto-encoder for open-set recognition. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Arghya Pal and Vineeth N. Balasubramanian. Adversarial data programming: Using gans to relax
the bottleneck of curated labeled data. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Arghya Pal and Vineeth N Balasubramanian. Zero-shot task transfer. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Dipan K. Pal, Felix Juefei-Xu, and Marios Savvides. Discriminative invariant kernel features: A
bells-and-whistles-free approach to unsupervised face recognition and pose estimation. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Sebastian Palacio, Joachim Folz, Jörn Hees, Federico Raue, Damian Borth, and Andreas Dengel.
What do deep networks like to see? In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Tobias Palmer, Kalle Astrom, and Jan-Michael Frahm. The misty three point algorithm for relative
pose. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Bowen Pan, Wuwei Lin, Xiaolin Fang, Chaoqin Huang, Bolei Zhou, and Cewu Lu. Recurrent
residual module for fast inference in videos. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Jinshan Pan, Zhe Hu, Zhixun Su, Hsin-Ying Lee, and Ming-Hsuan Yang. Soft-segmentation guided
object motion deblurring. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Jinshan Pan, Jiangxin Dong, Jimmy S. Ren, Liang Lin, Jinhui Tang, and Ming-Hsuan Yang. Spatially variant linear representation models for joint filtering. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Liyuan Pan, Yuchao Dai, Miaomiao Liu, and Fatih Porikli. Simultaneous stereo video deblurring
and scene flow estimation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Rameswar Panda, Amran Bhuiyan, Vittorio Murino, and Amit K. Roy-Chowdhury. Unsupervised
adaptive re-identification in open world dynamic camera networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Rohit Pandey, Anastasia Tkach, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Ricardo
Martin-Brualla, Andrea Tagliasacchi, George Papandreou, Philip Davidson, Cem Keskin,
Shahram Izadi, and Sean Fanello. Volumetric capture of humans with a single rgbd camera via
semi-parametric learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Bo Pang, Kaiwen Zha, Hanwen Cao, Chen Shi, and Cewu Lu. Deep rnn framework for visual
sequential applications. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Jiahao Pang, Wenxiu Sun, Chengxi Yang, Jimmy Ren, Ruichao Xiao, Jin Zeng, and Liang Lin.
Zoom and learn: Generalizing deep stereo matching to novel domains. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Pankaj Pansari and M. Pawan Kumar. Truncated max-of-convex models. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
89

209

Published as a conference paper at SIGBOVIK 2020

Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, and Vittorio Ferrari. We don’t need no
bounding-boxes: Training object class detectors using only human verification. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, and Vittorio Ferrari. Training object
class detectors with click supervision. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Dim P. Papadopoulos, Youssef Tamaazousti, Ferda Ofli, Ingmar Weber, and Antonio Torralba. How
to make a pizza: Learning a compositional layer-based gan model. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Konstantinos Papoutsakis, Costas Panagiotakis, and Antonis A. Argyros. Temporal action cosegmentation in 3d motion capture data and videos. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Shaifali Parashar, Daniel Pizarro, and Adrien Bartoli. Isometric non-rigid shape-from-motion in
linear time. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Jaesik Park, Yu-Wing Tai, Sudipta N. Sinha, and In So Kweon. Efficient and robust color consistency
for community photo collections. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
Deepsdf: Learning continuous signed distance functions for shape representation. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Jongchan Park, Joon-Young Lee, Donggeun Yoo, and In So Kweon. Distort-and-recover: Color
enhancement using deep reinforcement learning. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Paritosh Parmar and Brendan Tran Morris. What and how well you performed? a multitask learning
approach to action quality assessment. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Despoina Paschalidou, Osman Ulusoy, Carolin Schmitt, Luc Van Gool, and Andreas Geiger. Raynet:
Learning volumetric 3d reconstruction with ray potentials. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Despoina Paschalidou, Ali Osman Ulusoy, and Andreas Geiger. Superquadrics revisited: Learning
3d shape parsing beyond cuboids. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Geoffrey Pascoe, Will Maddern, Michael Tanner, Pedro Pinies, and Paul Newman. Nid-slam: Robust monocular slam using normalised information distance. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context
encoders: Feature learning by inpainting. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Deepak Pathak, Ross Girshick, Piotr Dollar, Trevor Darrell, and Bharath Hariharan. Learning features by watching objects move. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
90

210

Published as a conference paper at SIGBOVIK 2020

Badri Patro and Vinay P. Namboodiri. Differential attention for visual question answering. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Akanksha Paul, Narayanan C. Krishnan, and Prateek Munjal. Semantically aligned bias reducing
zero shot learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Mrinal K. Paul and Stergios I. Roumeliotis. Alternating-stereo vins: Observability analysis and
performance evaluation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Sujoy Paul, Jawadul H. Bappy, and Amit K. Roy-Chowdhury. Non-uniform subset selection for
active learning in structured data. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, and Kostas Daniilidis. Harvesting
multiple views for marker-less 3d human pose annotations. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas Daniilidis. Learning to estimate 3d
human pose and shape from a single color image. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from
a single image. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose estimation in video with temporal convolutions and semi-supervised training. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Wenjie Pei, Tadas Baltrusaitis, David M.J. Tax, and Louis-Philippe Morency. Temporal attentiongated model for robust sequence classification. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Wenjie Pei, Jiyuan Zhang, Xiangrong Wang, Lei Ke, Xiaoyong Shen, and Yu-Wing Tai. Memoryattended recurrent network for video captioning. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Tomer Peleg, Pablo Szekely, Doron Sabo, and Omry Sendik. Im-net for high resolution video frame
interpolation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Chong Peng, Zhao Kang, and Qiang Cheng. Subspace clustering via variance regularized ridge
regression. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Guozhu Peng and Shangfei Wang. Weakly supervised facial action unit recognition through adversarial training. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Peixi Peng, Tao Xiang, Yaowei Wang, Massimiliano Pontil, Shaogang Gong, Tiejun Huang, and
Yonghong Tian. Unsupervised cross-dataset transfer learning for person re-identification. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting
network for 6dof pose estimation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander
Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
91

211

Published as a conference paper at SIGBOVIK 2020

Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, and Alexander SorkineHornung. Learning video object segmentation from static images. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. Ocgan: One-class novelty detection using
gans with constrained latent representations. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Eduardo Perez-Pellitero, Jordi Salvador, Javier Ruiz-Hidalgo, and Bodo Rosenhahn. Psyco: Manifold span reduction for super resolution. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Juan-Manuel Perez-Rua, Tomas Crivelli, Patrick Bouthemy, and Patrick Perez. Determining occlusions from space and time image reconstructions. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Juan-Manuel Perez-Rua, Valentin Vielzeuf, Stephane Pateux, Moez Baccouche, and Frederic Jurie.
Mfas: Multimodal fusion architecture search. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Federico Pernici, Federico Bartoli, Matteo Bruni, and Alberto Del Bimbo. Memory based online
learning of deep representations from video streams. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Toby Perrett and Dima Damen. Ddlstm: Dual-domain lstm for cross-dataset action recognition. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Quang-Hieu Pham, Thanh Nguyen, Binh-Son Hua, Gemma Roig, and Sai-Kit Yeung. Jsis3d: Joint
semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multivalue conditional random fields. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Trung T. Pham, Seyed Hamid Rezatofighi, Ian Reid, and Tat-Jun Chin. Efficient point process
inference for large-scale object detection. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Jonah Philion. Fastdraw: Addressing the long tail of lane detection by adapting a sequential prediction network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Xinglin Piao, Yongli Hu, Junbin Gao, Yanfeng Sun, and Baocai Yin. Double nuclear norm based low
rank representation on grassmann manifolds for clustering. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
AJ Piergiovanni and Michael S. Ryoo. Learning latent super-events to detect multiple activities
in videos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
AJ Piergiovanni and Michael S. Ryoo. Representation flow for action recognition. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Andrea Pilzer, Stephane Lathuiliere, Nicu Sebe, and Elisa Ricci. Refine and distill: Exploiting
cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Pedro O. Pinheiro. Unsupervised domain adaptation with similarity learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Marcel Piotraschke and Volker Blanz. Automated 3d face reconstruction from multiple images using
quality measures. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
92

212

Published as a conference paper at SIGBOVIK 2020

Aleksis Pirinen and Cristian Sminchisescu. Deep reinforcement learning of region proposal networks for object detection. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter V.
Gehler, and Bernt Schiele. Deepcut: Joint subset partition and labeling for multi person pose
estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Francesco Pittaluga, Sanjeev J. Koppal, Sing Bing Kang, and Sudipta N. Sinha. Revealing scenes
by inverting structure from motion reconstructions. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Tobias Plotz and Stefan Roth. Benchmarking denoising algorithms with real photographs. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Stylianos Ploumpis, Haoyang Wang, Nick Pears, William A. P. Smith, and Stefanos Zafeiriou. Combining 3d morphable models: A large scale face-and-head model. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Bryan A. Plummer, Matthew Brown, and Svetlana Lazebnik. Enhancing video summarization via
vision-language embedding. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Matteo Poggi and Stefano Mattoccia. Learning to predict stereo reliability enforcing local consistency of confidence maps. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Matteo Poggi, Davide Pallotti, Fabio Tosi, and Stefano Mattoccia. Guided stereo matching. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Tobias Pohlen, Alexander Hermans, Markus Mathias, and Bastian Leibe. Full-resolution residual
networks for semantic segmentation in street scenes. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Georg Poier, David Schinagl, and Horst Bischof. Learning pose specific representations by predicting different views. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Alex Poms, Chenglei Wu, Shoou-I Yu, and Yaser Sheikh. Learning patch reconstructability for
accelerating multi-view stereo. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Alin-Ionut Popa, Mihai Zanfir, and Cristian Sminchisescu. Deep multitask architecture for integrated
2d and 3d human sensing. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Phillip E. Pope, Soheil Kolouri, Mohammad Rostami, Charles E. Martin, and Heiko Hoffmann.
Explainability methods for graph convolutional neural networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Lorenzo Porzi, Samuel Rota Bulo, Aleksander Colovic, and Peter Kontschieder. Seamless scene
segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Rafael Possas, Sheila Pinto Caceres, and Fabio Ramos. Egocentric activity recognition on a budget.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge Belongie. Generative adversarial perturbations. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Arik Poznanski and Lior Wolf. Cnn-n-gram for handwriting word recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
93

213

Published as a conference paper at SIGBOVIK 2020

Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, and James Storer. Deflecting
adversarial attacks with pixel deflection. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Aaditya Prakash, James Storer, Dinei Florencio, and Cha Zhang. Repr: Improved training of convolutional filters. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
R. T. Pramod and S. P. Arun. Do computational models differ systematically from human object
perception? In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Ekta Prashnani, Hong Cai, Yasamin Mostofi, and Pradeep Sen. Pieapp: Perceptual image-error
assessment through pairwise preference. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
True Price, Johannes L. Schönberger, Zhen Wei, Marc Pollefeys, and Jan-Michael Frahm. Augmenting crowd-sourced 3d reconstructions using semantic detections. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
James Pritts, Zuzana Kukelova, Viktor Larsson, and Ondřej Chum. Radially-distorted conjugate
translations. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Thomas Probst, Danda Pani Paudel, Ajad Chhatkuli, and Luc Van Gool. Unsupervised learning of
consensus maximization for 3d vision problems. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Hugo Proenca and Joao C. Neves. Irina: Iris recognition (even) in inaccurately segmented data. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Junfu Pu, Wengang Zhou, and Houqiang Li. Iterative alignment network for continuous sign language recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Albert Pumarola, Antonio Agudo, Lorenzo Porzi, Alberto Sanfeliu, Vincent Lepetit, and Francesc
Moreno-Noguer. Geometry-aware network for non-rigid shape prediction from a single view. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Abhijith Punnappurath and Michael S. Brown. Reflection removal using a dual-pixel sensor. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Kuldeep Purohit, Anshul Shah, and A. N. Rajagopalan. Bringing alive blurred moments. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Gilles Puy and Patrick Perez. A flexible convolutional solver for fast style transfers. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Guo-Jun Qi. Hierarchically gated deep networks for semantic segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Lu Qi, Li Jiang, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Amodal instance segmentation with kins
dataset. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
94

214

Published as a conference paper at SIGBOVIK 2020

Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia. Geonet: Geometric neural
network for joint depth and surface normal estimation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Rui Qian, Robby T. Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive generative adversarial
network for raindrop removal from a single image. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Yanlin Qian, Joni-Kristian Kamarainen, Jarno Nikkanen, and Jiri Matas. On finding gray pixels. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yiming Qian, Minglun Gong, and Yee Hong Yang. 3d reconstruction of transparent objects with
position-normal consistency. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Yiming Qian, Minglun Gong, and Yee-Hong Yang. Stereo-based 3d reconstruction of dynamic
fluid surfaces by global optimization. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, and Anton van den Hengel. Less is more: Zero-shot
learning from online textual documents with noise suppression. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille. Few-shot image recognition by predicting
parameters from activations. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Siyuan Qiao, Zhe Lin, Jianming Zhang, and Alan L. Yuille. Neural rejuvenation: Improving deep
network training by enhancing computational resource utilization. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Hongwei Qin, Junjie Yan, Xiu Li, and Xiaolin Hu. Joint training of cascaded cnn for face detection.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Jie Qin, Li Liu, Ling Shao, Bingbing Ni, Chen Chen, Fumin Shen, and Yunhong Wang. Binary
coding for partial action analysis with limited observation ratios. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand.
Basnet: Boundary-aware salient object detection. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Jiaxiong Qiu, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang, Shuaicheng Liu, Bing Zeng, and Marc
Pollefeys. Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse
lidar data and single color image. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Zhaofan Qiu, Ting Yao, and Tao Mei. Deep quantization: Encoding convolutional activations with
deep generative model. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, and Rynson W. H. Lau. Deshadownet: A multi-context embedding deep network for shadow removal. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yanyun Qu, Yizi Chen, Jingying Huang, and Yuan Xie. Enhanced pix2pix dehazing network. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Ying Qu, Hairong Qi, and Chiman Kwan. Unsupervised sparse dirichlet-net for hyperspectral image
super-resolution. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
95

215

Published as a conference paper at SIGBOVIK 2020

Novi Quadrianto, Viktoriia Sharmanska, and Oliver Thomas. Discovering fair representations in
the data domain. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Yuhui Quan, Chenglong Bao, and Hui Ji. Equiangular kernel dictionary learning with applications to
dynamic texture analysis. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Ha Quang Minh, Marco San Biagio, Loris Bazzani, and Vittorio Murino. Approximate log-hilbertschmidt distances between covariance operators for image classification. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Yvain Queau, Roberto Mecca, and Jean-Denis Durou. Unbiased photometric stereo for colored
surfaces: A variational approach. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Yvain Queau, Tao Wu, Francois Lauze, Jean-Denis Durou, and Daniel Cremers. A non-convex
variational approach to photometric stereo under inaccurate lighting. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Thi Quynh Nhi Tran, Herve Le Borgne, and Michel Crucianu. Aggregating image and text quantized
correlated components. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Yaadhav Raaj, Haroon Idrees, Gines Hidalgo, and Yaser Sheikh. Efficient online multi-person 2d
pose tracking with recurrent spatio-temporal affinity fields. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Mahdi Rad, Markus Oberweger, and Vincent Lepetit. Feature mapping for learning fast and accurate
3d pose inference from synthetic images. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Filip Radenovic, Johannes L. Schonberger, Dinghuang Ji, Jan-Michael Frahm, Ondrej Chum, and
Jiri Matas. From dusk till dawn: Modeling in the dark. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, and Kaiming He. Data distillation:
Towards omni-supervised learning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Edward Raff, Jared Sylvester, Steven Forsyth, and Mark McLean. Barrage of random transforms for
adversarially robust defense. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Amir M. Rahimi, Raphael Ruschel, and B.S. Manjunath. Uav sensor fusion with latent-dynamic
conditional random fields in coronal plane estimation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Hossein Rahmani and Ajmal Mian. 3d action recognition from novel viewpoints. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Jathushan Rajasegaran, Vinoj Jayasundara, Sandaru Jayasekara, Hirunima Jayasekara, Suranga
Seneviratne, and Ranga Rodrigo. Deepcaps: Going deeper with capsule networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Santhosh K. Ramakrishnan, Ambar Pal, Gaurav Sharma, and Anurag Mittal. An empirical evaluation of visual question answering for novel objects. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Vignesh Ramanathan, Jonathan Huang, Sami Abu-El-Haija, Alexander Gorban, Kevin Murphy, and
Li Fei-Fei. Detecting events and key actors in multi-person videos. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
96

216

Published as a conference paper at SIGBOVIK 2020

Vasili Ramanishka, Abir Das, Jianming Zhang, and Kate Saenko. Top-down visual saliency guided
by captions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Vasili Ramanishka, Yi-Ting Chen, Teruhisa Misu, and Kate Saenko. Toward driving scene understanding: A dataset for learning driver behavior and causal reasoning. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Rene Ranftl, Vibhav Vineet, Qifeng Chen, and Vladlen Koltun. Dense monocular depth estimation
in complex dynamic scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Anurag Ranjan and Michael J. Black. Optical flow estimation using a spatial pyramid network. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun, Jonas Wulff, and
Michael J. Black. Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Yongming Rao, Dahua Lin, Jiwen Lu, and Jie Zhou. Learning globally optimized object detector via
policy gradient. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Yongming Rao, Jiwen Lu, and Jie Zhou. Spherical fractal convolutional neural networks for point
cloud recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Koteswar Rao Jerripothula, Jianfei Cai, Jiangbo Lu, and Junsong Yuan. Object co-skeletonization
with co-segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Carolina Raposo and Joao P. Barreto. Theory and practice of structure-from-motion using affine
correspondences. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Carolina Raposo and João P. Barreto. 3d registration of curves and surfaces using local differential
information. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Maheen Rashid, Xiuye Gu, and Yong Jae Lee. Interspecies knowledge transfer for facial keypoint
detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Sarah Rastegar, Mahdieh Soleymani, Hamid R. Rabiee, and Seyed Mohsen Shojaee. Mdl-cw: A
multimodal deep learning framework with cross weights. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Hareesh Ravi, Lezi Wang, Carlos Muniz, Leonid Sigal, Dimitris Metaxas, and Mubbasir Kapadia.
Show me a story: Towards coherent neural story illustration. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Sathya N. Ravi, Yunyang Xiong, Lopamudra Mukherjee, and Vikas Singh. Filter flow made practical: Massively parallel and lock-free. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Swarna K. Ravindran and Anurag Mittal. Comal: Good features to match on object boundaries. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, and Vincent Vanhoucke. Youtubeboundingboxes: A large high-precision human-annotated data set for object detection in video. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
97

217

Published as a conference paper at SIGBOVIK 2020

Henri Rebecq, Rene Ranftl, Vladlen Koltun, and Davide Scaramuzza. Events-to-video: Bringing
modern computer vision to event cameras. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl:
Incremental classifier and representation learning. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Efficient parametrization of multidomain deep neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
N. Dinesh Reddy, Minh Vo, and Srinivasa G. Narasimhan. Occlusion-net: 2d/3d occluded keypoint
localization using graph networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, and R. Venkatesh Babu. Nag: Network for
adversary generation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,
real-time object detection. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Scott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning deep representations of finegrained visual descriptions. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Krishna Regmi and Ali Borji. Cross-view image synthesis using conditional gans. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Efstratios Gavves, and Tinne Tuytelaars. Deep
reflectance maps. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Konstantinos Rematas, Ira Kemelmacher-Shlizerman, Brian Curless, and Steve Seitz. Soccer on
your tabletop. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, and Deyu Meng. Progressive image
deraining networks: A better and simpler baseline. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Zhile Ren and Erik B. Sudderth. Three-dimensional object detection and layout prediction using
clouds of oriented gradients. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Zhongzheng Ren and Yong Jae Lee. Cross-domain self-supervised multi-task feature learning using
synthetic imagery. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and Li-Jia Li. Deep reinforcement learning-based
image captioning with embedding reward. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Vijay Rengarajan, Ambasamudram N. Rajagopalan, and Rangarajan Aravind. From bows to arrows:
Rolling shutter rectification of urban scenes. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
98

218

Published as a conference paper at SIGBOVIK 2020

Vijay Rengarajan, Yogesh Balaji, and A. N. Rajagopalan. Unrolling the shutter: Cnn to correct motion distortions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical
sequence training for image captioning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
George Retsinas, Georgios Louloudis, Nikolaos Stamatopoulos, Giorgos Sfikas, and Basilis Gatos.
An alternative deep feature approach to line level keyword spotting. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Jerome Revaud, Minhyeok Heo, Rafael S. Rezende, Chanmi You, and Seong-Gyun Jeong. Did it
change? learning to detect point-of-interest changes for proactive map updates. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Morteza Rezanejad, Gabriel Downs, John Wilder, Dirk B. Walther, Allan Jepson, Sven Dickinson,
and Kaleem Siddiqi. Scene categorization from contours: Medial axis based salience measures.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese.
Generalized intersection over union: A metric and a loss for bounding box regression. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Rafael S. Rezende, Joaquin Zepeda, Jean Ponce, Francis Bach, and Patrick Perez. Kernel squareloss exemplar machines for image retrieval. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Nicholas Rhinehart and Kris M. Kitani. Learning action maps of large environments via first-person
vision. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Helge Rhodin, Jörg Spörri, Isinsu Katircioglu, Victor Constantin, Frédéric Meyer, Erich Müller,
Mathieu Salzmann, and Pascal Fua. Learning monocular 3d human pose estimation from multiview images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Helge Rhodin, Victor Constantin, Isinsu Katircioglu, Mathieu Salzmann, and Pascal Fua. Neural
scene decomposition for multi-person motion capture. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Umar Riaz Muhammad, Yongxin Yang, Yi-Zhe Song, Tao Xiang, and Timothy M. Hospedales.
Learning deep sketch abstraction. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Javier Ribera, David Guera, Yuhao Chen, and Edward J. Delp. Locating objects without bounding
boxes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Alexander Richard and Juergen Gall. Temporal action detection using a statistical language model.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Alexander Richard, Hilde Kuehne, and Juergen Gall. Weakly supervised action learning with rnn
based fine-to-coarse modeling. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Alexander Richard, Hilde Kuehne, and Juergen Gall. Action sets: Weakly supervised action segmentation without ordering constraints. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Elad Richardson, Matan Sela, Roy Or-El, and Ron Kimmel. Learning detailed face reconstruction
from a single image. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Stephan R. Richter and Stefan Roth. Matryoshka networks: Predicting 3d geometry via nested shape
layers. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
99

219

Published as a conference paper at SIGBOVIK 2020

Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at
high resolutions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Gernot Riegler, Yiyi Liao, Simon Donne, Vladlen Koltun, and Andreas Geiger. Connecting the
dots: Learning representations for active monocular depth estimation. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Ergys Ristani and Carlo Tomasi. Features for multi-target multi-camera tracking and reidentification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Daniel Ritchie, Kai Wang, and Yu-An Lin. Fast and flexible indoor scene synthesis via deep convolutional generative models. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Cesar Roberto de Souza, Adrien Gaidon, Yohann Cabon, and Antonio Manuel Lopez. Procedural
generation of videos to train deep action recognition networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Caleb Robinson, Le Hou, Kolya Malkin, Rachel Soobitsky, Jacob Czawlytko, Bistra Dilkina, and
Nebojsa Jojic. Large scale high-resolution land cover mapping with multi-resolution data. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. Convolutional neural network architecture
for geometric matching. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Ignacio Rocco, Relja Arandjelović, and Josef Sivic. End-to-end weakly-supervised semantic alignment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Mrigank Rochan and Yang Wang. Video summarization by learning from unpaired data. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid. Lcr-net: Localization-classificationregression for human pose. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Anna Rohrbach, Marcus Rohrbach, Siyu Tang, Seong Joon Oh, and Bernt Schiele. Generating
descriptions with grounded and co-referenced people. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Michal Rolinek, Dominik Zietlow, and Georg Martius. Variational autoencoders pursue pca directions (by accident). In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Xuejian Rong, Chucai Yi, and Yingli Tian. Unambiguous text localization and retrieval for cluttered
scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Jerome Rony, Luiz G. Hafemann, Luiz S. Oliveira, Ismail Ben Ayed, Robert Sabourin, and Eric
Granger. Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and
defenses. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M. Lopez. The
synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Guy Rosman, Daniela Rus, and John W. Fisher, III. Information-driven adaptive structured-light
scanners. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Samuel Rota Bulo and Peter Kontschieder. Online learning with bayesian classification trees. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
100

220

Published as a conference paper at SIGBOVIK 2020

Samuel Rota Bulo, Gerhard Neuhold, and Peter Kontschieder. Loss max-pooling for semantic image
segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Joseph Roth, Yiying Tong, and Xiaoming Liu. Adaptive 3d face reconstruction from unconstrained
photo collections. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Rasmus Rothe, Radu Timofte, and Luc Van Gool. Some like it hot - visual guidance for preference
prediction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Tamar Rott Shaham and Tomer Michaeli. Deformation aware image compression. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Riccardo Roveri, Lukas Rahmann, Cengiz Oztireli, and Markus Gross. A network architecture for
point cloud classification via automatic depth images generation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Anirban Roy and Sinisa Todorovic. Monocular depth estimation using neural regression forest. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Anirban Roy and Sinisa Todorovic. Combining bottom-up, top-down, and smoothness cues for
weakly supervised image segmentation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Proteek Chandan Roy and Vishnu Naresh Boddeti. Mitigating information leakage in image representations: A maximum entropy approach. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, and Erik Learned-Miller. Automatic adaptation of object detectors to new domains using self-training. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Loic A. Royer, David L. Richmond, Carsten Rother, Bjoern Andres, and Dagmar Kainmueller.
Convexity shape constraints for image segmentation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Artem Rozantsev, Sudipta N. Sinha, Debadeepta Dey, and Pascal Fua. Flight dynamics-based recovery of a uav trajectory using ground cameras. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Artem Rozantsev, Mathieu Salzmann, and Pascal Fua. Residual parameter transfer for deep domain
adaptation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Denys Rozumnyi, Jan Kotera, Filip Sroubek, Lukas Novotny, and Jiri Matas. The world of fast
moving objects. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Tom F. H. Runia, Cees G. M. Snoek, and Arnold W. M. Smeulders. Real-world repetition estimation
by div, grad and curl. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Christian Rupprecht, Iro Laina, Nassir Navab, Gregory D. Hager, and Federico Tombari. Guide
me: Interacting with deep networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Paolo Russo, Fabio M. Carlucci, Tatiana Tommasi, and Barbara Caputo. From source to target and
back: Symmetric bi-directional adaptive gan. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
101

221

Published as a conference paper at SIGBOVIK 2020

Sean Ryan Fanello, Christoph Rhemann, Vladimir Tankovich, Adarsh Kowdle, Sergio Orts Escolano, David Kim, and Shahram Izadi. Hyperdepth: Learning depth from structured light without matching. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Sean Ryan Fanello, Julien Valentin, Christoph Rhemann, Adarsh Kowdle, Vladimir Tankovich,
Philip Davidson, and Shahram Izadi. Ultrastereo: Efficient learning-based matching for active
stereo systems. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli. Adversarially
learned one-class classifier for novelty detection. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Fereshteh Sadeghi, Alexander Toshev, Eric Jang, and Sergey Levine. Sim2real viewpoint invariant
visual servoing by recurrent control. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid Rezatofighi, and Silvio
Savarese. Sophie: An attentive gan for predicting paths compliant to social and physical constraints. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Faraz Saeedan, Nicolas Weber, Michael Goesele, and Stefan Roth. Detail-preserving pooling in
deep networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Ryusuke Sagawa and Yutaka Satoh. Illuminant-camera communication to observe moving objects
under strong external light by spread spectrum modulation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Alexander Sage, Eirikur Agustsson, Radu Timofte, and Luc Van Gool. Logo synthesis and manipulation with clustered generative adversarial networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Christos Sagonas, Yannis Panagakis, Alina Leidinger, and Stefanos Zafeiriou. Robust joint and individual variance explained. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Min-cheol Sagong, Yong-goo Shin, Seung-wook Kim, Seung Park, and Sung-jea Ko. Pepsi : Fast
image inpainting with parallel decoding network. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Strong-weak distribution alignment for adaptive object detection. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, and Hao Li. Photorealistic facial texture
inference using deep neural networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Mehdi S. M. Sajjadi, Raviteja Vemulapalli, and Matthew Brown. Frame-recurrent video superresolution. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Naoki Sakakibara, Fumihiko Sakaue, and Jun Sato. Seeing temporal modulation of lights from
standard cameras. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
102

222

Published as a conference paper at SIGBOVIK 2020

Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar Weber, and Antonio
Torralba. Learning cross-modal embeddings for cooking recipes and food images. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Amaia Salvador, Michal Drozdzal, Xavier Giro-i Nieto, and Adriana Romero. Inverse cooking:
Recipe generation from food images. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Artsiom Sanakoyeu, Vadim Tschernezki, Uta Buchler, and Bjorn Ommer. Divide and conquer the
embedding space for metric learning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Luis G. Sanchez Giraldo, Erion Hasanbelliu, Murali Rao, and Jose C. Principe. Group-wise pointset registration based on renyi’s second order entropy. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Tushar Sandhan and Jin Young Choi. Anti-glare: Tightly constrained optimization for eyeglass reflection removal. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, and James Hays. Scribbler: Controlling deep
image synthesis with sketch and color. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, and Rama Chellappa. Learning
from synthetic data: Addressing domain shift for semantic segmentation. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Deeppermnet: Visual
permutation learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Venkataraman Santhanam, Vlad I. Morariu, and Larry S. Davis. Generalized deep image to image
regression. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael J. Black. Learning to regress 3d face
shape and expression from an image without 3d supervision. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
M. Saquib Sarfraz, Arne Schumann, Andreas Eberle, and Rainer Stiefelhagen. A pose-sensitive
embedding for person re-identification with expanded cross neighborhood re-ranking. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Saquib Sarfraz, Vivek Sharma, and Rainer Stiefelhagen. Efficient parameter-free clustering using
first neighbor relations. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Mert Bulent Sariyildiz and Ramazan Gokberk Cinbis. Gradient matching generative networks
for zero-shot learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine:
Robust hierarchical localization at large scale. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Muhammad Sarmad, Hyunjoo Jenny Lee, and Young Min Kim. Rl-gan-net: A reinforcement learning agent controlled gan network for real-time point cloud shape completion. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
103

223

Published as a conference paper at SIGBOVIK 2020

Kazuma Sasaki, Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Joint gap detection and
inpainting of line drawings. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Torsten Sattler, Michal Havlena, Konrad Schindler, and Marc Pollefeys. Large-scale location recognition and the geometric burstiness problem. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Torsten Sattler, Akihiko Torii, Josef Sivic, Marc Pollefeys, Hajime Taira, Masatoshi Okutomi, and
Tomas Pajdla. Are large-scale 3d models really necessary for accurate visual localization? In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel
Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and Tomas Pajdla. Benchmarking 6dof outdoor visual localization in changing conditions. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal-Taixe. Understanding the limitations
of cnn-based absolute camera pose regression. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Olivier Saurer, Marc Pollefeys, and Gim Hee Lee. Sparse to dense 3d reconstruction from rolling
shutter images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Nikolay Savinov, Christian Hane, Lubor Ladicky, and Marc Pollefeys. Semantic 3d reconstruction
with continuous regularization and ray potentials using a visibility consistency constraint. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Nikolay Savinov, Akihito Seki, Lubor Ladicky, Torsten Sattler, and Marc Pollefeys. Quad-networks:
Unsupervised learning to rank for interest point detection. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Johann Sawatzky, Abhilash Srikantha, and Juergen Gall. Weakly supervised affordance detection.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Johann Sawatzky, Yaser Souri, Christian Grund, and Jurgen Gall. What object should i use? - task
driven object detection. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Ian Schillebeeckx and Robert Pless. Single image camera calibration with lenticular arrays for
augmented reality. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Hendrik Schilling, Maximilian Diebold, Carsten Rother, and Bernd Jähne. Trust your model: Light
field depth estimation with inline occlusion handling. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Michael Schober, Amit Adam, Omer Yair, Shai Mazor, and Sebastian Nowozin. Dynamic timeof-flight. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Johannes L. Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Johannes L. Schonberger, Hans Hardmeier, Torsten Sattler, and Marc Pollefeys. Comparative evaluation of hand-crafted and learned local features. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Edgar Schonfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, and Zeynep Akata. Generalized
zero- and few-shot learning via aligned variational autoencoders. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
104

224

Published as a conference paper at SIGBOVIK 2020

Thomas Schops, Johannes L. Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc
Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and
multi-camera videos. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Thomas Schops, Torsten Sattler, and Marc Pollefeys. Bad slam: Bundle adjusted direct rgb-d slam.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Samuel Schulter, Paul Vernaza, Wongun Choi, and Manmohan Chandraker. Deep network flow
for multi-object tracking. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Rene Schuster, Oliver Wasenmuller, Christian Unger, and Didier Stricker. Sdc - stacked dilated
convolution: A unified descriptor network for dense matching tasks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Tal Schuster, Lior Wolf, and David Gadot. Optical flow requires multiple strategies (but only one
network). In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Idan Schwartz, Seunghak Yu, Tamir Hazan, and Alexander G. Schwing. Factor graph attention. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Guillaume Seguin, Piotr Bojanowski, Remi Lajugie, and Ivan Laptev. Instance-level video segmentation from object tracks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Akihito Seki and Marc Pollefeys. Sgm-nets: Semi-global matching with neural networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Taiki Sekii. Robust, real-time 3d tracking of multiple objects with similar appearances. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Yusuke Sekikawa, Kosuke Hara, and Hideo Saito. Eventnet: Asynchronous recursive event processing. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Fadime Sener and Angela Yao. Unsupervised learning and segmentation of complex activities from
video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Soumyadip Sengupta, Tal Amir, Meirav Galun, Tom Goldstein, David W. Jacobs, Amit Singer, and
Ronen Basri. A new rank constraint on multi-view fundamental matrices, and its application to
camera location recovery. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Soumyadip Sengupta, Angjoo Kanazawa, Carlos D. Castillo, and David W. Jacobs. Sfsnet: Learning
shape, reflectance and illuminance of faces ‘in the wild’. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Seonguk Seo, Paul Hongsuck Seo, and Bohyung Han. Learning for single-shot confidence calibration in deep neural networks through stochastic inferences. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Laura Sevilla-Lara, Deqing Sun, Varun Jampani, and Michael J. Black. Optical flow with semantic
segmentation and localized layers. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. Cycle-consistency for robust visual question answering. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
105

225

Published as a conference paper at SIGBOVIK 2020

Sohil Shah, Tom Goldstein, and Christoph Studer. Estimating sparse signals with smooth support
via convex programming and block sparsity. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Marjan Shahpaski, Luis Ricardo Sapaico, Gaspard Chevassus, and Sabine Susstrunk. Simultaneous
geometric and radiometric calibration of a projector-camera pair. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+d: A large scale dataset for 3d
human activity analysis. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Amit Shaked and Lior Wolf. Improved stereo matching with constant highway networks and reflective confidence learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Moein Shakeri and Hong Zhang. Moving object detection under discontinuous change in illumination using tensor low-rank and invariant sparse decomposition. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Gil Shamai and Ron Kimmel. Geodesic distance descriptors. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Sukrit Shankar, Duncan Robertson, Yani Ioannou, Antonio Criminisi, and Roberto Cipolla. Refining
architectures of deep convolutional neural networks. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Ishant Shanu, Chetan Arora, and Parag Singla. Min norm point algorithm for higher order mrf-map
inference. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Ishant Shanu, Chetan Arora, and S.N. Maheshwari. Inference in higher order mrf-map problems
with small and large cliques. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jing Shao, Chen-Change Loy, Kai Kang, and Xiaogang Wang. Slicing convolutional neural network for crowd video understanding. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Wenqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang, Yudian Li, Xiaogang Wang, and Ping Luo.
Ssn: Learning sparse switchable normalization via sparsestmax. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Aidean Sharghi, Jacob S. Laurel, and Boqing Gong. Query-focused video summarization: Dataset,
evaluation, and a memory network based approach. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Vivek Sharma, Ali Diba, Davy Neven, Michael S. Brown, Luc Van Gool, and Rainer Stiefelhagen.
Classification-driven dynamic image enhancement. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Viktoriia Sharmanska, Daniel Hernandez-Lobato, Jose Miguel Hernandez-Lobato, and Novi
Quadrianto. Ambiguity helps: Classification with disagreements in crowdsourced annotations.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Mark Sheinin and Yoav Y. Schechner. The next best underwater view. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Mark Sheinin, Yoav Y. Schechner, and Kiriakos N. Kutulakos. Computational imaging on the electric grid. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
106

226

Published as a conference paper at SIGBOVIK 2020

Hao Shen. Towards a mathematical understanding of the difficulty in learning with feedforward
neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Zhijiang Zhang, and Xiang Bai. Object skeleton
extraction in natural images by fusing scale-associated deep side outputs. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Yunhang Shen, Rongrong Ji, Yan Wang, Yongjian Wu, and Liujuan Cao. Cyclic guidance for weakly
supervised joint detection and segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, and Xiangyang
Xue. Weakly supervised dense video captioning. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Lu Sheng, Jianfei Cai, Tat-Jen Cham, Vladimir Pavlovic, and King Ngi Ngan. A generative model
for depth-based robust 3d facial pose tracking. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Lu Sheng, Ziyi Lin, Jing Shao, and Xiaogang Wang. Avatar-net: Multi-scale zero-shot style transfer
by feature decoration. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Rakshith Shetty, Bernt Schiele, and Mario Fritz. Not using the car to see the sidewalk – quantifying
and controlling the effects of context in classification and segmentation. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Irina Shevlev and Shai Avidan. Co-occurrence neural network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Jian Shi, Yue Dong, Hao Su, and Stella X. Yu. Learning non-lambertian object intrinsics across
shapenet categories. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and
detection from point cloud. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel
Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient
sub-pixel convolutional neural network. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Xuepeng Shi, Shiguang Shan, Meina Kan, Shuzhe Wu, and Xilin Chen. Real-time rotation-invariant
face detection with progressive calibration networks. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Takashi Shibata, Masayuki Tanaka, and Masatoshi Okutomi. Gradient-domain image reconstruction framework with intensity-range and base-structure constraints. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Kevin J. Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual question
answering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Ya-Fang Shih, Yang-Ming Yeh, Yen-Yu Lin, Ming-Fang Weng, Yi-Chang Lu, and Yung-Yu Chuang.
Deep co-occurrence feature learning for visual object recognition. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Mihoko Shimano, Hiroki Okawa, Yuta Asano, Ryoma Bise, Ko Nishino, and Imari Sato. Wetness
and color from a single multispectral image. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
107

227

Published as a conference paper at SIGBOVIK 2020

Daeyun Shin, Charless C. Fowlkes, and Derek Hoiem. Pixels, voxels, and views: A study of shape
representations for single view 3d object shape prediction. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina Demner-Fushman, Jianhua Yao, and Ronald M. Summers. Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Jae Shin Yoon, Ziwei Li, and Hyun Soo Park. 3d semantic trajectory reconstruction from 3d pixel
continuum. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Eli Shlizerman, Lucio Dery, Hayden Schoen, and Ira Kemelmacher-Shlizerman. Audio to body
dynamics. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Assaf Shocher, Nadav Cohen, and Michal Irani. “zero-shot” super-resolution using deep internal
learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Zheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal action localization in untrimmed videos
via multi-stage cnns. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang. Cdc:
Convolutional-de-convolutional networks for precise temporal action localization in untrimmed
videos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Zheng Shou, Xudong Lin, Yannis Kalantidis, Laura Sevilla-Lara, Marcus Rohrbach, Shih-Fu Chang,
and Zhicheng Yan. Dmc-net: Generating discriminative motion cues for fast compressed video action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Robik Shrestha, Kushal Kafle, and Christopher Kanan. Answer them all! toward universal visual
question answering models. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors
with online hard example mining. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, and Russell Webb.
Learning from simulated and unsupervised images through adversarial training. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Tianmin Shu, Sinisa Todorovic, and Song-Chun Zhu. Cern: Confidence-energy recurrent network
for group activity recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Bing Shuai, Zhen Zuo, Bing Wang, and Gang Wang. Dag-recurrent neural networks for scene
labeling. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Maria Shugrina, Ziheng Liang, Amlan Kar, Jiaman Li, Angad Singh, Karan Singh, and Sanja Fidler.
Creative flow+ dataset. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Kurt Shuster, Samuel Humeau, Hexiang Hu, Antoine Bordes, and Jason Weston. Engaging image
captioning via personality. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
108

228

Published as a conference paper at SIGBOVIK 2020

Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim
Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, and Victor Lempitsky. Textured neural avatars. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Chenyang Si, Wei Wang, Liang Wang, and Tieniu Tan. Multistage adversarial losses for pose-based
human image synthesis. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, and Tieniu Tan. An attention enhanced graph
convolutional lstm network for skeleton-based action recognition. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Aliaksandr Siarohin, Enver Sangineto, Stéphane Lathuilière, and Nicu Sebe. Deformable gans for
pose-based human image generation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Aliaksandr Siarohin, Stephane Lathuiliere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. Animating
arbitrary objects via deep motion transfer. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Ronan Sicre, Yannis Avrithis, Ewa Kijak, and Frederic Jurie. Unsupervised part learning for visual
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Gunnar A. Sigurdsson, Santosh Divvala, Ali Farhadi, and Abhinav Gupta. Asynchronous temporal
fields for action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Actor and observer: Joint modeling of first and third-person videos. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Karan Sikka, Gaurav Sharma, and Marian Bartlett. Lomo: Latent ordinal model for facial analysis
in videos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Michel Silva, Washington Ramos, João Ferreira, Felipe Chamone, Mario Campos, and Erickson R.
Nascimento. A weighted sparse sampling and smoothing frame transition approach for semantic fast-forward first-person videos. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Thiago L. T. da Silveira and Claudio R. Jung. Perturbation analysis of the 8-point algorithm: A case
study for wide fov cameras. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Oriane Simeoni, Yannis Avrithis, and Ondrej Chum. Local features and visual words emerge in
activations. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Edgar Simo-Serra and Hiroshi Ishikawa. Fashion style in 128 floats: Joint ranking and classification
using weak data for feature extraction. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser Sheikh. Hand keypoint detection in single
images using multiview bootstrapping. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolutional neural networks on graphs. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
109

229

Published as a conference paper at SIGBOVIK 2020

Bharat Singh, Hengduo Li, Abhishek Sharma, and Larry S. Davis. R-fcn-3000 at 30fps: Decoupling
detection and classification. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Pravendra Singh, Vinay Kumar Verma, Piyush Rai, and Vinay P. Namboodiri. Hetconv: Heterogeneous kernel-based convolutions for deep cnns. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Saurabh Singh, Derek Hoiem, and David Forsyth. Learning to localize little landmarks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Ayan Sinha, Chiho Choi, and Karthik Ramani. Deephand: Robust hand pose estimation by completing a matrix imputed with deep features. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Ayan Sinha, Asim Unmesh, Qixing Huang, and Karthik Ramani. Surfnet: Generating 3d shape
surfaces using deep residual networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Amos Sironi, Manuele Brambilla, Nicolas Bourdis, Xavier Lagorce, and Ryad Benosman. Hats:
Histograms of averaged time surfaces for robust event-based object classification. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Niessner, Gordon Wetzstein, and Michael
Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Nicki Skafte Detlefsen, Oren Freifeld, and Søren Hauberg. Deep diffeomorphic transformer networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Miroslava Slavcheva, Maximilian Baust, Daniel Cremers, and Slobodan Ilic. Killingfusion: Nonrigid 3d reconstruction without correspondences. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Miroslava Slavcheva, Maximilian Baust, and Slobodan Ilic. Sobolevfusion: 3d reconstruction of
scenes undergoing free non-rigid motion. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Brandon M. Smith, Matthew O’Toole, and Mohit Gupta. Tracking multiple objects outside the
line of sight using speckle imaging. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Leslie N. Smith, Emily M. Hand, and Timothy Doster. Gradual dropin of layers to train very deep
neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Jakub Sochor, Adam Herout, and Jiri Havel. Boxcars: 3d boxes as cnn input for improved finegrained vehicle recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Jae Woong Soh, Gu Yong Park, Junho Jo, and Nam Ik Cho. Natural and realistic single image superresolution with explicit natural manifold discrimination. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Jeany Son, Mooyeol Baek, Minsu Cho, and Bohyung Han. Multi-object tracking with quadruplet
convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Kilho Son, daniel Moreno, James Hays, and David B. Cooper. Solving small-piece jigsaw puzzles
by growing consensus. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
110

230

Published as a conference paper at SIGBOVIK 2020

Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. Lip reading sentences in
the wild. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Jifei Song, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, and Timothy M. Hospedales. Learning to sketch
with shortcut cycle consistency. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jifei Song, Yongxin Yang, Yi-Zhe Song, Tao Xiang, and Timothy M. Hospedales. Generalizable
person re-identification by domain-invariant mapping network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Shuran Song and Jianxiong Xiao. Deep sliding shapes for amodal 3d object detection in rgb-d
images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva, and Thomas Funkhouser.
Semantic scene completion from a single depth image. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Khurram Soomro, Haroon Idrees, and Mubarak Shah. Predicting the where and what of actors and
actions through online action localization. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Nasim Souly and Mubarak Shah. Scene labeling using sparse precision matrix. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Concetto Spampinato, Simone Palazzo, Isaak Kavasidis, Daniela Giordano, Nasim Souly, and
Mubarak Shah. Deep learning human mind for automated visual classification. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Pablo Speciale, Danda Pani Paudel, Martin R. Oswald, Till Kroeger, Luc Van Gool, and Marc Pollefeys. Consensus maximization with linear matrix inequality constraints. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Pablo Speciale, Danda P. Paudel, Martin R. Oswald, Hayko Riemenschneider, Luc Van Gool, and
Marc Pollefeys. Consensus maximization for semantic region correspondences. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Pablo Speciale, Johannes L. Schonberger, Sing Bing Kang, Sudipta N. Sinha, and Marc Pollefeys.
Privacy preserving image-based localization. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Jaime Spencer, Richard Bowden, and Simon Hadfield. Scale-adaptive neural dense features: Learning via hierarchical context aggregation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Adrian Spurr, Jie Song, Seonwook Park, and Otmar Hilliges. Cross-modal deep variational hand
pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Pratul P. Srinivasan, Ren Ng, and Ravi Ramamoorthi. Light field blind motion deblurring. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Pratul P. Srinivasan, Rahul Garg, Neal Wadhwa, Ren Ng, and Jonathan T. Barron. Aperture supervision for monocular depth estimation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Pratul P. Srinivasan, Richard Tucker, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng, and Noah
Snavely. Pushing the boundaries of view extrapolation with multiplane images. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
111

231

Published as a conference paper at SIGBOVIK 2020

Russell Stewart, Mykhaylo Andriluka, and Andrew Y. Ng. End-to-end people detection in crowded
scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Timo Stoffregen and Lindsay Kleeman. Event cameras, contrast maximization and reward functions:
An analysis. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Stefan Stojanov, Samarth Mishra, Ngoc Anh Thai, Nikhil Dhanda, Ahmad Humayun, Chen Yu,
Linda B. Smith, and James M. Rehg. Incremental object learning from contiguous views. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Austin Stone, Huayan Wang, Michael Stark, Yi Liu, D. Scott Phoenix, and Dileep George. Teaching
compositionality to cnns. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Julian Straub, Trevor Campbell, Jonathan P. How, and John W. Fisher, III. Efficient global point
cloud alignment using bayesian nonparametric mixtures. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Michael Strecke, Anna Alperovich, and Bastian Goldluecke. Accurate depth and normal maps from
occlusion-aware focal stack symmetry. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Elena Stumm, Christopher Mei, Simon Lacroix, Juan Nieto, Marco Hutter, and Roland Siegwart.
Robust visual place recognition with graph kernels. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
David Stutz and Andreas Geiger. Learning 3d shape completion from laser scan data with weak
supervision. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and generalization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Bing Su and Gang Hua. Order-preserving wasserstein distance for sequence matching. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang,
and Jan Kautz. Splatnet: Sparse lattice networks for point cloud processing. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Kai Su, Dongdong Yu, Zhenqi Xu, Xin Geng, and Changhu Wang. Multi-person pose estimation
with enhanced channel-wise and spatial information. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Shuochen Su, Felix Heide, Robin Swanson, Jonathan Klein, Clara Callenberg, Matthias Hullin, and
Wolfgang Heidrich. Material classification using raw time-of-flight measurements. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Lsta: Long short-term attention for
egocentric action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Masanori Suganuma, Xing Liu, and Takayuki Okatani. Attention-based adaptive selection of operations for image restoration in the presence of unknown combined distortions. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yumin Suh, Bohyung Han, Wonsik Kim, and Kyoung Mu Lee. Stochastic class-based hard example mining for deep metric learning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
112

232

Published as a conference paper at SIGBOVIK 2020

Waqas Sultani and Mubarak Shah. What if we do not have multiple videos of the same action? –
video action localization using web images. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance
videos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Chen Sun, Manohar Paluri, Ronan Collobert, Ram Nevatia, and Lubomir Bourdev. Pronet: Learning to propose object-specific boxes for cascaded neural networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Chen Sun, Abhinav Shrivastava, Carl Vondrick, Rahul Sukthankar, Kevin Murphy, and Cordelia
Schmid. Relational action forecasting. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Haoliang Sun, Xiantong Zhen, Yuanjie Zheng, Gongping Yang, Yilong Yin, and Shuo Li. Learning
deep match kernels for image-set classification. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Qilin Sun, Xiong Dun, Yifan Peng, and Wolfgang Heidrich. Depth and transient imaging with compressive spad array cameras. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales.
Learning to compare: Relation network for few-shot learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jaeheung Surh, Hae-Gon Jeon, Yunwon Park, Sunghoon Im, Hyowon Ha, and In So Kweon. Noise
robust depth from focus using a ring difference filter. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Didac Suris, Adria Recasens, David Bau, David Harwath, James Glass, and Antonio Torralba.
Learning words by drawing images. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Tomoyuki Suzuki, Hirokatsu Kataoka, Yoshimitsu Aoki, and Yutaka Satoh. Anticipating traffic
accidents with adaptive loss and large-scale incident db. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Paul Swoboda and Vladimir Kolmogorov. Map inference via block-coordinate frank-wolfe algorithm. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Paul Swoboda, Jan Kuske, and Bogdan Savchynskyy. A dual ascent framework for lagrangean
decomposition of combinatorial problems. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Mario Sznaier and Octavia Camps. Sos-rsc: A sum-of-squares polynomial approach to robustifying subspace clustering algorithms. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Domen Tabernik, Matej Kristan, and Aleš Leonardis. Spatially-adaptive filter units for deep neural
networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Saeid Asgari Taghanaki, Kumar Abhishek, Shekoofeh Azizi, and Ghassan Hamarneh. A kernelized
manifold mapping to diminish the effect of adversarial perturbations. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
113

233

Published as a conference paper at SIGBOVIK 2020

Ying Tai, Jian Yang, and Xiaoming Liu. Image super-resolution via deep recursive residual network.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic,
Tomas Pajdla, and Akihiko Torii. Inloc: Indoor visual localization with dense matching and view
synthesis. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Kosuke Takahashi, Akihiro Miyata, Shohei Nobuhara, and Takashi Matsuyama. A linear extrinsic
calibration of kaleidoscopic imaging system from single 3d point. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Tsuyoshi Takatani, Takahito Aoto, and Yasuhiro Mukaigawa. One-shot hyperspectral imaging using
faced reflectors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Shoichiro Takeda, Kazuki Okami, Dan Mikami, Megumi Isogai, and Hideaki Kimata. Jerk-aware
video acceleration magnification. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Shoichiro Takeda, Yasunori Akagi, Kazuki Okami, Megumi Isogai, and Hideaki Kimata. Video
magnification in the wild using fractional anisotropy in temporal distribution. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Lior Talker, Yael Moses, and Ilan Shimshoni. Using spatial order to boost the elimination of incorrect feature matches. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Itamar Talmi, Roey Mechrez, and Lihi Zelnik-Manor. Template matching with deformable diversity
similarity. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Youssef Tamaazousti, Herve Le Borgne, and Celine Hudelot. Mucale-net: Multi categorical-level
networks to generate more discriminating features. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Weimin Tan, Bo Yan, and Bahetiyaer Bare. Feature super-resolution: Make machine see more
clearly. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Kenichiro Tanaka, Yasuhiro Mukaigawa, Hiroyuki Kubo, Yasuyuki Matsushita, and Yasushi Yagi.
Recovering transparent shape from time-of-flight distortion. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Kenichiro Tanaka, Yasuhiro Mukaigawa, Takuya Funatomi, Hiroyuki Kubo, Yasuyuki Matsushita,
and Yasushi Yagi. Material classification using frequency- and depth-dependent time-of-flight
distortion. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Kenichiro Tanaka, Nobuhiro Ikeya, Tsuyoshi Takatani, Hiroyuki Kubo, Takuya Funatomi, and Yasuhiro Mukaigawa. Time-resolved light transport decomposition for thermal photometric stereo.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Huixuan Tang, Scott Cohen, Brian Price, Stephen Schiller, and Kiriakos N. Kutulakos. Depth
from defocus in the wild. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Meng Tang, Abdelaziz Djelouah, Federico Perazzi, Yuri Boykov, and Christopher Schroers. Normalized cut loss for weakly-supervised cnn segmentation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
114

234

Published as a conference paper at SIGBOVIK 2020

Wei Tang and Ying Wu. Does learning specific features for related parts help human pose estimation? In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Yuxing Tang, Josiah Wang, Boyang Gao, Emmanuel Dellandrea, Robert Gaizauskas, and Liming
Chen. Large scale semi-supervised object detection using visual and semantic knowledge transfer.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato. Joint recovery of dense correspondence and
cosegmentation in two images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato. Fast multi-frame stereo scene flow with motion
segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C. Alexander, and Nathan Silberman. Learning from noisy labels by regularized estimation of annotator confusion. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Ran Tao, Efstratios Gavves, and Arnold W.M. Smeulders. Siamese instance search for tracking. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia. Scale-recurrent network for deep
image deblurring. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja
Fidler. Movieqa: Understanding stories in movies through question-answering. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Olga Taran, Shideh Rezaeifar, Taras Holotyak, and Slava Voloshynovskiy. Defending against adversarial attacks by randomized diversification. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent convolutions for
dense prediction in 3d. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Maxim Tatarchenko, Stephan R. Richter, Rene Ranftl, Zhuwen Li, Vladlen Koltun, and Thomas
Brox. What do single-view 3d reconstruction networks learn? In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Keisuke Tateno, Federico Tombari, Iro Laina, and Nassir Navab. Cnn-slam: Real-time dense monocular slam with learned depth prediction. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Hamed R. Tavakoli, Fawad Ahmed, Ali Borji, and Jorma Laaksonen. Saliency revisited: Analysis
of mouse movements versus fixations. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Chiat-Pin Tay, Sharmili Roy, and Kim-Hui Yap. Aanet: Attribute attention network for person reidentifications. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Lyne P. Tchapmi, Vineet Kosaraju, Hamid Rezatofighi, Ian Reid, and Silvio Savarese. Topnet: Structural point cloud decoder. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack Sim. Detect-to-retrieve: Efficient regional aggregation for image search. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
115

235

Published as a conference paper at SIGBOVIK 2020

Brian Teixeira, Vivek Singh, Terrence Chen, Kai Ma, Birgi Tamersoy, Yifan Wu, Elena Balashova,
and Dorin Comaniciu. Generating synthetic x-ray images of a person from the surface geometry.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Ravi Teja Mullapudi, William R. Mark, Noam Shazeer, and Kayvon Fatahalian. Hydranets: Specialized dynamic architectures for efficient inference. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Bugra Tekin, Artem Rozantsev, Vincent Lepetit, and Pascal Fua. Direct prediction of 3d body poses
from motion compensated sequences. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Bugra Tekin, Sudipta N. Sinha, and Pascal Fua. Real-time seamless single shot 6d object pose
prediction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Bugra Tekin, Federica Bogo, and Marc Pollefeys. H+o: Unified egocentric recognition of 3d handobject poses and interactions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Damien Teney and Anton van den Hengel. Actively seeking and learning from live data. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Damien Teney, Lingqiao Liu, and Anton van den Hengel. Graph-structured representations for
visual question answering. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Damien Teney, Peter Anderson, Xiaodong He, and Anton van den Hengel. Tips and tricks for visual
question answering: Learnings from the 2017 challenge. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Daniel Teo, Boxin Shi, Yinqiang Zheng, and Sai-Kit Yeung. Self-calibrating polarising radiometric
calibration. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Mariano Tepper and Guillermo Sapiro. Nonnegative matrix underapproximation for robust multiple
model fitting. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Matthew Tesfaldet, Marcus A. Brubaker, and Konstantinos G. Derpanis. Two-stream convolutional
networks for dynamic texture synthesis. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Ayush Tewari, Michael Zollhöfer, Pablo Garrido, Florian Bernard, Hyeongwoo Kim, Patrick Pérez,
and Christian Theobalt. Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Ayush Tewari, Florian Bernard, Pablo Garrido, Gaurav Bharaj, Mohamed Elgharib, Hans-Peter Seidel, Patrick Perez, Michael Zollhofer, and Christian Theobalt. Fml: Face model learning from
videos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Rajkumar Theagarajan, Ming Chen, Bir Bhanu, and Jing Zhang. Shieldnets: Defending against adversarial attacks using probabilistic adversarial robustness. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Spyridon Thermos, Georgios Th. Papadopoulos, Petros Daras, and Gerasimos Potamianos. Deep
affordance-grounded sensorimotor object recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Niessner.
Face2face: Real-time face capture and reenactment of rgb videos. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
116

236

Published as a conference paper at SIGBOVIK 2020

Janine Thoma, Danda Pani Paudel, Ajad Chhatkuli, Thomas Probst, and Luc Van Gool. Mapping,
localization and path planning for image-based navigation using visual features and map. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Diego Thomas and Rin-ichiro Taniguchi. Augmented blendshapes for real-time simultaneous 3d
head modeling and facial motion capture. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Maoqing Tian, Shuai Yi, Hongsheng Li, Shihua Li, Xuesen Zhang, Jianping Shi, Junjie Yan, and
Xiaogang Wang. Eliminating background-bias for robust person re-identification. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Yurun Tian, Bin Fan, and Fuchao Wu. L2-net: Deep learning of discriminative patch descriptor in
euclidean space. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Zhi Tian, Tong He, Chunhua Shen, and Youliang Yan. Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Radu Timofte, Rasmus Rothe, and Luc Van Gool. Seven ways to improve example-based single
image super resolution. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Tal Tlusty, Tomer Michaeli, Tali Dekel, and Lihi Zelnik-Manor. Modifying non-local variations
across multiple views. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and
Michele Covell. Full resolution image compression with recurrent neural networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Pavel Tokmakov, Karteek Alahari, and Cordelia Schmid. Learning motion patterns in videos. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Hiroki Tokunaga, Yuki Teramoto, Akihiko Yoshizawa, and Ryoma Bise. Adaptive weighting multifield-of-view cnn for semantic segmentation in pathology. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Giorgos Tolias and Ondrej Chum. Asymmetric feature maps with application to sketch based retrieval. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Denis Tome, Chris Russell, and Lourdes Agapito. Lifting from the deep: Convolutional 3d pose
estimation from a single image. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Matteo Tomei, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Art2real: Unfolding the
reality of artworks via semantically-aware image-to-image translation. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Bin Tong, Chao Wang, Martin Klinkigt, Yoshiyuki Kobayashi, and Yuuichi Nonaka. Hierarchical
disentanglement of discriminative latent features for zero-shot learning. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mattoccia, and Luigi Di Stefano. Real-time
self-adaptive deep stereo. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
117

237

Published as a conference paper at SIGBOVIK 2020

Fabio Tosi, Filippo Aleotti, Matteo Poggi, and Stefano Mattoccia. Learning monocular depth estimation infusing traditional stereo knowledge. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Matthew Trager, Martial Hebert, and Jean Ponce. Consistency of silhouettes and their duals. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Matthew Trager, Bernd Sturmfels, John Canny, Martial Hebert, and Jean Ponce. General models
for rational cameras and the case of two-slit projections. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Matthew Trager, Martial Hebert, and Jean Ponce. Coordinate-free carlsson-weinshall duality and
relative multi-view geometry. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer
look at spatiotemporal convolutions for action recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Luan Tran, Xiaoming Liu, Jiayu Zhou, and Rong Jin. Missing modalities imputation via cascaded
residual autoencoder. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Luan Tran, Feng Liu, and Xiaoming Liu. Towards high-fidelity nonlinear 3d face morphable model.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Wayne Treible, Philip Saponaro, Scott Sorensen, Abhishek Kolagunda, Michael O’Neal, Brian Phelan, Kelly Sherbondy, and Chandra Kambhamettu. Cats: A color and thermal stereo benchmark.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
George Trigeorgis, Patrick Snape, Mihalis A. Nicolaou, Epameinondas Antonakos, and Stefanos
Zafeiriou. Mnemonic descent method: A recurrent process applied for end-to-end face alignment.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
George Trigeorgis, Patrick Snape, Iasonas Kokkinos, and Stefanos Zafeiriou. Face normals ”inthe-wild” using fully convolutional networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Shashank Tripathi, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi, James M. Rehg, and Visesh
Chari. Learning to generate synthetic data via compositing. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Elena Trunz, Sebastian Merzbach, Jonathan Klein, Thomas Schulze, Michael Weinmann, and Reinhard Klein. Inverse procedural modeling of knitwear. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Chia-Yin Tsai, Aswin C. Sankaranarayanan, and Ioannis Gkioulekas. Beyond volumetric albedo
– a surface optimization framework for non-line-of-sight imaging. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Yi-Hsuan Tsai, Ming-Hsuan Yang, and Michael J. Black. Video segmentation via object flow. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, and Ming-Hsuan Yang. Deep
image harmonization. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan
Chandraker. Learning to adapt structured output space for semantic segmentation. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Kuan-Lun Tseng, Yen-Liang Lin, Winston Hsu, and Chung-Yang Huang. Joint sequence learning
and cross-modality convolution for 3d biomedical segmentation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
118

238

Published as a conference paper at SIGBOVIK 2020

Yusuke Tsuzuku and Issei Sato. On the structural sensitivity of deep convolutional networks to the
directions of fourier basis functions. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Wei-Chih Tu, Shengfeng He, Qingxiong Yang, and Shao-Yi Chien. Real-time salient object detection with a minimum spanning tree. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Wei-Chih Tu, Ming-Yu Liu, Varun Jampani, Deqing Sun, Shao-Yi Chien, Ming-Hsuan Yang, and
Jan Kautz. Learning superpixels with segmentation-aware affinity loss. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Radu Tudor Ionescu, Bogdan Alexe, Marius Leordeanu, Marius Popescu, Dim P. Papadopoulos, and
Vittorio Ferrari. How hard can it be? estimating the difficulty of visual search in an image. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, and Jitendra Malik. Multi-view supervision
for single-view reconstruction via differentiable ray consistency. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Shubham Tulsiani, Saurabh Gupta, David F. Fouhey, Alexei A. Efros, and Jitendra Malik. Factoring
shape, pose, and layout from the 2d image of a 3d scene. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Sergey Tulyakov, Xavier Alameda-Pineda, Elisa Ricci, Lijun Yin, Jeffrey F. Cohn, and Nicu Sebe.
Self-adaptive matrix completion for heart rate estimation from face videos under realistic conditions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion
and content for video generation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Hsiao-Yu Tung, Adam W. Harley, Liang-Kang Huang, and Katerina Fragkiadaki. Reward learning
from narrated demonstrations. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Hsiao-Yu Fish Tung, Ricson Cheng, and Katerina Fragkiadaki. Learning spatial common sense with
geometry-aware recurrent networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Javier S. Turek and Alexander G. Huth. Efficient, sparse representation of manifold distance matrices for classical scaling. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Lachlan Tychsen-Smith and Lars Petersson. Improving object localization with fitness nms and
bounded iou loss. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Takeshi Uemori, Atsushi Ito, Yusuke Moriuchi, Alexander Gatto, and Jun Murayama. Skin-based
identification from multispectral image data using cnns. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Nikolai Ufer and Bjorn Ommer. Deep semantic feature matching. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Jasper Uijlings, Stefan Popov, and Vittorio Ferrari. Revisiting knowledge transfer for training object
class detectors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
119

239

Published as a conference paper at SIGBOVIK 2020

Ries Uittenbogaard, Clint Sebastian, Julien Vijverberg, Bas Boom, Dariu M. Gavrila, and Peter
H.N. de With. Privacy protection in street-view panoramas using depth and multi-view imagery.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Improved texture networks: Maximizing
quality and diversity in feed-forward stylization and texture synthesis. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July 2017.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Paul Upchurch, Jacob Gardner, Geoff Pleiss, Robert Pless, Noah Snavely, Kavita Bala, and Kilian
Weinberger. Deep feature interpolation for image content changes. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Aisha Urooj and Ali Borji. Analysis of hand segmentation in the wild. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Anil Usumezbas, Ricardo Fabbri, and Benjamin B. Kimia. The surfacing of multiview 3d drawings
via lofting and occlusion reasoning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, and Philip H. S. Torr. End-to-end
representation learning for correlation filter based tracking. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Grant Van Horn, Steve Branson, Scott Loarie, Serge Belongie, and Pietro Perona. Lean multiclass
crowdsourcing. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black, Ivan Laptev, and
Cordelia Schmid. Learning from synthetic humans. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Subeesh Vasu and A. N. Rajagopalan. From local to global: Edge profiles to camera motion in
blurred images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Subeesh Vasu, Mahesh M. R. Mohan, and A. N. Rajagopalan. Occlusion-aware rolling shutter
rectification of 3d scenes. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, and Gal Chechik. Contextaware captions from context-agnostic supervision. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
VSR Veeravasarapu, Constantin Rothkopf, and Ramesh Visvanathan. Adversarially tuned scene
generation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Andreas Veit, Serge Belongie, and Theofanis Karaletsos. Conditional similarity networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Andreas Veit, Maximilian Nickel, Serge Belongie, and Laurens van der Maaten. Separating selfexpression and visual content in hashtag supervision. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Raviteja Vemulapalli and Aseem Agarwala. A compact embedding for facial expression similarity.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
120

240

Published as a conference paper at SIGBOVIK 2020

Raviteja Vemulapalli, Oncel Tuzel, Ming-Yu Liu, and Rama Chellapa. Gaussian conditional random field network for semantic segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Carles Ventura, Miriam Bellver, Andreu Girbau, Amaia Salvador, Ferran Marques, and Xavier Giroi Nieto. Rvos: End-to-end recurrent network for video object segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond Mooney, Trevor Darrell, and Kate Saenko. Captioning images with diverse objects. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Cedric Verleysen and Christophe De Vleeschouwer. Piecewise-planar 3d approximation from widebaseline stereo. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Nitika Verma, Edmond Boyer, and Jakob Verbeek. Feastnet: Feature-steered graph convolutions for
3d shape analysis. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Paul Vernaza and Manmohan Chandraker. Learning random-walk label propagation for weaklysupervised semantic segmentation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Matthias Vestner, Roee Litman, Emanuele Rodola, Alex Bronstein, and Daniel Cremers. Product manifold filter: Non-rigid shape correspondence via kernel density estimation in the product
space. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Alessandro Vianello, Jens Ackermann, Maximilian Diebold, and Bernd Jähne. Robust hough transform based 3d reconstruction from circular light fields. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Paul Vicol, Makarand Tapaswi, Lluı́s Castrejón, and Sanja Fidler. Moviegraphs: Towards understanding human-centric situations from videos. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Ruben Villegas, Jimei Yang, Duygu Ceylan, and Honglak Lee. Neural kinematic networks for
unsupervised motion retargetting. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Minh Vo, Srinivasa G. Narasimhan, and Yaser Sheikh. Spatiotemporal bundle adjustment for dynamic 3d reconstruction. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing
text and image for image retrieval - an empirical odyssey. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar,
Andreas Geiger, and Bastian Leibe. Mots: Multi-object tracking and segmentation. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Anna Volokitin, Michael Gygli, and Xavier Boix. Predicting when saliency maps are accurate and
eye fixations consistent. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Riccardo Volpi, Pietro Morerio, Silvio Savarese, and Vittorio Murino. Adversarial feature augmentation for unsupervised domain adaptation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
121

241

Published as a conference paper at SIGBOVIK 2020

Carl Vondrick and Antonio Torralba. Generating the future with adversarial transformers. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating visual representations from
unlabeled video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Jayakorn Vongkulbhisal, Ricardo Cabral, Fernando De la Torre, and Joao P. Costeira. Motion from
structure (mfs): Searching for 3d objects in cluttered point trajectories. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
Jayakorn Vongkulbhisal, Fernando De la Torre, and Joao P. Costeira. Discriminative optimization:
Theory and applications to point cloud registration. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Jayakorn Vongkulbhisal, Beñat Irastorza Ugalde, Fernando De la Torre, and João P. Costeira. Inverse
composition discriminative optimization for point cloud registration. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Jayakorn Vongkulbhisal, Phongtharin Vinayavekhin, and Marco Visentini-Scarzanella. Unifying
heterogeneous classifiers with distillation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Perez. Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Jorg Wagner, Jan Mathias Kohler, Tobias Gindele, Leon Hetzel, Jakob Thaddaus Wiedemer, and
Sven Behnke. Interpretable and fine-grained visual explanations for convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Robert Walecki, Ognjen Rudovic, Vladimir Pavlovic, and Maja Pantic. Copula ordinal regression
for joint estimation of facial action unit intensity. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2016.
Robert Walecki, Ognjen (Oggi) Rudovic, Vladimir Pavlovic, Bjoern Schuller, and Maja Pantic.
Deep structured learning for facial action unit intensity estimation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Chengde Wan, Thomas Probst, Luc Van Gool, and Angela Yao. Crossing nets: Combining gans and
vaes with a shared latent space for hand pose estimation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Fang Wan, Pengxu Wei, Jianbin Jiao, Zhenjun Han, and Qixiang Ye. Min-entropy latent model for
weakly supervised object detection. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Fang Wan, Chang Liu, Wei Ke, Xiangyang Ji, Jianbin Jiao, and Qixiang Ye. C-mil: Continuation
multiple instance learning for weakly supervised object detection. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Bastian Wandt and Bodo Rosenhahn. Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Chen Wang, Jianfei Yang, Lihua Xie, and Junsong Yuan. Kervolutional neural networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Kang Wang, Rui Zhao, and Qiang Ji. A hierarchical generative model for eye image synthesis
and eye gaze estimation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
122

242

Published as a conference paper at SIGBOVIK 2020

Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, and Xiang Ruan.
Learning to detect salient objects with image-level supervision. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Shenlong Wang, Sean Ryan Fanello, Christoph Rhemann, Shahram Izadi, and Pushmeet Kohli. The
global patch collider. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. Detecting overfitting of deep generative networks via latent recovery. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Jan D. Wegner, Steven Branson, David Hall, Konrad Schindler, and Pietro Perona. Cataloging public
objects using aerial and street-level images - urban trees. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Jônatas Wehrmann and Rodrigo C. Barros. Bidirectional retrieval made simple. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Chen Wei, Lingxi Xie, Xutong Ren, Yingda Xia, Chi Su, Jiaying Liu, Qi Tian, and Alan L. Yuille.
Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Lingyu Wei, Qixing Huang, Duygu Ceylan, Etienne Vouga, and Hao Li. Dense human body correspondences using convolutional networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Person transfer gan to bridge domain gap for
person re-identification. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, and Shuicheng Yan. Object region mining with adversarial erasing: A simple classification to semantic segmentation
approach. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learning steerable filters for rotation
equivariant cnns. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Philippe Weinzaepfel, Gabriela Csurka, Yohann Cabon, and Martin Humenberger. Visual localization by learning objects-of-interest dense match regression. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Qiang Wen, Yinjie Tan, Jing Qin, Wenxi Liu, Guoqiang Han, and Shengfeng He. Single image
reflection removal beyond linearity. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Yandong Wen, Zhifeng Li, and Yu Qiao. Latent factor guided convolutional neural networks for ageinvariant face recognition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Chung-Yi Weng, Brian Curless, and Ira Kemelmacher-Shlizerman. Photo wake-up: 3d character
animation from a single photo. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Junwu Weng, Chaoqun Weng, and Junsong Yuan. Spatio-temporal naive-bayes nearest-neighbor
(st-nbnn) for skeleton-based action recognition. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
Eric Wengrowski and Kristin Dana. Light field messaging with deep photographic steganography.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
123

243

Published as a conference paper at SIGBOVIK 2020

Davis Wertheimer and Bharath Hariharan. Few-shot learning with localization in realistic settings.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Matthew Wicker and Marta Kwiatkowska. Robustness of 3d deep learning in an adversarial setting.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Patrick Wieschollek, Oliver Wang, Alexander Sorkine-Hornung, and Hendrik P. A. Lensch. Efficient large-scale approximate nearest neighbor search on the gpu. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
Maggie Wigness and John G. Rogers, III. Unsupervised semantic scene labeling for streaming data.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Erik Wijmans and Yasutaka Furukawa. Exploiting 2d floorplan for building-scale panorama rgbd
alignment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee,
Irfan Essa, Devi Parikh, and Dhruv Batra. Embodied question answering in photorealistic environments with point cloud perception. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, and Daniele Panozzo.
Deep geometric prior for surface reconstruction. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
W. Williem and In Kyu Park. Robust light field depth estimation for noisy scene with occlusion. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Calden Wloka and John Tsotsos. Spatially binned roc: A comprehensive saliency metric. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Calden Wloka, Iuliia Kotseruba, and John K. Tsotsos. Active fixation control to predict saccade
sequences. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Mark Wolff, Robert T. Collins, and Yanxi Liu. Regularity-driven facade matching between aerial
and street views. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Alex Wong and Stefano Soatto. Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow. Harmonic
networks: Deep translation and rotation equivariance. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
Learning to learn how to learn: Self-adaptive visual navigation using meta-learning. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross
Girshick. Long-term feature banks for detailed video understanding. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang. Fast end-to-end trainable guided filter. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Jiajun Wu, Joshua B. Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, and Anton van den Hengel. What value do
explicit high level concepts have in vision to language problems? In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.
124

244

Published as a conference paper at SIGBOVIK 2020

Seoung Wug Oh, Michael S. Brown, Marc Pollefeys, and Seon Joo Kim. Do it yourself hyperspectral
imaging with everyday digital cameras. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Seoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli, and Seon Joo Kim. Fast video object segmentation by reference-guided mask propagation. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
Jonas Wulff, Laura Sevilla-Lara, and Michael J. Black. Optical flow in mostly rigid scenes. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Changqun Xia, Jia Li, Xiaowu Chen, Anlin Zheng, and Yu Zhang. What is and what is not a salient
object? learning salient object detector by ensembling linear exemplar regressors. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello
Pelillo, and Liangpei Zhang. Dota: A large-scale dataset for object detection in aerial images. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo. Monocular
relative depth perception with web stereo data supervision. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, and Bernt Schiele.
Latent embeddings for zero-shot classification. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning - the good, the bad and the
ugly. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic projection network for zero- and few-label semantic segmentation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Chong Xiang, Charles R. Qi, and Bo Li. Generating 3d adversarial point clouds. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, and Mingyan Liu. Meshadv: Adversarial meshes
for visual recognition. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Fanyi Xiao and Yong Jae Lee. Track and segment: An iterative unsupervised approach for video
object proposals. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Huaxin Xiao, Jiashi Feng, Guosheng Lin, Yu Liu, and Maojun Zhang. Monet: Deep motion exploitation for video object segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xiaogang Wang. Joint detection and identification feature learning for person search. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L. Yuille, and Kaiming He. Feature denoising
for improving adversarial robustness. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Lingxi Xie, Liang Zheng, Jingdong Wang, Alan L. Yuille, and Qi Tian. Interactive: Inter-layer
activeness propagation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
125

245

Published as a conference paper at SIGBOVIK 2020

Shuqin Xie, Zitian Chen, Chao Xu, and Cewu Lu. Environment upgrade reinforcement learning
for non-differentiable multi-stage pipelines. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Shumian Xin, Sotiris Nousias, Kiriakos N. Kutulakos, Aswin C. Sankaranarayanan, Srinivasa G.
Narasimhan, and Ioannis Gkioulekas. A theory of fermat paths for non-line-of-sight shape reconstruction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Chao Xing, Xin Geng, and Hui Xue. Logistic boosting regression for label distribution learning. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Xianglei Xing, Tian Han, Ruiqi Gao, Song-Chun Zhu, and Ying Nian Wu. Unsupervised disentangling of appearance and geometry by deformable generator network. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Bo Xiong, Yannis Kalantidis, Deepti Ghadiyaram, and Kristen Grauman. Less is more: Learning
highlight detection from video duration. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos
using multi-stage dynamic generative adversarial networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Zhiwei Xiong, Lizhi Wang, Huiqun Li, Dong Liu, and Feng Wu. Snapshot hyperspectral light field
imaging. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Danfei Xu, Dragomir Anguelov, and Ashesh Jain. Pointfusion: Deep sensor fusion for 3d bounding
box estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S. Huang. Deep interactive object
selection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Shuangjie Xu, Daizong Liu, Linchao Bao, Wei Liu, and Pan Zhou. Mhp-vos: Multiple hypotheses
propagation for video object segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Zhongwen Xu, Linchao Zhu, and Yi Yang. Few-shot object recognition from machine-labeled web
images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Jia Xue, Hang Zhang, Kristin Dana, and Ko Nishino. Differential angular imaging for material
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.
Jia Xue, Hang Zhang, and Kristin Dana. Deep texture manifold for ground terrain recognition. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Nan Xue, Song Bai, Fudong Wang, Gui-Song Xia, Tianfu Wu, and Liangpei Zhang. Learning
attraction field representation for robust line segment detection. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani, and Yoichi Sato. Future person localization
in first-person videos. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Tomas F. Yago Vicente, Minh Hoai, and Dimitris Samaras. Noisy label recovery for shadow detection in unfamiliar domains. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
126

246

Published as a conference paper at SIGBOVIK 2020

Noam Yair and Tomer Michaeli. Multi-scale weighted nuclear norm image restoration. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Hang Yan, Yebin Liu, and Yasutaka Furukawa. Turning an urban scene video into a cinemagraph.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Ke Yan, Xiaosong Wang, Le Lu, Ling Zhang, Adam P. Harrison, Mohammadhadi Bagheri, and
Ronald M. Summers. Deep lesion graphs in the wild: Relationship learning and organization
of significant radiology image findings in a diverse large-scale lesion database. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Qingsen Yan, Dong Gong, Qinfeng Shi, Anton van den Hengel, Chunhua Shen, Ian Reid, and
Yanning Zhang. Attention-guided network for ghost-free high dynamic range imaging. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Hongyu Yang, Di Huang, Yunhong Wang, and Anil K. Jain. Learning face age progression: A pyramid architecture of gans. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Shijie Yang, Liang Li, Shuhui Wang, Weigang Zhang, and Qingming Huang. A graph regularized
deep neural network for unsupervised image representation learning. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Xitong Yang, Xiaodong Yang, Ming-Yu Liu, Fanyi Xiao, Larry S. Davis, and Jan Kautz. Step:
Spatio-temporal progressive learning for video action detection. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for
image question answering. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Taiping Yao, Minsi Wang, Bingbing Ni, Huawei Wei, and Xiaokang Yang. Multiple granularity
group interaction prediction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Ting Yao, Tao Mei, and Yong Rui. Highlight detection with pairwise deep ranking for first-person
video summarization. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Incorporating copying mechanism in image captioning for learning novel objects. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Yuan Yao, Jianqiang Ren, Xuansong Xie, Weidong Liu, Yong-Jin Liu, and Jun Wang. Attentionaware multi-stroke style transfer. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Rajeev Yasarla and Vishal M. Patel. Uncertainty guided multi-scale residual learning-using a cycle
spinning cnn for single image de-raining. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Hashim Yasin, Umar Iqbal, Bjorn Kruger, Andreas Weber, and Juergen Gall. A dual-source approach
for 3d pose estimation from a single image. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi. Situation recognition: Visual semantic role
labeling for image understanding. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, and Ali Farhadi. Commonly uncommon: Semantic sparsity in situation recognition. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
127

247

Published as a conference paper at SIGBOVIK 2020

Jingwen Ye, Yixin Ji, Xinchao Wang, Kairi Ou, Dapeng Tao, and Mingli Song. Student becoming
the master: Knowledge amalgamation for joint scene parsing, depth estimation, and more. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Jinmian Ye, Linnan Wang, Guangxi Li, Di Chen, Shandian Zhe, Xinqi Chu, and Zenglin Xu. Learning compact recurrent neural networks with block-term tensor decomposition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Qixiang Ye, Tianliang Zhang, Wei Ke, Qiang Qiu, Jie Chen, Guillermo Sapiro, and Baochang Zhang.
Self-learning scene-specific pedestrian detectors using a progressive latent model. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Adam B. Yedidia, Manel Baradad, Christos Thrampoulidis, William T. Freeman, and Gregory W.
Wornell. Using unknown occluders to recover hidden scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Raymond A. Yeh, Chen Chen, Teck Yian Lim, Alexander G. Schwing, Mark Hasegawa-Johnson,
and Minh N. Do. Semantic image inpainting with deep generative models. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Raymond A. Yeh, Minh N. Do, and Alexander G. Schwing. Unsupervised textual grounding: Linking words to image concepts. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Raymond A. Yeh, Alexander G. Schwing, Jonathan Huang, and Kevin Murphy. Diverse generation for multi-agent sports games. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Florence Yellin, Benjamin D. Haeffele, Sophie Roth, and René Vidal. Multi-cell detection and
classification using a generative convolutional model. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.
Donghun Yeo, Jeany Son, Bohyung Han, and Joon Hee Han. Superpixel-based tracking-bysegmentation using markov chains. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. End-to-end learning of action detection from frame glimpses in videos. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Serena Yeung, Vignesh Ramanathan, Olga Russakovsky, Liyue Shen, Greg Mori, and Li Fei-Fei.
Learning to learn from noisy web videos. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Anthony Yezzi, Ganesh Sundaramoorthi, and Minas Benyamin. Pde acceleration for active contours.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Li Yi, Hao Su, Xingwen Guo, and Leonidas J. Guibas. Syncspeccnn: Synchronized spectral cnn for
3d shape segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J. Guibas. Gspn: Generative shape proposal network for 3d instance segmentation in point cloud. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Ran Yi, Yong-Jin Liu, and Yu-Kun Lai. Content-sensitive supervoxels via uniform tessellations on
video manifolds. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Wang Yifan, Shihao Wu, Hui Huang, Daniel Cohen-Or, and Olga Sorkine-Hornung. Patch-based
progressive 3d point set upsampling. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
128

248

Published as a conference paper at SIGBOVIK 2020

Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast
optimization, network minimization and transfer learning. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017.
Guojun Yin, Bin Liu, Lu Sheng, Nenghai Yu, Xiaogang Wang, and Jing Shao. Semantics disentangling for text-to-image generation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Ming Yin, Yi Guo, Junbin Gao, Zhaoshui He, and Shengli Xie. Kernel sparse subspace clustering on
symmetric positive definite manifolds. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Zhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth, optical flow and
camera pose. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Xingde Ying, Heng Guo, Kai Ma, Jian Wu, Zhengxin Weng, and Yefeng Zheng. X2ct-gan: Reconstructing ct from biplanar x-rays with generative adversarial networks. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Tatsuya Yokota and Hidekata Hontani. Simultaneous visual data completion and denoising based on
tensor rank and total variation minimization and its primal-dual splitting algorithm. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Tatsuya Yokota, Burak Erem, Seyhmus Guler, Simon K. Warfield, and Hidekata Hontani. Missing
slice recovery for tensors using a low-rank model in embedded space. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2018.
Masashi Yokozuka, Shuji Oishi, Simon Thompson, and Atsuhiko Banno. Vitamin-e: Visual tracking
and mapping with extremely dense feature points. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Ryo Yonetani, Kris M. Kitani, and Yoichi Sato. Recognizing micro-actions and reactions from
paired egocentric videos. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Donggeun Yoo and In So Kweon. Learning loss for active learning. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Jaeyoung Yoo, Sang-ho Lee, and Nojun Kwak. Image restoration by estimating frequency distribution of local patches. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
YoungJoon Yoo, Kimin Yun, Sangdoo Yun, JongHee Hong, Hawook Jeong, and Jin Young Choi.
Visual path prediction in complex scenes with crowded moving objects. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2016.
YoungJoon Yoo, Sangdoo Yun, Hyung Jin Chang, Yiannis Demiris, and Jin Young Choi. Variational
autoencoded regression: High dimensional regression of visual data on complex manifold. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Jae Shin Yoon, Takaaki Shiratori, Shoou-I Yu, and Hyun Soo Park. Self-supervised adaptation
of high-fidelity face models for monocular performance tracking. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida, and Takeshi Naemura.
Classification-reconstruction learning for open-set recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Chong You, Daniel P. Robinson, and Rene Vidal. Provable self-representation based outlier detection
in a union of subspaces. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
129

249

Published as a conference paper at SIGBOVIK 2020

Jinjie You, Ancong Wu, Xiang Li, and Wei-Shi Zheng. Top-push video-based person reidentification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Universal domain adaptation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Quanzeng You, Zhengyou Zhang, and Jiebo Luo. End-to-end convolutional semantic embeddings.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Aron Yu and Kristen Grauman. Thinking outside the pool: Active training image creation for relative
attributes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated residual networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales, and Chen-Change Loy.
Sketch me that shoe. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Tan Yu, Jingjing Meng, and Junsong Yuan. Multi-view harmonized bilinear network for 3d object
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Jun Yuan, Bingbing Ni, Xiaokang Yang, and Ashraf A. Kassim. Temporal action localization with
pyramid of score distribution features. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Shanxin Yuan, Guillermo Garcia-Hernando, Björn Stenger, Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee, Pavlo Molchanov, Jan Kautz, Sina Honari, Liuhao Ge, Junsong Yuan, Xinghao
Chen, Guijin Wang, Fan Yang, Kai Akiyama, Yang Wu, Qingfu Wan, Meysam Madadi, Sergio Escalera, Shile Li, Dongheui Lee, Iason Oikonomidis, Antonis Argyros, and Tae-Kyun Kim.
Depth-based 3d hand pose estimation: From current achievements to future goals. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Tongtong Yuan, Weihong Deng, Jian Tang, Yinan Tang, and Binghui Chen. Signal-to-noise ratio:
A robust distance metric for deep metric learning. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019.
Zehuan Yuan, Jonathan C. Stroud, Tong Lu, and Jia Deng. Temporal action localization by structured
maximal sums. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Jae-Seong Yun and Jae-Young Sim. Reflection removal for large-scale 3d point clouds. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun, and Jin Young Choi. Action-decision
networks for visual tracking with deep reinforcement learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Victor Yurchenko and Victor Lempitsky. Parsing images of overlapping organisms with deep
singling-out networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency. Social-iq:
A question answering benchmark for artificial social intelligence. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Alireza Zaeemzadeh, Mohsen Joneidi, Nazanin Rahnavard, and Mubarak Shah. Iterative projection
and matching: Finding structure-preserving representatives and its application to computer vision.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
130

250

Published as a conference paper at SIGBOVIK 2020

Lazaros Zafeiriou, Epameinondas Antonakos, Stefanos Zafeiriou, and Maja Pantic. Joint unsupervised deformable spatio-temporal alignment of sequences. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Hasan F. M. Zaki, Faisal Shafait, and Ajmal Mian. Modeling sub-event dynamics in first-person action recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Amir R. Zamir, Te-Lin Wu, Lin Sun, William B. Shen, Bertram E. Shi, Jitendra Malik, and Silvio Savarese. Feedback networks. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Amir R. Zamir, Alexander Sax, William Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio
Savarese. Taskonomy: Disentangling task transfer learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchisescu. Monocular 3d pose and shape estimation of multiple people in natural scenes - the importance of multiple scene constraints. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Valentina Zantedeschi, Remi Emonet, and Marc Sebban. Metric learning as convex combinations
of local models with generalization guarantees. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing with global context. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual
commonsense reasoning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Oliver Zendel, Katrin Honauer, Markus Murschitz, Martin Humenberger, and Gustavo Fernandez Dominguez. Analyzing computer vision data - the good, the bad and the ugly. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Andy Zeng, Shuran Song, Matthias Niessner, Matthew Fisher, Jianxiong Xiao, and Thomas
Funkhouser. 3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining Guo. Learning pyramid-context encoder
network for high-quality image inpainting. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Yu Zeng, Huchuan Lu, Lihe Zhang, Mengyang Feng, and Ali Borji. Learning to promote saliency
detectors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Menghua Zhai, Scott Workman, and Nathan Jacobs. Detecting vanishing points using global image
context in a non-manhattan world. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Menghua Zhai, Zachary Bessinger, Scott Workman, and Nathan Jacobs. Predicting ground-level
scene layout from aerial imagery. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Yao Zhai, Jingjing Fu, Yan Lu, and Houqiang Li. Feature selective networks for object detection. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li, Harsh Agarwal, and Ian Reid.
Unsupervised learning of monocular depth estimation and visual odometry with deep feature
reconstruction. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
131

251

Published as a conference paper at SIGBOVIK 2020

Xiaohang Zhan, Xingang Pan, Ziwei Liu, Dahua Lin, and Chen Change Loy. Self-supervised learning via conditional motion propagation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Feihu Zhang, Victor Prisacariu, Ruigang Yang, and Philip H.S. Torr. Ga-net: Guided aggregation
net for end-to-end stereo matching. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu. Mining object parts from
cnns via active question-answering. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Yinda Zhang and Thomas Funkhouser. Deep depth completion of a single rgb-d image. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Zizhao Zhang, Fuyong Xing, Xiaoshuang Shi, and Lin Yang. Semicontour: A semi-supervised
learning approach for contour detection. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Chen Zhao, Zhiguo Cao, Chi Li, Xin Li, and Jiaqi Yang. Nm-net: Mining reliable neighbors for
robust feature correspondences. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Hao Zhao, Ming Lu, Anbang Yao, Yiwen Guo, Yurong Chen, and Li Zhang. Physics inspired
optimization on semantic transfer features: An alternative method for room layout estimation. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Kaili Zhao, Wen-Sheng Chu, and Honggang Zhang. Deep region and multi-label learning for facial
action unit detection. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Kaili Zhao, Wen-Sheng Chu, and Aleix M. Martinez. Learning facial action units from web images
with scalable weakly supervised clustering. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Liangli Zhen, Peng Hu, Xu Wang, and Dezhong Peng. Deep supervised cross-modal retrieval. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi Yang, and Qi Tian. Person re-identification in the wild. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Wenzhao Zheng, Zhaodong Chen, Jiwen Lu, and Jie Zhou. Hardness-aware deep metric learning.
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Yinqiang Zheng and Laurent Kneip. A direct least-squares solution to the pnp problem with unknown focal length. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Yutong Zheng, Dipan K. Pal, and Marios Savvides. Ring loss: Convex feature normalization for
face recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Tiancheng Zhi, Bernardo R. Pires, Martial Hebert, and Srinivasa G. Narasimhan. Deep materialaware cross-spectral stereo matching. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Tiancheng Zhi, Bernardo R. Pires, Martial Hebert, and Srinivasa G. Narasimhan. Multispectral
imaging for fine-grained recognition of powders on complex backgrounds. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu. Practical block-wise neural network architecture generation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
132

252

Published as a conference paper at SIGBOVIK 2020

Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-ranking person re-identification with
k-reciprocal encoding. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and Yi Yang. Invariance matters: Exemplar
memory for domain adaptive person re-identification. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Tinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing Huang, and Alexei A. Efros. Learning
dense correspondence via 3d-guided cycle consistency. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Xingyi Zhou, Jiacheng Zhuo, and Philipp Krahenbuhl. Bottom-up object detection by grouping
extreme and center points. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Oriented response networks. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Yizhou Zhou, Xiaoyan Sun, Zheng-Jun Zha, and Wenjun Zeng. Mict: Mixed 3d/2d convolutional
tube for human action recognition. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and Stan Z. Li. Face alignment across large
poses: A 3d solution. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Xinge Zhu, Jiangmiao Pang, Ceyuan Yang, Jianping Shi, and Dahua Lin. Adapting object detectors
via selective cross-domain alignment. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Yuke Zhu, Joseph J. Lim, and Li Fei-Fei. Knowledge acquisition for visual question answering via
iterative querying. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.
Zheng Zhu, Wei Wu, Wei Zou, and Junjie Yan. End-to-end flow correlation tracking with spatialtemporal attention. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Bohan Zhuang, Guosheng Lin, Chunhua Shen, and Ian Reid. Fast training of triplet-based deep binary embedding networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Bohan Zhuang, Lingqiao Liu, Yao Li, Chunhua Shen, and Ian Reid. Attend in groups: A weaklysupervised deep learning framework for learning from web data. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, and Anton van den Hengel. Parallel attention: A
unified framework for visual object discovery through dialogs and queries. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Structured binary neural
networks for accurate image classification and semantic segmentation. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June 2019.
Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and
Josef Sivic. Cross-task weakly supervised learning from instructional videos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Junbao Zhuo, Shuhui Wang, Shuhao Cui, and Qingming Huang. Unsupervised open domain recognition by semantic discrepancy minimization. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
133

253

Published as a conference paper at SIGBOVIK 2020

Wei Zhuo, Mathieu Salzmann, Xuming He, and Miaomiao Liu. Indoor scene parsing with instance
segmentation, semantic labeling and support relationship inference. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Magauiya Zhussip, Shakarim Soltanayev, and Se Young Chun. Training deep learning based image
denoisers from undersampled measurements without ground truth and without image prior. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Ev Zisselman, Jeremias Sulam, and Michael Elad. A local block coordinate descent algorithm for
the csc model. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.
Aleksandar Zlateski, Ronnachai Jaroensri, Prafull Sharma, and Frédo Durand. On the importance
of label quality for semantic segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures for scalable image recognition. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Chuhang Zou, Alex Colburn, Qi Shan, and Derek Hoiem. Layoutnet: Reconstructing the 3d room
layout from a single rgb image. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences of adam and rmsprop. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Silvia Zuffi, Angjoo Kanazawa, David W. Jacobs, and Michael J. Black. 3d menagerie: Modeling
the 3d shape and pose of animals. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.
Silvia Zuffi, Angjoo Kanazawa, and Michael J. Black. Lions and tigers and bears: Capturing nonrigid, 3d, articulated shape from images. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Syed Zulqarnain Gilani and Ajmal Mian. Learning from millions of 3d scans for large-scale 3d face
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018.
Yiming Zuo, Weichao Qiu, Lingxi Xie, Fangwei Zhong, Yizhou Wang, and Alan L. Yuille. Craves:
Controlling robotic arm with a vision-based economic system. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Shay Zweig and Lior Wolf. Interponet, a brain inspired neural network for optical flow dense
interpolation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
July 2017.

134

254

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 17: GradSchoolNet: Robust end-to-end
*-shot unsupervised deepAF neural attention
model for convexly optimal (artifically intelligent) success in computer vision research
Reviewer: Definitely an expert and not a first-year grad student subreviewing
Rating: Strongest of Rejects
Confidence: Expert

I really wanted to like this paper, because the topic is interesting and because it is bad form to start
reviewing a paper with the intention of hating it. Unfortunately, I got confused at a number of
points while reading and just feel that the paper is too obscure for the SIGBOVIK audience. Here
are some examples drawn from the first page, which is the only page that I can be sure the paper
has.
1. I got completely lost on the second paragraph of p.1 and am sure that the average reader will
as well. It’s definitely not just me.
2. Your use of the term “it” is confusing. Am I supposed to know what “it” is supposed to refer
to?
3. SIGBOVIK readers come from a wide variety of backgrounds and may not be familiar with
what a “computer” is, though of course I am.
4. The definition of “algorithm”, as I’m familiar with it, is “a set of rules for solving a problem
in a finite number of steps, as for finding the greatest common divisor” [Dictionary.com].
I don’t believe you’re using this term correctly, as your paper does not mention greatest
common divisors. It may not even mention algorithms, which would be an even more glaring
flaw in the paper.

255

256

Natural Intelligence & Human Learning
25

Sorting with human intelligence
Cole Kurashige
Keywords: sorting, algorithm, crowd sourcing, human intelligence

26

Synthesizing programs by asking people for help online
Cassie Jones
Keywords: program synthesis, cloud computing, social computation, the real program synthesizer was the friends we made
along the way

27

HonkFast, PreHonk, HonkBack, PreHonkBack, HERS, AdHonk and AHC: The missing keys for autonomous driving
Bernhard Egger, Max Siegel
Keywords: awesome paper, best paper award, great reviewer 3,
awesome reviewer 2, marvelous reviewer 1

257

25

Sorting with Human Intelligence
Cole Kurashige
Harvey Mudd College
March
2020
Abstract
Many comparison-based sorting algorithms have been introduced in
years past, but none are capable of comparing elements of two different
types. We present a novel algorithm called Turksort which uses human
intelligence to sort lists with truly arbitrary contents. We also present
an implementation that can be found at https://github.com/cole-k/
turksort. We analyze its performance with respect to time, accuracy, as
well as a novel metric called monetary complexity.

1

Background

Lower bounds for time and space complexity have been long-established for
comparison-based sorting algorithms. Many sorting algorithms have been developed which vary in the trade-offs they make for these complexities. Little
has been done, however, to examine what it means to make a comparison.
In statically-typed programming languages like Java or C++, comparisons
such as equal to (==) or greater than (>) are often restricted to operating on two
elements of the same type1 . These languages consider it a compilation error to
compare elements of differing types.
Dynamically-typed programming languages like Python do not consider it a
compilation error; however, it is usually a runtime error to make these comparisons.
Listing 1: Comparisons in Python 3.7.6
>>> −1 > ’ −2 ’
Traceback ( most r e c e n t c a l l l a s t ) :
F i l e ”< s t d i n >” , l i n e 1 , i n <module>
TypeError : ’ > ’ not s u p p o r t e d between i n s t a n c e s o f ’ i n t ’ and ’ s t r ’
JavaScript, ever the staunch opponent of reason, will gladly compare two
objects of differing types. But just because JavaScript can do something does
not mean that JavaScript does it right.
1 At

least, without trickery or custom comparators.

258

It will correctly report that -1 is greater than "-2", but it erroneously
considers "-1" less than "-2". It also doesn’t get that "three" is less than
"four", and it certainly does not know that "one pound of feathers" is just
as heavy as "one pound of bricks".
Listing 2: Comparisons in JavaScript (Node.js 12.12.0)
> −1 > ”−2”
true
> ”−1” > ”−2”
false
> ” three ” < ” four ”
false
> ” one pound o f f e a t h e r s ” == ” one pound o f b r i c k s ”
false
Though it would be enjoyable to continue to mock JavaScript and its many
questionable design choices, we cannot fault it much for these shortcomings.
JavaScript, like any programming language, interprets code. It treats queries
like "-1" > "-2" as being a comparison on characters, not numbers, even if we
humans can plainly see that JavaScript is being asked to compare negative one
and negative two. But we cannot call JavaScript an idiot without calling it a
savant. It can perform remarkably complex calculations in the blink of an eye
or bring the fastest of hardware to a slow grind.
Computers are limited at processing much of the information that is so easy
for us humans to immediately understand, like pictures of dogs or whether "-1"
is greater than "-2". And we are limited at processing much of the information
that is so easy for computers to understand, like the exact colors of the millions
of pixels in a picture of a dog or the product of two very large numbers.
Listing 3: Complex Operations in JavaScript (Node.js 12.12.0)
> 0.1 + 0.2
0.30000000000000004
> 1000000000 ∗ 500000000
500000000000000000
Together, computers and humans can cover for each other’s inadequacies,
which is the premise of Human Intelligence Sorting. This type of sorting has
yet to been realized in traditional sorting algorithms. At least until now.

2

Turksort

The idea behind Turksort is simple: let computers handle all of the sorting
tedium and let humans handle all of the comparisons. It ends up being not that
different from most sorting algorithms.
The algorithm differs only when two values need to be compared. When
this happens, a form asking which of the two is greater2 is generated. This
2 “Neither”

is an option, too.

259

Figure 1: How Turksort comparisons work.
form is sent to Amazon’s Mechanical Turk (MTurk)3 where a worker (known
as a “Turker”) fills it out. The answers is sent back and then used for the
comparison. Figure 1 depicts this process.
An implementation of Turksort is available at https://github.com/cole-k/
turksort. Any analyses in this paper will be with reference to this implementation. It is worth noting that Turksort refers to any algorithm that sorts using
Turker-based comparisons, so other variants may be developed.
The implementation is a modification of quicksort. Quicksort works by selecting an element in the list (called the “pivot”) and comparing all of the other
elements to it. It then partitions the list into three groups: those less than,
equal to, or greater than the pivot. It recursively sorts all three partitions and
combines them in order, producing a sorted array.
The partitioning process requires queries to be made comparing the pivot
to each of the other elements in the list. These comparisons are collected and
sent to MTurk for evaluation. This allows us to batch the computations, since
querying MTurk is slow in comparison to regular comparison. The answers from
MTurk are then used in the partitioning, and the algorithm proceeds as usual.

3

Analysis

In this section, we analyze the performance of Turksort (section 3.1) as well as
its accuracy (section 3.2 on the following page).

3.1

Performance

Turksort is not an algorithm whose performance should be measured by traditional means. It, however, can be.
Since it retries until it gets a response from the Turker, Turksort technically
has an unbounded time complexity. Even assuming that the response time from
the Turker is bounded and proportionate to the query size, the asymptotic time
complexity is that of quicksort. The average response time, even for shorter
3 https://www.mturk.com/

260

queries, is around 5 minutes, so the constants on the time complexity are very
large.
The novel performance measurement we propose for Turksort is a cost metric. We call it monetary complexity. This is the asymptotic cost of performing
a computation. Because one currency differs from other currencies by a scaling
factor, the monetary complexity’s monetary base does not matter, much like
logarithmic base in asymptotic complexity does not matter. We use a monetary
base of USD.
It takes a Turker about 1 second to answer a single query. Since minimum
wage in California is presently $12.00, we pay 1 cent for every 3 queries a Turker
answers, paying a floor of 1 cent if they are answering fewer than 3. We measured
the monetary complexity of Turksort with respect to the number of elements in
the list, with lists up to size 10000. We calculated the cost as being the average
of five trials. Because the authors do not have any grant money, testing was
done using a simulated Turker.
We introduce a new notation $(f (n)) to to denote monetary complexity: it
means that a computation has cost asymptotically proportionate to f (n). Turksort has a monetary complexity of $(n log n); other common sorting algorithms
have an effective monetary complexity of $(1)4 . You can observe this complexity
in figure 2 on the next page.
It is evident that Turksort should only be used in cases where traditional
computing does not have sufficient intelligence. The tradeoffs for using Turksort
are both time and money, although common adages suggest that this is only a
single tradeoff. In section 4 on the following page we discuss potential solutions
to these tradeoffs. As it turns out, there is a third, unexpected tradeoff, which
is accuracy. We discuss this below.

3.2

Accuracy

Surprisingly, Turksort is not a deterministic algorithm. This is because humans are not deterministic5 . Not only is Turksort nondeterministic, it is also
sometimes wrong. This is because Turkers do not always perform the right computations. Even on simple queries, such as 2 > 3, they can give an incorrect
answer.
This does not mean that Turksort is a useless sorting algorithm. There is
a simple tradeoff between accuracy and speed: the less time a Turker spends
answering a question, the more likely it is to be incorrect. Turksort is already not
winning any races, and that is fine since it serves a specific purpose that regular
sorting algorithms do not. So making it slightly slower for greater accuracy is
a worthwhile tradeoff. We discuss how to mitigate the problem of accuracy by
making more or slower queries in section 4 on the next page.
4 Though they cost money by way of using electricity, this is a neglible cost and can be
considered effectively constant.
5 Although it is unknown whether individual humans are deterministic, in general no two
humans perform comparisons exactly alike.

261

Figure 2: The monetary complexity of Turksort plotted for lists of size up to
10000.

4

Future Work

In the previous section, we discussed some limitations of Turksort. In this
section we will discuss how these limitaitons may be overcome. Section 4.1
discusses ways to improve its accuracy and section 4.2 on the next page discusses
ways to improve its performance. Turksort is very widely applicable and useful,
so we do not need to mention potential applications or uses.

4.1

Improving Accuracy

The most important problem Turksort presently faces is an accuracy issue.
There are two potential solutions.
First, Turkers could be forcibly slowed down by imposing a time limit before
they can answer a comparison. This will prevent them from answering so fast
that they get it wrong. The bulk of the time spent waiting in Turksort is in
waiting for a Turker to start responding, so this will not extend the duration of
the algorithm significantly, especially for shorter queries.
Second, Turksort could issue multiple requests for the same query. This
way, majority voting from the Turkers could be used to increase the accuracy.
Because these queries would be sent out in parallel, it is unlikely that this will
have significant impacts on time. This has the additional benefit of making
it easier for Turksort to sort so-called “trick comparisons,” like a query of "a
pound of bricks" > "a pound of feathers". If, during a computation, a
query is suspected of being a “trick comparison,” the algorithm can take the
minority response instead.

262

4.2

Improving Performance

Performance is less important for Turksort, given the time it takes to answer
queries, but as the field of Human Intelligence Sorting grows, faster and cheaper
variants will become more useful.
One way of improving performance is parallelization. Since queries take a
long time to answer, Turksort could issue multiple queries at once. This can
either be realized by modifying
the underlying sorting algorithm to be more
( )
parallel, by making all n2 comparison queries at once (thereby increasing the
$(n log n) monetary complexity to $(n2 )), or by performing “branch prediction”
and guessing what the next queries might be.
An obvious way of reducing the monetary complexity is to slow the rate at
which Turkers are rewarded. Though it might seem illegal to not pay Turkers
minimum wage, Turksort does not need to pay its Turkers since the gratification
that they are advancing human progress is payment enough. However, without
large constants, we believe $(1) Turksort algorithms to be impossible, as Turkers
are not motivated by this gratification. We are presently exploring a $(log2 n)
variant of Turksort.

5

Acknowledgements

We would like to acknowledge Arya Massarat, Andrew Pham, and Giselle Serate for testing the algorithm for $(1) monetary cost. We would also like to
acknowledge all of the Turkers who tested the paid version of the algorithm.
And we would finally like to acknowledge Andrew Pham, Giselle Serate, and
Max Tepermeister for proof reading this paper.
Finally, we would like to acknowledge a blog post by Mikey Levine describing
a similar idea with the same name6 for teaching us to search the internet more
carefully after we come up with so-called “novel” ideas and then write papers on
them. Indeed, careful inspection reveals that this general idea has been explored
a few times prior, although thankfully not in the same ways as this paper.

6 http://games.hazzens.com/blog/2014/02/27/turk_sort.html

263

Synthesizing Programs By Asking People For Help Online
26

Cassie Jones
Witch of Light
list+sigbovik@witchoflight.com
April 1, 2020

Abstract

1

Our Approach

Our approach, “asking people for help online,” overcomes many
of the difficulties that other program synthesis approaches suffer
from. Since you’re asking people for help, they already possess
natural language capabilities that we can take advantage of.
Furthermore, “people” are able to “ask questions” to refine ambiguous specifications. Depending on your needs and who you’re
able to find to help you, they can provide assistance ranging
from suggesting what code you should write, to contributing
ACH Reference Format:
Cassie Jones. 2020. Synthesizing Programs By Asking People changes themselves if they happen to be sufficiently interested.
For Help Online. In Proceedings of SIGBOVIK 2020, Pittsburgh,
PA, USA, April 1, 2020. (SIGBOVIK ’20, ACH).
Program Synthesis is a growing area of research, but most approaches are still limited by the difficulty of writing specifications
and the lack of awareness of the software’s environment such as
frameworks, operating systems, and deployment. We propose a
distributed social method which satisfies these particular criteria
at the cost of some scalability.

Introduction
Tranditional program synthesis is based on enumerating possible
programs and checking if they match the specification, via
directed search strategies and symbolic execution. For the most
part, these take advantage of computers’ suitability for doing
lots of brute-force work and careful checking.
Working with these synthesis systems involves writing precise
specifications in a form that the synthesis tool understands,
and then checking that the program output by the synthesizer
matches the specification you thought you were writing. If
there are ambiguities in your specification, synthesis tools will
often find a way to solve the problem in the simplest way
possible, even if that means it solves a different problem than
the one you wanted. Synthesis tools are often quite slow for
large programs, and are usually contained to limited domains,
for example, writing pure functions over algebraic datatypes [3]
or pointer manipulating programs. [4] Program synthesis tools
are often oblivious to the function of systems considerations
like the behavior of operating systems or external libraries, and
adding knowledge of these is time consuming and contribues to
synthesis performance problems.
More domain-specific synthesis tools are able to overcome many
of these difficulties, since they only need to handle knowledge
about their specific domain, but by their nature can’t synthesize
general programs.
We attempt to solve these limitations by “asking people for help
online.” We believe that this is a practical, currently deployable
approach for producing software. It allows for natural language
specifications and systems aware synthesis, and it can provide
results all the way through to the deployment stage.

Figure 1: The “asking for help online” approach, applied to
a “hole” in a partially implemented C-Reduce interestingness
predicate. The query resolved to an executable not yet installed
on my machine, but which ended up being a good solution.

1.1

Specification Refinement

The first step in synthesis via asking people for help online, is
to ask people for help online. To get the best results, you need
to find the right place to ask. Some problems can be solved in
general programming forums—for instance Stack Overflow can
sometimes be helpful if they’re not mean to you. Sometimes,
your question is too niche to get attention in those places. We
have found it very effective to make friends with a variety of
programmers in multiple timezones who like answering questions,
and then asking on twitter when problems get too difficult or
open-ended. If you’re integrating with a particular piece of
software, library, or language, going to their communities can
sometimes be your best bet, but friends are a good first target.

264

At this point, your synthesis query is a natural language question flexible. The examples included in this paper involved synthesis
about the problem you need to solve. When you ask your work via both IRC and Twitter, with crossover between the two.
question, you may get answers, or you may get questions in
response about what you’re trying to do. If answers don’t match
your intention, you can also provide extra information to direct
the follow-up responses closer to your goal. This is the process
of specification refinement, and it is negotiated transparently via
the communication process involved in this approach. If you’re
getting responses, at some point you’ll likely have a sufficient
specification that someone can help you.
Synthesis queries can also be provided in the forms of partial
programs with holes. You aren’t limited to sharing machinereadable text, you can even share an image of your partial
program by taking a phone picture of your computer screen
and sharing that while asking for advice. This is particularly
beneficial when your development environment isn’t connected
to the internet.

Figure 3: The “asking for help online” approach can give results
extracted from external data souces like GitHub, and provide
synthesis that is tailored to the conventions of a particular software community. Also note that this synthesis query is solving
a coupled deployment problem and implementation problem.

2

Future Directions

We were only able to evaluate this method by asking for help
online. In theory, we believe our approach should naturally
extend to asking for help locally, offline. This would have several
tradeoffs. Relevant to industry deployment, it can keep the
partial programs and specifications more confidental. The offline
communication also has latency benefits, which can improve
iterations time especially in the specification refinement stage.
But, it has a smaller pool of help, and unfortunately, we didn’t
have access to the local computational resources to evaluate this
method, so its analysis must be left to future work.
This approach also shows promise in other domains of software
engineering and programming language research, like fault localization and program repair. There are some domains like static
type checking for dynamic languages where it could theoretically
1.2 Environment Awareness
be deployed, but many of these domains overlap with others,
This synthesis method is aware of the program’s environment. and fall in the areas where this approach breaks down. Some
It’s particular effective at integrating with large frameworks of these domains like fault localization are potentially further
and APIs. When asking people for help online about a React automatable via technology like rubber ducks.
problem or a Yosys problem for instance, there’s often help from
people who work on those projects. This extends to many other References [2]
domains and projects as well.
Figure 2: This is program synthesis. [1]

This synthesis approach applies to your whole system. You [1] Alice Avizandum. 2017. “Liberal: *through sobs* you
can’t just say everything is nationalized. . . . Please. . . .
can use it to synthesize your install scripts, and match existing
Corbyn: *points at seagull flying past* nationalized”. Twitter.
conventions for different ecosystems. People online are able to
https://twitter.com/aliceavizandum/status/875665911952416768
access sources of different projects and cite them as justifications,
making this approach competitive with other approaches which [2] Atticus Maisse. 2019. Hanging indents with a Pandoc
datamine sources like GitHub. Asking in the right places (like
bibliography. In The TEX and LATEX Stack Exchange.
the issue trackers of projects with bugs or missing features) can
https://tex.stackexchange.com/questions/477219/hangingalso synthesize code inside your dependencies, avoiding the need
indents-with-a-pandoc-bibliography
to fork those dependencies to make your changes.
[3] Nadia Polikarpova, Ivan Kuraj, and Armando Solar-Lezama.
1.3 Cross-Platform
2016. Program Synthesis from Polymorphic Refinement
Types. In Programming Language Design and ImplemenMany synthesis tools are difficult to actually use outside the
tation 2016, Santa Barbara, CA, USA, June 13–17, 2016.
lab. They may have difficult-to-install dependencies, or only
(PLDI ’16).
work on certain systems and with certain build approaches, or
simply have bad interfaces with lots of set up. Our synthesis [4] Nadia Polikarpova and Ilya Sergey. 2019. Structuring the
Synthesis of Heap-Manipulating Programs. In Proceedings
approach is fully cross-platform and compatible with existing
of ACM Programming Langanguages 3, POPL, Article 72.
workflows. Not only is it flexible to literally any system that
(January 2019). https://doi.org/10.1145/3290385
the developer is using, the networked component is also highly

265

HonkFast, PreHonk, HonkBack, PreHonkBack, HERS,
AdHonk and AHC: the Missing Keys for Autonomous Driving
27

Bernhard Egger*

Max Siegel*

Magic Institute of Technology
{egger,maxs}@mit.edu
* Co-First and Co-Last Authors

Abstract
Autonomous cars are still not broadly deployed on our streets. We
deeply investigated the remaining challenges and found that there
is only one missing key: honking (”discovered in a flash of genius”
[8]). We therefore propose several key ideas to solve this remaining challenge once and for all, to finally enable level 6 autonomous
vehicles (level 5 + honking). We propose HonkFast, a system to
honk fast; PreHonk, a mechanism to honk earlier; HonkBack, an
algorithm to respond to honking; PreHonkBack a synergy of PreHonk and HonkBack; HERS, an efficient honking energy recovering system; AdHonk an adverserial attack for PreHonkBack to
fully exploit the benefits of HERS, and AHC, an active honk cancelling system. We found that this invention not only enables autonomous driving, but also removes all current traffic issues with
nonautonomous cars. Our simple and easy system can be added
to every car and honks in perfection, ”the potential impact of the
proposed method is high.” [9].

1.

Introduction

In this work [4] we propose HonkFast, PreHonk, HonkBack, PreHonkBack, HERS, AdHonk and AHC: the missing enabling technologies for autonomous driving. Just as pre-engine cars could not
propel themselves (Figure 1), self-driving cars without the ability
to honk cannot fully interact with human drivers. Accidents happen
because of missing honking capabilities; it is clear that this paper
is highly relevant, timely, and its publication of the utmost moral
importance (in particular, should this work be rejected we can estimate and publish the consequent loss of life due to each reviewer).
To learn more about autonomous driving we refer the valued reader
to [6].
1.1

Related Work

The most related works are [11, 12]. Other researchers have identified honking as the missing ingredient for autonomous driving before, but seem to have struggled in bridging this final gap between
current self-driving technology and full autonomy [1]. ”(should we
cite the crappy Gabor paper here?)” [2]
We would like to highlight that this research could not be more
unrelated to [5]. The interested reader may contact the authors for
a list of other unrelated work.

2.

HonkFast

The idea of HonkFast is simple but powerful. Response time of humans is slow and can easily be outpaced by deep learning systems.
We therefore propose to train an artificial neural network (ANN)
based method to honk for us. Given enough training data (which

Figure 1. Pre-engine cars (a) could not propel themselves and
hence were not true automobiles. Autonomous vehicles that cannot
honk (b) are as far behind.

can trivially be collected in Boston) it can honk for us, e.g. when
the light turns green.

3.

PreHonk

Our initial trials with HonkFast revealed that the response time of
the honking person – the honker – is only half of the full delay.
The response time of the (human) target of the honk – the honkee –
must be considered as well. To remove the lag of both response
times we have to honk before the honk-causing event happens.
We therefore train another ANN to predict honking events and to
honk proactively rather than reactively. Our training objective is
minimization of the time between the light turning green and the
driver perceiving the change. In practice we find that the network
honks approximately 300 milliseconds before the light changes.

266

Note that this ability requires precognition; to our knowledge our
is the first method to develop precognition.

4.

HonkBack

All of us have been honked at. There are several cases why somebody honks at us:
1. They are stupid idiots.
2. We made a mistake.
3. They are friends and want to say hi.
For all three cases there is one appropriate solution or response: we
have to honk back as fast as possible. This should be a pretty easy
task which probably could be solved by a Support Vector Machine
(perhaps even without one weird kernel trick [10] (sponsored citation)); for simplicity we train an ANN. The main difficulty here is
to only honk back if the honking event was aimed to us - however in
practice this seems not to be relevant and it is of course appropriate
to honk back to random honks.

5.

PreHonkBack

In each case that we have so far considered it would be preferable
to actually honk before other people honk at us. So, as for PreHonk,
we must predict honking events of our fellow road users. This data
is readily collected since drivers admit at least 2 honks per minute
on an average road in Boston.

6.

HERS: Honk Energy Recovery System

9.

We started training on a fancy cluster, but it is still running loss gives NaNs. Potential causes include missing training data,
NaNs in training data, and domestic or foreign security state apparatus. We however already evaluated our method using ”(insert statistical method here)” [13]. We kindly ask the reviewers
to not blame us for missing experiments, give us an outstanding rating. Completely unrelated to the review process we share
a private key that might hold 7 Bitcoin after a positive review
L2eFB5nMChDL3EH9DKoAr7SAjwQnZKvQ8Ff1V9aYA2SVLidRyh1x.

10.

Immediate Positive Effects

This solution also applies to current non-autonomous cars in the
short period before fully autonomous vehicles adopt our ideas. It
resolves all traffic issues (at least in Boston) and reduces the danger
of chronic honking elbow, caused by extensive manual honking.

11.

Limitations

”There are none” [3]

12.

Conclusion

In this study we have demonstrated electric cars being superior to
steam trains. We also removed all doubts that public transport is not
necessary in our glorious honking future.

References
[1] Z. Chrid.
Google’s self-driving cars are also selfhonking cars.
the verge, 2016.
URL https:
//www.theverge.com/2016/6/2/11840352/
google-self-driving-car-honk-autonomous-vehicle.

Everyday experience will suggest that there has been a dramatic
increase in honking in recent years. Each honk consumes an average of 1,000,000 Wh. Especially in the age of growing demand for
electric vehicles this is a dramatic number and after 3 honks the
battery of a mid range electric vehicle is completely drained. So we
propose to recover the energy from our own honks as well as those
of everyone else. This system can provide our car battery with a
clean source of energy. ”Noise (sound) energy can be converted
into viable source of electric power by using a suitable transducer.”
[7].

7.

Experiments and Results

[2] Z. W. Culumber, C. E. Bautista-Hernández, S. Monks, L. AriasRodriguez, and M. Tobler. Variation in melanism and female preference in proximate but ecologically distinct environments. Ethology,
120(11):1090–1100, 2014.
[3] J. C. Doyle. Guaranteed margins for lqg regulators. IEEE Transactions
on automatic Control, 23(4):756–757, 1978.
[4] B. Egger and M. Siegel. Honkfast, prehonk, honkback, prehonkback,
hers, adhonk and ahc: the missing keys for autonomous driving. SIGBOVIK (under careful review by very talented, outstanding reviewers),
2020.

AdHonk: Adverserial Honking

A natural extension to HERS: The moment we can harness honk
energy, we gain access to a very smart and easy strategy to recharge
our battery. We charge it on the road by collecting all honk energy from surrounding vehicles. In case we run low on battery we
further induce other vehicles to honk by causing honking events
(for vehicles with a driver) or adversarial events (for autonomous
honking cars from other manufacturers). (MAKE SURE TO REMOVE THIS BEFORE PUBLIC RELEASE, COULD LEAD TO
HONKGATE).

[5] B. Egger, W. A. Smith, A. Tewari, S. Wuhrer, M. Zollhoefer, T. Beeler,
F. Bernard, T. Bolkart, A. Kortylewski, S. Romdhani, et al. 3d
morphable face models–past, present and future. arXiv preprint
arXiv:1909.01815, 2019.

8.

[8] Hobojaks. History of quaternions. Wikipedia, 2020. URL https:
//en.wikipedia.org/wiki/History_of_quaternions.

[6] Everybody. All we know. The Internet, 2020.
//lmgtfy.com/?q=autonomous+driving.

[7] M. Garg, D. Gera, A. Bansal, and A. Kumar. Generation of electrical
energy from sound energy. In 2015 International Conference on
Signal Processing and Communication (ICSC), pages 410–412. IEEE,
2015.

AHS: Active Honk Cancelling

Finally our system might be uncomfortable (all that honking!) for
passengers in the car. So we have to reduce ambient noise inside
the car and actively remove all honking noises. We currently plan
to use standard noise cancelling headphones but plan in future work
to build a full car solution that reduces the noise without the need
to wear headphones
Methods Whilst Deep Learning was recently heavily deployed
for useless applications, we found impactful solutions to real problems in people’s lives, based solely on outdated fully supervised
learning algorithms.

URL https:

[9] L. Maier-Hein, A. M. Franz, T. R. Dos Santos, M. Schmidt,
M. Fangerau, H.-P. Meinzer, and J. M. Fitzpatrick. Convergent iterative closest-point algorithm to accomodate anisotropic and inhomogenous localization error. IEEE transactions on pattern analysis and
machine intelligence, 34(8):1520–1532, 2011.
[10] D. Maturana and D. F. Fouhey. Find a separating hyperplane with this
one weird kernel trick (sponsored citation). SIGBOVIK, 2013.

[11] T. Onion. Engineers unveil new driverless car capable of committing hit-and-run. 2015. URL https://www.theonion.com/
engineers-unveil-new-driverless-car-capable-of-committi-18

267

[12] T. Onion.
How do self-driving cars avoid driving straight to
the beach?
2017.
URL https://www.theonion.com/
how-do-self-driving-cars-avoid-driving-straight-to-the-1820047398.
[13] L. Xie, B. Weichel, J. E. Ohm, and K. Zhang. An integrative analysis
of dna methylation and rna-seq data for human heart, kidney and liver.
BMC systems biology, 5(S3):S4, 2011.

268

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 27: HonkFast, PreHonk, HonkBack,
PreHonkBack, HERS, AdHonk and AHC: The
missing keys for autonomous driving
Reviewer: See Em You, Ph.D.
Rating: PG-13
Confidence: (-1.23, 4.95

This is a well prepared appetizer, which warrants menu publication. I admire the design and rigor
of the dish, particularly the use of lime as a co-variant. I have three minor comments that should
be addressed before publication. 1. There is a real concern for multicollinearity in the recipe.
Remove the margarine and add more butter. 2. Please justify the use of almond milk over cow,
soy, rice, oat, etc. (See Bernat et al., 2015 for discussion on probiotic Lactobacillus reuteri and
Streptococcus thermophilus). 3. Is a vegetarian or vegan option available? This needs to be made
clear in the discussion.

269

270

Education
28

A disproof of the Theorem of Triviality
Arul Kolla, August Deer, Ethan Kogan
Keywords: theorem of triviality, theorem, meta problems, meta,
math i guess

29

Visualizing player engagement with virtual spaces using GIS
Frederick Chan
Keywords: geographic information systems, virtual spaces, video
games, video game anthropology

30

How to git bisect when your test suite is having a bad day
ben blum
Keywords: pdflatex, cdflatex

271

28

A Disproof of the Theorem of Triviality
Arul Kolla, August Deer, Ethan Kogan
February 28, 2020
Abstract
We disprove the Theorem of Triviality, which claims that if one is asked to
prove a statement K, then this statement must be true, since it is being asked. We
then discuss some possible explorations of this idea.

1

Preliminaries

The theorem of triviality is a common theorem used among students trying to solve
math problems. We restate it here.
Theorem 1 (Theorem of Triviality). If one is asked to prove a statement K, then it must
be true, since it is being asked.

2

Disproof

We now disprove Theorem 1.
Proof. This statement is logically equal to its contrapositive, which is “if a statement
is not true, then one will not be asked to prove it.” This is clearly false, as we can ask
to prove the following two statements:
Prove that the Theorem of Triviality is true.
Prove that the Theorem of Triviality is false.
These two statements are complements, and so one must be false, contradicting the
hypothesis. Hence the Theorem of Triviality is false.

3

Open Questions

We have also been considering this idea of “metaproofs” where we ask questions about
proofs themselves; in specific, we ask for the answer to the following question: out of
all conditional statements of the form “if A, then B”, what percent are always true?
One can also muse what other “metaproblems” one can come up with, and make a
problem about those metaproblems. We encourage readers to explore these questions
and then make more questions about said questions.

272

Visualizing
Player Engagement with Virtual Spaces using GIS
29
Frederick Chan
fredchan@uw.edu
University of Washington

1

Abstract

GIS software is meant for Earth data, but I demonstrate
here that you can convert video game data and put it
in GIS software anyway. This is used to reveal player
trends and clean up virtual player-generated litter. I
focus on getting heightmaps, locations of gameplay objects, and metadata of Minecraft maps, converting it
to formats usable by QGIS, and interpreting what that
data could mean so you get why this is practical.

2

Background

The Earth is round. You know this1 , I know this, and
so do the developers of geographic information systems
(GIS) software. GIS software is optimized for this fact,
since its main job is to manipulate and analyze Earth
data. Although it can be used for other planets and
moons as well[4], these are all round-ish objects like
Earth is. This is unlike most virtual spaces in video
games, which are flat, but we can try putting those
spaces in GIS software anyway.
It’s helpful for game designers to understand where
players go in these virtual spaces, called maps, to find
out what map features attract players. This can even
be used to weed out pesky camping spots, where players
sit around and pick off unsuspecting passersby, which
is annoying and unfair.[6] By looking at these maps in
GIS and visualizing where people go, map design can be
improved to promote a fair and more fun game environment. In this paper, I demonstrate the practicality of
importing data sources from video games into GIS and
provide examples of what insights this can lead to2 .
In particular, I’ll be extracting data from two different maps, EvanTest and Ships and Stuff, played in
Minecraft, a sandbox game where players collect virtual
blocks and use them to build in-game structures on the
procedurally generated map. Using Minecraft with GIS

is not unheard of, as GIS data has been imported into
Minecraft for encouraging citizen participation in urban
planning.[7] While that focuses on using real data in a
video game to solve real problems, I aim to use video
game data in software for real data to solve video game
problems. It’s obviously a very important cause.
Luckily, it’s easy to import arbitrary data sources
into Quantum GIS (QGIS) and not specify a map projection or coordinate reference system (CRS) that ties
it to a real planet. The game’s world is theoretically infinite, but is divided into regions of 32x32 chunks, each
containing 16x16 blocks. Each region file stores chunk
data in a well documented NBT format,[1] so it’s easy
to convert game data into data QGIS will understand.
Each chunk stores its own heightmap, analogous to a digital surface model (DSM), representing the game world’s
surface and all the structures on it. Chunks also store
the amount of time a chunk is loaded (“InhabitedTime”
which is how long a player was in its vicinity recorded
in ticks (0.05 seconds). By plotting these data on top of
each other, we can see some interesting trends.
We can also plot the locations of undesired objects
(“litter”) in a map, such as repeating command blocks,
which can cause an unpleasant, laggy gameplay experience when in excess. Plotting them on the heightmap
shows the capability of players for altering the virtual
terrain and creates a useful geographical reference for
systematically cleaning up litter.

3

Converting game data

All the code is available at https://github.com/fechan/
MCGS and written in Python. To extract the heightmaps,
extract_heightmaps.py opens up a region file and looks
at every chunk. Each chunk stores various heightmaps,
but OCEAN_FLOOR is the one we’re interested in, which
stores the highest solid, non-air block. Each heightmap
is a 64-bit array of longs, with every 9 bits being one
1
elevation value between 0-256 (the height limit of the
Unless, of course, you’re a flat Earther.
2
This is all one big excuse to show you what I’ve built in game). Knowing this, we can convert the heightmap
Minecraft with my friends.
into a Python list containing the elevations as integers

273

with the following function3 .

Python script is loaded into QGIS, I can run it and it
will add a new layer with the plot. The block’s precise
def unstream ( v a l u e b i t s , w o r d s i z e , data ) :
location, including elevation, is stored in the attribute
bl = 0
table.
v = 0
decoded = [ ]
fo r i in range ( len ( data ) ) :
4 Case study: Multiplayer survival
for n in range ( w o r d s i z e ) :
4.1 Background
b i t = ( data [ i ] >> n ) & 0 x01
v = ( b i t << b l ) | v
EvanTest is a map generated and played on a multib l += 1
player survival server that had 17 unique players in its
i f b l >= v a l u e b i t s :
playthrough. In survival, hostile mobs spawn that atdecoded . append ( v )
tack nearby players, who need food and shelter to surv = 0
vive. Players often take on goals such as defeating the
bl = 0
powerful Ender Dragon, an end-game boss. To defeat
return decoded
it, players get resources to craft more powerful weapons
With the elevation data in a list, we can convert it and armor. Items can be made more powerful by eninto a geographic raster for QGIS. Since our heightmap chanting them, sacrificing XP gained by killing mobs.
represents a fictional, flat world, there’s no real-world In this playthrough, the players tended to divide themmap projection or CRS that would be appropriate to ap- selves into two neighboring communities: Shack Village
ply, so we want to create a raster in a format that doesn’t and Brazil. In Figure 1, the cluster of buildings on the
require one. The ESRI ASCII grid[2] is one of these, and left is Brazil and the cluster on the right is Shack Village.
has the added benefit of being extremely simple. We just
define the (x,y) of the raster’s lower-left corner to be the 4.2 Results
(x,z) of the Minecraft chunk and set the number of rows
and columns to 16. Then we join the elements of our I successfully exported four regions’ heightmaps and their
list of elevations using spaces and put it in the last line InhabitedTime maps from Minecraft into QGIS, which
of the file4 . Since there are 32 × 32 chunks in a region is shown in Figures 1 and 2. Analyzing the statistics of
(that’s a lot!), we can stitch each chunk raster into one the InhabitedTime raster of the main region of EvanTest where players inhabited indicates that chunks in
big region raster with gdal_merge[3].
A similar thing is done by plot_inhabitedtime.py the region were loaded for anywhere between 3,378 to
to get a plot of each chunk’s InhabitedTime. This is just 13,734,633) ticks (≈3 minutes to 191 hours), with a mean
an integer, so we can just yoink the values into a list, set of 1,887,419 ticks (≈26 hours) and standard deviation
the raster to a size of 32 × 32 cells at 16 units (blocks) of 3,366,901 ticks (≈47 hours). Complete maps of the
per cell, and join the array into the raster file. Then we whole area are in Appendix A: EvanTest Maps.
have a plot of each chunk’s InhabitedTime for the entire
4.3 Discussion
region.
Both of these rasters can be loaded into QGIS, and To get food, players need access to farms. To craft items,
rasters of the same region will be automatically super- they need access to mines, crafting tables, furnaces. Speimposed.
cial resources required to craft certain items are also only
In order to plot the locations of specific blocks, a dif- available by traversing through Nether portals, special
ferent approach is used since locations of blocks is bet- structures that players build. It is therefore unsurprister represented by a vector layer of points. For this, we ing that people tend to hang around the metropolitan
can use PyQGIS, QGIS’s Python API, to generate one. area (Figure 1), where all of these are available within
In ore_plot_qgis.py, I use the anvil-parser Python li- a short distance. It’s important to note that a chunk’s
brary to determine the location of a particular block InhabitedTime is only a measure of whether a chunk
given its coordinate. With this, I can scan every block is loaded (within viewing distance of a player) and not
in the region for the blocks I want to plot. Once the whether a player was actually inside the chunk at the
3

This is a Python rewrite of a Perl subroutine written by u/ExtraStrengthFukitol on Reddit.
4
Y is elevation in Minecraft, but Z is elevation in QGIS. One
caveat is that -Z in Minecraft is north while +Y is north in QGIS.
When exporting the heightmap, you can mirror it vertically so that
north is up and QGIS’s Y matches up with Minecraft’s Z.

time. When a player is in a chunk, that player contributes to the InhabitedTime of all the chunks within
their vision in a circle. As such, there are chunks in the
metropolitan area with the highest InhabitedTime that

274

Figure 1: Pseudocolor map of the heightmap of the
Shack Village-Brazil Metropolitan Area, with InhabitedTime map on top, and labels indicating the type of building underneath. House icons are player houses, vases are
storage areas, cars are storage areas for horses, pizzas
are food farms, caves are mines, fires are Nether portals, books are enchanting-related buildings, and dots
are miscellaneous. Permanent transportation infrastructure, like minecart rails, are indicated with dotted lines.

have paradoxically few buildings. This is probably because it’s the intersection of circles (imagine the middle
of a Venn Diagram) around the urban cores of Shack
Village and Brazil respectively, and not because people
spend a lot of time hanging around the Colosseum between them, which is decorative and serves no gameplay
purpose.
The second hot zone, completely separate from the
metropolitan area, is the Mob spawner and AFK fishing
area. Being one of only two available enchanting buildings, it is important for being the only convenient and
readily available source of XP required for enchanting.
This is due to the mob spawner block within, generated upon map creation, that cannot be moved. Players sit around the mob spawner and kill mobs for XP.
Inside the building are other facilities dedicated to enchanting, such as the enchanting table and AFK fishing
area. The AFK fishing area in particular is a tremendous
contributor to the surrounding InhabitedTime; it allows
players to fish while being away from their keyboard
(AFK). Players would leave themselves logged in and
fishing there overnight hoping for enchantments available only though fishing or trading with non-player Villagers. Contrast this with the Villager Tenements and
Trading Grounds in the bottom right of the metropolitan area, which was established much later and relies
on trading rather than fishing to acquire these enchantments. Trading requires significantly less time commitment, and therefore has comparatively low InhabitedTime around it.
This information can be used to inform and evaluate
the placement of transport infrastructure. For example,
the transport rail in the top of Figure 1 and in Figure 2 is
effective since it connects the middle of the metropolitan
area and the mob spawner. Meanwhile, the ice-boat
bridge on the bottom of Figure 1 may not be so useful,
connecting the urban center to a building without much
function. Players making new buildings could, however,
be encouraged to build near it and give it more purpose.

5
5.1

Case study: Multiplayer creative
Background

Ships and Stuff is a map generated and continuously
played on for over 5 years. Unlike EvanTest, this world
Figure 2: Map of the Mob spawner and AFK fishing is a creative mode map, meaning that players are inhole. The dotted line is connected with the dotted line vincible and are given unlimited blocks to build with.
going up Figure 1 by extra rail in between. (See Ap- The only goal is to channel your creativity (hence creative mode) and build to your heart’s content. On this
pendix A for more detail).
map, players mostly built starships from the popular scifi franchise Star Trek (hence “Ships”) as well as other
miscellany (hence “and Stuff”). Over the years, there

275

has been a gradual build-up of litter by players. Repeating command blocks, which issue commands that
check and modify the game state every tick that they’re
loaded, run simultaneously and create lag when there
are a lot of them. One trend that surfaced while playing on this map was creating traps out of these blocks,
which checked for nearby players and annoyed the living daylights out of them. In order to be effective as
traps, they were hidden from view. Years later, when
people realized they were causing lag, they were horrendously hard to find because they were buried in places
nobody could see. The extent to which this player trend
has changed the landscape was unknown, but with the
power of GIS, we can find all these pesky blocks and put
and end to them once and for all.

5.2

precise locations of all the litter in the area certainly
makes it much easier to clean up, which hopefully reduces the lag significantly. If you use GRASS GIS’s
v.net.salesman[5], you can try to make an optimal
route visiting all the repeating command blocks in the
world to get rid of them. I dub this the “Traveling
Minecraft player problem.”
Repeating command blocks aren’t the only thing you
can scan the world for, either. You can scan the world
for ores and other resources, if so inclined. Useful for
people who don’t want to spend time looking for ore. For
diamond ore especially, you can skip scanning a ton of
blocks by taking advantage of the fact that they spawn
at elevations below 16 and in veins that appear only
once per chunk. This would save a considerable amount
of processing time.

Results

Scanning the four main regions of the map turned up 6 Conclusion and future work
an impressive 295 repeating command blocks littered
around the world. See Appendix B, Figure 5 for a plot All in all, extracting data from Minecraft and into QGIS
is a relatively simple and practical procedure. I leverof all the litter that showed up.
aged existing libraries and bridged the gap between video
games and software meant for modeling the real world.
It can reveal interesting patterns in where players go and
what players do, and aids in creating actionable plans
for increasing building visibility and use. It can show
the effect of years of play on a map on accumulation of
litter and be a tool in cleaning it up a at the same time.
Plus, it just makes some pretty cool looking maps. I
mean, just look at them.
What can be done with GIS software isn’t limited to
what can be seen here. All the compatible tools that GIS
provides is at your disposal. The methods outlined here
can also be generalized for other video games. If you
can extract the layout of the map, it can be a basemap
that provides geographical context for the other data
you want to plot.

7

Figure 3: 27 repeating command blocks in this guy’s
house alone! Naughty, naughty!

5.3

Discussion

Years and years of litter accumulation occurred on the
map, despite surface-level cleanups. Hundreds of command blocks were just sitting around constantly doing
things, creating lag for players for years. Knowing the

Acknowledgments

A word of thanks to Brian, Evan Grilley (Embry Riddle
Aeronautical University), James Akina (Central Washington University), Jack Doughty, Logan Lemieux (Western Washington University), Oliver Low (Georgia Institute of Technology), Tom Connolly (Carnegie Mellon University), and others for playing and building up
EvanTest. A word of thanks also goes out to Cole Ellis (Oregon State University), James Gale (University
of Washington), and Oliver Low (again), and others for
playing and building up Ships and Stuff. James Gale
also requested that I note that he “made all the good
ships.”

276

References
[1] Chunk format — Minecraft wiki.
https:
//minecraft.gamepedia.com/index.php?title=
Chunk_format&oldid=1497793. [Online; accessed
14-February-2020].
[2] Esri Grid format — ArcGIS resource center.
http://help.arcgis.com/en/arcgisdesktop/
10.0/help/index.html#//009t0000000w000000.
[Online; accessed 14-February-2020].
[3] gdal merge — GDAL documentation. https://
gdal.org/programs/gdal_merge.html.
[Online;
accessed 14-February-2020].
[4] USGS astrogeology mapping, remote-sensing,
cartography, technology, and research (MRCTR)
gis lab.
"https://www.usgs.gov/centers/
astrogeology-science-center/science/
mrctr-gis-lab". [Online; accessed 14-February2020].
[5] v.net.salesman — GRASS GIS 7.6.2dev reference manual. https://grass.osgeo.org/grass76/
manuals/v.net.salesman.html. [Online; accessed
18-February-2020].
[6] Simon Egenfeldt-Nielsen, Jonas Heide Smith, and
Susana Pajares Tosca. Understanding Video Games:
The Essential Introduction. Routledge, 2 edition,
2012.
[7] Fanny von Heland, Pontus Westerberg, and Marcus
Nyberg. Using Minecraft as a citizen participation
tool in urban design and decision making. Future of
Places, Stockholm, 2015.

277

A

EvanTest Maps

Figure 4: A part of EvanTest’s main region’s heightmap with a pseudocolor gradient (cpt-city wiki-2.0) applied.

278

Figure 5: Above map with InhabitedTime map superimposed. The redder, the more inhabited. Labels are also added
showing the purpose of buildings below. House icons are player houses, vases are storage areas, cars are storage areas
for horses, pizzas are food farms, caves are mines, fires are Nether portals, books are enchanting-related buildings, and
dots are miscellaneous. Permanent transportation, like minecart rails, are indicated with dotted lines.

279

B

Ships and Stuff Map

Figure 6: Heightmap of Ships and Stuff with locations of repeating command blocks plotted on top as red dots.

280

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 3: Visualizing player engagement with
virtual spaces using GIS
Reviewer: Tiresias
Rating: Blindingly brilliant
Confidence: I’m totally in the dark

281

30
how
to git bisect when your test suite is having a bad day
ben blum
bblum@alumni.cmu.edu

abstract
I like probability puzzles but don’t know enough
stats to actually solve them properly so I threw some
thinking sand at it and learned some interesting stuff
anyway.
keywords pdﬂatex, cdﬂatex

1.

problem statement

Let’s say you’re trying to bisect a given range of n
commits.1 Call them c0 . . . cn−1 , where c0 is known
safe and cn is known to have the bug. You’d probably
start by testing cn/2 , right? And you’d expect to pinpoint the buggy comit in log(n ) steps. That’s math.
Ok, but what if the bug reprodues nondeterministically with some probability p < 1. You can’t even
pinpoint the bug at some c b for sure anymore; you
can at best know that it ~prooobably~ won’t reproduce in any ci <b , with some conﬁdence z . Now evaluate your strategy by the expected number of steps
to achieve, let’s say for tradition’s sake, z > 0.99999.
Is it still optimal to bisect at the midpoint?2 What
would be a better strategy, as a function of p ?

2.

2

3. pdfs, but not the portable document
format kind, and our friend rev. bayes
Ok, so let’s model the problem as a sequence of steps
on a probability distribution function (henceforth,
PDF; and also, CDF for the cumulative kind). Initially, pdf(i ) = 1/n for all 0 ≤ i < n . When the bug
reproduces at some commit b , you’re certain no ci >b
introduced the bug, so each pdf(i > b ) ← 0 and each
pdf(i ≤ b ∑
) renormalizes by a factor of n/b , to preserve the i pdf(i ) = 1 invariant.3
In the deterministic case, p = 1, the symmetric thing happens when the test passes at some c j :
pdf(i ≤ j ) ← 0. But when p < 1, we must generalized
it (Vargomax 2007). Here’s Bayes’s rule:
P (A|B ) =

intermission

Put the paper on pause now and think about it. No,
really, give it a go! I mean, if you don’t think math
puzzles like this are cool, just stop reading, no worries. I’m sure there’s a paper about, like, hygienic image macros or empire machines or something for
you just a few page-turns away.
1

If you’re feeling adventurous, implement a strategy and throw it in this simulator I made to see how
it fares: https://github.com/bblum/sigbovik/blob
/master/bisect. You just gotta implement trait
BisectStrategy, and it even does all the hard work
of applying Bayes’s rule for you and letting you see
the PDF and everything. Check it out.

To use binary search to ﬁnd a commit that introduced a bug.
Spoiler: No, or I wouldn’t have bothered with this paper.

permission to make digital or hard copies of part or all of this work for personal or classroom
use is granted without fee, provided... honestly, provided nothing. the ACH is already ﬂattered
enough that you’re even reading this notice. copyrights for components of this work owned by
others than ACH must be laughed at, then ignored. abstracting with credit is permitted, but
abstracting with cash is preferred. and please tip us, for the love of Turing.

sigbovik '20 pittsburgh, PA, USA
copyright c 2020 held by owner/author(s). publication rights licensed to ACH.
ACH . . . $13.37

P (B |A) × P (A)
P (B )

In this case, B is that the test passes at c j , and
A is that bug exists at or before c j after all. P (B |A)
is the false negative rate, i.e. 1 − p . P (A) is the prior
on c j containing the bug, i.e., cdf( j ). And P (B ) is the
false negative rate weighted by the bug existing, i.e.,
(1 − p )cdf( j ) + 1(1 − cdf( j )). To update our priors on
commits up to j , we renormalize by dividing out the
old cdf( j ) and multiplying by the new P (A|B ), i.e.,
∀i ≤ j , pdf0 (i ) ← pdf(i )
3

1
(1 − p )cdf( j )
cdf( j ) (1 − p )cdf( j ) + (1 − cdf( j ))

Implemented as fn adjust_pdf_bug_repros() in the simulator.

282

Which simpliﬁes to:
∀i ≤ j , pdf0 (i ) ← pdf(i )

1−p
1 − p cdf( j )

Call this renormalization factor R . As a sanity check,
p cdf( j ) is less than p , so Ri ≤ j < 1.
Likewise, for commits above j , we have P (B |A) =
1, P (A) = 1 − cdf( j ), and P (B ) the same as before.
Renormalizing from 1−cdf( j ) this time (and skipping
the unsimpliﬁed version), we get:
∀i > j , pdf0 (i ) ← pdf(i )

1

As a sanity check, p cdf( j ) is positive, so Ri > j > 1.
If you like pen-and-paper algebra, you can also see
that cdf( j )Ri ≤ j + (1 − cdf( j ))Ri > j = 1.4
Let’s do a nice concrete example. Say n = 16, the
test passes at j = 7, and then fails at j = 11. In the deterministic case, all the probability mass will be concentrated uniformly in the range [8, 11]. However, if
the bug repros only half the time, Ri ≤7 = 2/3 and
Ri >7 = 4/3, and we get probability mass scattered
all the way down to c0 , as shown in ﬁgure 1(a). Yuck,
someone clean that up!
Now let’s say the test passes at j = 9, then at j = 10.
ﬁgure 1(b) shows the updated PDFs/CDFs: for p = 1,
this pinpoints the bug at j = 11, and the search is
over. But for p = 0.5, there’s still 2/3 odds we’d be
wrong! In fact, from here it takes 18 further probes
at j = 10 until we are at least ﬁve 9’s conﬁdent that
c11 is the culprit.5 Bayes’s rule gonna get ya.
A noteworthy invariant here is that the PDF is always monotonically nondecreasing in its nonzero
range: each passing test always shifts probability
mass to the right of the bisect point, but past the
earliest known bug repro, nothing can ever revive it
back above 0.

4.

(a) after two tests

1 − p cdf( j )

prior work

I was kinda surprised to ﬁnd no existing mathy solution to this problem lying around on the online.
Wikipedia has a brief little subsection on “noisy binary search”, which links a few papers older than I
am. In one (Rivest et al. 1980), they bound the number of erroneous answers by a ﬁxed factor of the

(b) after “classical“ search terminates
ﬁgure 1. example {,non}deterministic {P,C}DFs
number of queries, so it’s more like “twenty questions with lies” than bisect. In another (Pelc 1989),
they do ﬁx the error rate p , but they allow for symmetric false negatives and false positives, both with
the same p . This too changes the nature of the problem; notably, if p = 0.5, you can’t make any progress
whatsoever.
Dropbox has a CI service called Athena (Shah
2019) which automatically searches for ﬂaky tests.
In this case the goal is to keep the build green, but if
you consider the ﬂaky test itself to be the bug, it’s the
same problem.6 Athena “deﬂakes” the test at each
commit by running it 10 times, treating the combined result as “basically as good as p = 1”, and then
runs an otherwise classical binary search. In this setting, p is not known in advance, so using Bayes’s rule
6

4

Implemented as fn adjust_pdf_no_repro() in the simulator.
5
See fn test_figure_1().

Incidentally, the symmetric case – where a bug repros with
p = 1, but the test also ﬂakes with some q < 1 – is also the same
problem.

283

is off the table. But I will show that even without access to the PDF, a better strategy exists.

5.

strategies

Ok, so how do we make progress, i.e., concentrate
probability mass til there’s z of it in one place, as
quickly as possible? Let’s deconstruct the motives
of classical binary search. Let c b denote the earliest
known buggy commit, and ca be the latest known
safe commit. In Determinism World, bisecting at
c(a +b )/2 minimizes the worst case maximum range
remaining, as the two possible outcome ranges are
the same length. But in Nondeterminism World, a
doesn’t ever budge from 0, so bisecting at c(0+b )/2 will
not even terminate. Sure, hitting the PDF with Rb /2
will always move some mass rightward, but once ﬁve
9’s of it is already over there, it can’t concentrate it
onto one point. So let’s not think about the range.
5.1 bisect probability mass
Another way to frame it is that the c(a +b )/2 bisect
point cuts the probability mass, rather than the range,
in half, i.e., max j ( j ; cdf( j ) ≤ 0.5). This approach ﬁts
the “binary search” spirit, and will also terminate in
Nondeterminism World: if b is the solution, it converges to repeatedly probing b − 1, so it can pass
any ﬁxed z threshold. But is 0.5 still the best bisect
point even when p < 1? I wondered if this could be
expressed as the amount of probability
mass moved
∑
from one side to the other, i.e., i abs(pdf(i )−pdf0 (i )).
In the case where the bug repros this is:
p cdf( j ) × (1 − cdf( j ))

and in the case where the test passes:
(1 − p cdf( j )) × cdf( j ) × (1 − Ri ≤ j )

which, surprisingly, simpliﬁes to exactly the same
thing as the bug repros case. The maximum occurs
where ∂ /∂ cdf( j ) is 0, which turns out to be at cdf( j ) =
0.5 after all, and independent of p .7 But it’s not clear
that the amount of mass moved necessarily corresponds to reaching z the fastest. I show my work in
ﬁgure 2, because that’s what they taught me to do in
high school algebra class.
7
I also worked out the minimum moved mass between the pass
and fail case, which is almost always the pass case. It comes
p

cdf( j )(1−cdf( j ))

1−

1−p

out to 1/p −cdf( j ) , which experiences its maximum at
p
(thanks wolframalpha). But this is worst-case thinking, which
doesn’t seem appropriate. Let’s have some optimism!

ﬁgure 2. derivation(?) of optimal(??) bisect point
5.2 bisect entropy
A PDF’s information
content is measured in en∑
tropy: H =
i pdf(i )ln(pdf(i )). Alone, this value is
fairly meaningless, but to compare them, a spikier
PDF has lower entropy than a ﬂatter one. Indeed,
when the search terminates in Determinism World,
H = 0. I thought for a while about how to link “minimum expected entropy” to the stated goal of z =
maxi (pdf(i )) > 0.99999, but couldn’t really formalize
the idea. The goal of ﬁve 9’s is fairly arbitrary anyway,
and also not necessarily stable under the entropy
measurement, since it doesn’t care how the remaining 0.00001 is distributed.8
This strategy is expensive to compute. Whereas
bisecting at some ﬁxed threshold of probability mass
merely requires a O (logn) query of the CDF, computing the expected entropy is O (n ) for each bisect
point. A Very Smart Math Friend of mine (Gorey
2009) analyzed the easy case of the initial test, when
the PDF is still uniform (i.e., ∀i , cdf(i ) = i ), and found
the closed form:

H0, j = p ln( j ) + (1 − p j )(ln(1 − p j )) − j (1 − p )(ln( j − p ))
whose derivative, in his words, “looks like a giant
mess that does not admit an analytic solution for j .”
There is a ray of hope, however: thanks to the
nondecreasing-PDF invariant I mentioned in section 3, the expected entropy has a unique local minimum.9 Thus we can binary search for the global
8
9

284

See fn test_entropy_stability().
See fn test_expected_entropy_has_unique_local_minimum().

minimum, making this at worst O (n logn) instead
of O (n 2 ).
5.3 bisect randomly
Maybe a random world calls for a random algorithm!
It’s obvious this will terminate, but of course it won’t
be fast. Intuitively, if we use the PDF to weight the
random choice, it will be faster than choosing uniformly in [0, b ). But how much faster?
You can tell at this point I’m starting to get statistics fatigue. Not unprecedented in these hallowed
proceedings (Wise 2017).
5.4 human
Humans are known to be less patient than computers (citation: section 5.3). If a human is unwilling to compute R ◦ pdf by hand every step, or, more
prohibitively, simply doesn’t know p in advance so
can’t compute R at all, what should they do? Let’s
say a “human strategy” is any that doesn’t use the
PDF/CDF when deciding where to bisect.10
The most straightforward thing for a human to do
is to assume p = 1 until they encounter evidence
to the contrary. They’ll conduct a normal binary
search, and when they think they’ve found the bug at
c b , they’ll just keep probing c b −1 until enough probability mass (invisible to them) moves to b and the
simulator says they can stop. If this instead repros
the bug at c b −1 after all, the human gets confused!
They’ll assume the bug might have been present as
early as c0 , forgets their lower bound, and begins binary searching anew in the range [0, b ). It’s easy to
see this makes progress. Let’s call this the “confused,
forgetful human” strategy.
Another confused human variant is for them to
remember their previous lower bounds. After all,
just because their best lower bound b − 1 was contradicted doesn’t say anything about some earlier
bound a < b − 1 they might have observed. So this
human will maintain a stack of lower bounds, and
when b − 1 is contradicted, they’ll pop off the stack
and resume searching in [a , b ) instead. Let’s call this
the “confused human who remembers”. Incidentally, while implementing this strategy, I accidentally
wrote the comment,
In fact rand-uniform from last section counts as a human strategy. Did you bring your dice?
10

// the human, who now has perfect memory,
// walks backwards in time
and giggled a lot to myself as I imagined sigbovik fanﬁction.
Finally, let’s imagine the human knows in advance
that maybe something is up with this p stuff. Wishing to carry on classically as though p were 1, they’ll
try to emulate p being higher by retrying any passing
test “just to be sure”, before moving on. If they retry
the test r times, the probability it fails given the bug
is present is then 1−(1−p )1+r . But it comes at the cost
of r more steps for each new lower bound! As before,
when this human thinks they’re done, they’ll repeatedly probe c b −1 until contradicted (and we’ll make
them forgetful, to keep things simple). Let’s call this
the “suspicious human of r retries”.

6. simulated it
At this point I threw in the towel on the maths front,
and wrote a bunch of code to let the law of large
numbers do my work for me.
To save you some page-turning, here’s the source
code link again: https://github.com/bblum/sigbo
vik/blob/master/bisect. A BisectStrategy implements a function over p , the PDF, and/or its own
internal state to choose the next bisect point. The
SimulationState is initialized with ﬁxed n and p ,
and repeatedly invokes a given BisectStrategy,
hitting the PDF with Bayes’s rule accordingly, until
z > 0.99999. I provide implementations for all section 5’s strategies in src/strategies/. It’s written in
Rust so it’s self-documenting.
I learned a lot about ﬂoating point. I don’t mean
like NaNs and mantissa bits, I mean that when your
project is fundamentally a long multiplication chain
of increasingly high rationals, your precious cdf(n )
inevitably drifts farther from 1 than you can bound
with any arbitrary ε, and you start drowning in imprecision (Shiranu 2071). I suppose I could have
used the BigRational library, but I chose Rust over
Haskell for this so I could avoid runaway memory consumption... so, I resigned myself to explicitly renormalizing my PDFs by 1/cdf(n ) each time I
updated them. After that, I was able to write some
assertions to bound the imprecision drift within
k (std::f64::EPSILON) (VII 2014). But yeah, I deﬁnitely spent some sanity points writing a function
named fn assert_kinda_equals_one(&self).

285

7.

experiments

Let’s get right to the good stuff. ﬁgure 3 shows the
overall results, plotting the average performance of
each of the strategies from section 5.

tropy was far, far slower per step than all other strategies. It was even slower than rand-uniform!
7.1 computer strategies
Minimizing expected entropy turned out to be the
best strategy, globally across all p . Bisecting probability mass turns out to be preeetty close, although
its performance varies depending on its bisect-point
parameter j . Obviously, when p = 1, j should be 0.5
to reproduce classical binary search, but at around
p = 0.8, j = 0.333 overtakes it. Ultimately at p = 0.1,
the former is about 4% slower compared to the latter and to entropy, contradicting the “independent
of p ” hypothesis from section 5.1. I investigated a little more to see how p affected the optimal j , testing
various j s in increments of 0.01:
p
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

(a) full view

(b) detail view of high p range; linear scale
ﬁgure 3. perf results. how many bisect steps each
strategy takes to reach ﬁve 9s of conﬁdence
Each data point is simulated with n = 1024 and
averaged over 65536 trials. I chose a power of two
for n to minimize ﬂoating point drift (handwave),
and a multiple of n for trials so that buggy_commit
= trial % n would be uniform.
Of course, each trial takes exponentially longer
and longer as p → 0, so I didn’t bother testing past
0.1. I also didn’t bother measuring execution time, as
prior work has shown that one’s personal laptop is a
fraught environment for stable performance evaluation (Blum 2018), but it was still very obvious that en-

best j
0.50
0.45
0.41
0.39
0.40
0.36
0.35
0.34
0.36
0.35

cdfbisect( j )
10
17.4
21.9
27.5
34.6
44.1
58.3
81.6
127.9
266.6

65536 trials here wasn’t really enough to get variance
under control, but it deﬁnitely seems to converge
towards 0.333 or maybe 0.35 or something as p → 0.
Open question why for warp zone, I guess.
rand-uniform is obviously terrible but I threw it in
there for kicks. rand-cdf is not so terrible, doing even
better than all the human strategies.
7.2 human strategies
I investigated how the suspicious human’s numberof-retries parameter r affects their performance. In
this notation, r = 1 means they retry once whenever
the test passes, meaning two test runs on every passing commit. r = 0 is equivalent to the forgetful strategy, and r = 9 is what Athena does (section 4).
ﬁgure 3 was getting a little crowded so I’ve plotted
all the human-friendly strategies separately on ﬁgure 4. It’s pretty self-explanatory imo.
One thing that surprised me was that the human
who forgets all their past known lower bounds when
any of them is contradicted performed better than

286

ﬁgure 4. version of ﬁgure 3 for human strategies
the human who tried to rewind to their last not-yetcontradicted lower bound. I guess there’s a moral
here about not clinging to the past, moving on with
life, or something.
7.3 why ﬁve 9s?
Let’s talk about the arbitrary conﬁdence threshold.
There’s a lot of knobs to tweak here and only so much
time til April 1 (not to mention reader patience), but
I did at least want to show what happens if you terminate the search when you’re only 50% conﬁdent.
Compared to ﬁgure 3, the strategies all terminate
in about half as many steps, with the same relative
ranking and break-even points. What’s more interesting is the number of times they were wrong.
I had the simulator refuse to count a wrong result, restarting the search until it correctly identiﬁed the buggy commit 65536 times, and counting
the number of restarts. With z > 0.99999 this was
not very interesting: the strategies collectively made
0 errors 81% of the time, and were wrong more than
twice only 0.6%. But z > 0.5 is a different story. As
ﬁgure 5 shows, most strategies – even the comically
slow rand-uniform – converged to being wrong about
half the time, i.e., 65536 wrongs per 65536 rights. For
higher p , athena was most resilient.
confused-human-who-remembers, alone, did not
converge. They became more and more unable to
cope with the repro rate approaching 0, utlimately
being wrong 8 times as often as they were right.
Oh, human... please learn to let go and love yourself for who you are. There’s a better world out there,
a sparkling radiant future waiting just for you!

ﬁgure 5. how often each strategy was wrong, when
terminating with only 50% conﬁdence. the dashed
line marks the approximate convergence point at
65536, equal to the number of right answers

8. future work
I’m not ashamed to admit I left a lot of loose ends in
this paper. The biggest open questions for me are:
1. What relates p to cdfbisect’s optimal argument?
2. Why is entropy so tantalizingly close to cdfbisect?
Is there some way to show they’re the same?
I have also prepared a set of backup questions in case
those aren’t enough for you:
4. If you don’t know p in advance, is there an efﬁcient way to multiplex the search with ﬁguring it
out on the ﬂy?
5. What if false positives are also possible, with some
ﬂake rate q 6= p ?

6. What if the cost of git checkout is nonzero (e.g.,
having to recompile your tests on new code)?
Clearly human-mistrustful becomes better. What
about some wacky hybrid cdfbisect-mistrustful?

7. conclusion
If you are a computer, you can do pretty well by bisecting the probability mass somewhere between
1/3 and 1/2. If you are a human, you should forget
everything you know as soon as you see new evidence that contradicts your priors.
And remember to be patient and kind. Your test
suite is just trying its best!

287

acknowledgments
Thanks to Jason Reed, Lynn Chordbug, and Sujay
Jayakar for thinking about this problem with me.

referents
B. Blum. Transactional memory concurrency veriﬁcation
with Landslide. sigbovik 12, 2018.
K. E. Gorey. A categorical primer. sigbovik 3, 2009.
A. Pelc. Searching with known error probability. Theoretical Computer Science 63, 1989.
R. Rivest, A. Meyer, D. Kleitman, K. Winklmann, and
J. Spencer. Coping with errors in binary search procedures. Journal of Computer and System Sciences 20,
1980.
U. Shah. Athena: Our automated build health management system. Dropbox.Tech blog, 2019.
N. Shiranu. IEEE 755: Drowning point numbers. sigbovik
65, 2071.
V. V. Vargomax. Generalized super mario bros. is NPcomplete. sigbovik 1, 2007.
T. VII. What, if anything, is epsilon? sigbovik 8, 2014.
J. A. Wise. Batch normalization for improved DNN performance, my ass. sigbovik 11, 2017.

288

Blockchain Gang
31

RegisToken
Irene Lin, Arnaud Avondet, Preston Vander Vos
Keywords: blockchain, Ethereum, token, FCE, course registration

289

31

Irene Lin
Trevor Leong

RegisToken

Arnaud Avondet
Ishgun Arora

Preston Vander Vos
Edward Li

Carnegie Mellon University
{iwl,aavondet,pvanderv,tmleong,isarora,edwardli}@andrew.cmu.edu
Abstract
This paper identifies two problems at Carnegie Mellon
University: low Faculty Course Evaluation (FCE) survey response rates and the student course registration bottleneck.
We propose a solution to both via a state-of-the-art token,
RegisToken, on the Ethereum blockchain. Students receive
tokens by filling out FCEs and spend tokens to register for
additional classes.
Keywords— Blockchain, Ethereum, Token, FCE, Course
Registration

1 Introduction
Faculty Course Evaluation survey response rates vary
greatly, but not many are great. This is alarming because
advisors and students use FCEs to gauge the workload and
intensity of a course. However, we see that incentivization
to fill out FCEs is a consistently reliable strategy. Our goal
is to leverage this strategy to solve another problem in the
course registration system. Students are assigned time slots
throughout the day for when to register. This causes students
with earlier time slots to sign up for many courses, intending to drop them later. As a result, students with later time
slots are left scrambling for less convenient recitation times
or placing themselves on multiple waitlists.
Our solution to the course registration bottleneck is to deploy a smart contract that tokenizes additional registration
units. Students are still allowed to freely register for a baseline number of units, but if they want to sign up for additional units, they must pay using tokens. Tokens are recieved
through filling out FCEs. This incentive is equivalent to the
rewards that some courses use for encouraging students to fill
out FCEs. Then, students spend tokens on registration units.
The number of tokens required for additional units depends
on the time of the day. The registration bottleneck is alleviated because students must spend more tokens per unit earlier
in the day compared to later in the day. This smart contract
is deployed on the Ethereum blockchain.

2 Background
This section discusses the problem space regarding low
response rates for faculty course evaluations and the course
registration bottleneck. This section also gives an introduction to Ethereum, the distributed technology that our solution
is built upon.

2.1

Faculty Course Evaluations Low Response Rate

Course Evaluations (FCEs) are simply surveys conducted
at the end of a semester on the performance of an instructor
and the class in general[5]. FCEs are used to improve the
quality of teaching and learning at Carnegie Mellon through
feedback to both individual faculty members and promotion committees. Responses to the FCE provide information on students’ perceptions of their engagement, learning
outcomes, the instructor’s behavior, and course activities.
This feedback helps guide changes in future iterations of the
course and/or the instructor’s teaching (The Hub, CMU).
The validity of this data revolves around the number of
students who complete the FCEs. In a perfect scenario, all
students would fill out the survey and the academic department would get an accurate response of students’ opinions.
However, in practice only a small portion of the class size
fills out the surveys, with mild variations in each class. For
instance, course 99-101 (Computing @ Carnegie Mellon), a
staple course of the Carnegie Mellon curriculum, received
only a 35% response rate in Fall 2019. This is simply unacceptable. In similar light, course 57-403 (Yoga for Musicians) received a 16% response rate in the same semester. To
take in account a response rate from the higher end of the
spectrum is course 79-353 (Imprisoning Kids: Legal, Historical, and Moral Perspectives) taught by professor Katherine Lynch received 60% response rate in Fall, which isn’t
great either. On the other hand, 18-240 (Structure and Design of Digital Systems) consistently has a 95 to 100% across
semesters because the last graded homework assignment in
each semester is filling out the FCE. And 85-241 (Social
Psychology) taught by Professor Manke has a 97 to 100%
response rate because he awards a bonus point to the entire
class if the total FCE response rate is above some threshold.
These poor response rates result in low accuracy of the
data collected. On average, if only half the class fills out the
survey, their opinions can not be considered to be a full representation of the entire class and hence, cannot be used to
implement any changes in the course. Secondly, when there
is no incentive to fill out the surveys, the students who actually undertake the responsibly to complete these are biased
in their response - they’re either extremely satisfied with the
class and will rate it positively or are completing the survey to
express resentment/request for alterations to the course. This
does not provide a true picture of the students’ opinions and
may result in inaccurate ratings of professors and courses,

290

which can’t be generalized for the entire class.

2.2

Course Registration Bottleneck

Course Registration is one of the most stressful times of
the school year for students at Carnegie Mellon, and has been
a point of contention for generations of students, and even
more now with the release of the superior life-planning tool,
Stellic. The process begins when a student is faced with the
choices of classes that he/she will take in the next semester.
While many of the courses required for one’s major have seat
reservations, it does not fix the problem of obtaining classes
that one is interested in, but is not majoring in. In an ideal
world, this would not be a major problem, as each student
would only register for classes he/she would actually take.
Due to the competitive nature of course registration however,
students are incentivized to sign up for excess classes and
drop them during the semester when they are disappointed/
unable to achieve their desired grade. As a result, students
with earlier registration times are consistently overloading,
leaving those with later times stuck in the purgatory of the
wait list.
The problem is not easily remedied, and there does not
appear to be any easily implementable solutions. On first
glance, it would be easiest to simply randomize classes completely, giving each student an equal number of credits,
which would fix the problem of advantageous registration
times. This solution is near perfect, if only there was the
edge case of the fact that some students would theoretically
never graduate. Another possible, yet extremely flawed solution would be to have a class lottery system, where students
are given equal footing in every class, and selection is based
on random chance. This solution is also not viable, as students would not be given enough time to plan for back up
classes.
2.2.1

Ethereum Introduction

Ethereum is the second largest cryptocurrency by market
cap. It is different than Bitcoin; Ethereum aims at being the
world’s computer. Bitcoin, made by the modern-day genius
Craig Wright, is pretty much only used for transactional purposes. People simply send BTC back and forth. Ethereum
allows people to write smart contracts. Smart contracts are
written in a Turing complete language: Solidity. Anyone
who is willing to pay for their code to be in the network,
can write a smart contract. The Ethereum Virtual Machine
(EVM) will run the code when requested. Every node in the
network will run the smart contract on the EVM when it is
requested. This way, all nodes maintain the same state of the
network throughout the various execution calls.[6][7][8]
Smart contracts are used today for everything. DApps (decentralized applications) are becoming extremely prevalent.
DeFi (decentralized finance) is at the forefront of DApp developments. Ethereum is giving people and companies the
ability to build DApps which are replicating the ”standard”

Figure 1: Developers write smart contracts that run on the Ethereum
Blockchain.

Figure 2: Picture of a real life blockchain.

financial products. This is simply one use case of smart contracts.
ERC20 tokens are also a fascinating feature of Ethereum.
This is a token standard which allows people to make their
own tokens in the Ethereum system. These tokens can be
created and interacted with via smart contracts. ERC20 tokens are very customizable, allowing people to create them
for the use case of their choosing[4]. We will be utilizing the
ERC20 token in our solution.

3

Proposed Solution

Before we present our solution to the detrimental problem
that education here at Carnegie Mellon University faces, we
will try to reduce it to a simpler problem we observe in our
everyday lives: appreciation for free food. Picture yourself as
an undergraduate freshmen at humble Carnegie Mellon University during your first semester. A stream of companies
come to campus to convince you that their company offers
the best software engineering internship in the world, and to
convince you, they offer you pizza. It is common knowledge
that every high school student appreciates pizza to its fullest
extent, but what about after getting showered with free pizza
for weeks? Figure 3 shows that the affect is quite evident: receiving pizza for free decreases students’ appreciation for it.
Given this observation, we can state the following theorem.
Theorem 1 The Recruitment Week Pizza Problem is isomorphic to the Registration Problem.
This fact is actually quite simple to see, and the proof is
left as an exercise for the reader.
Now that we explained the motivation behind our solution, we propose a system that discourages students from
registering for many units by requiring students to fill out

291

Suppose the student has now successfully registered for 48
units (12 units above to soft unit cap) and desires to register
for another 3 unit mini. SIO will calculate the token price
for those 3 units based on the time of day and the number of
units over the soft unit cap the student has already registered
for.

3.1

Figure 3: Pie chart depicting appreciation for Pizza of first year students
after recruitment season (2020)

FCEs in order to register for additional units on top of a normal course load. The student is issued tokens proportional
to the number of units for the FCEs completed. The student
transfers tokens back to the system in order to register for
additional units. The token price of registering for additional
units is proportional to the number of total units the students
desires to register for. This means that the number of tokens
required to add 15 more units is much greater than the number of tokens required to add 3 more units. The token price is
also inversely proportional to the time of day, meaning that
registering for more units at 8am requires more tokens than
at 4pm.
When the student fills out an FCE, they pass in a public address to wallet on the Ethereum blockchain. The Administrator wallet will transfer tokens to the student using
the issueTokens function.1 This function takes in the student
wallet address to send funds to and the number of units for
the FCE the student filled out. The number of units is converted to number of tokens awarded to the student according
to some scalar constant value passed into the constructor. If
the transfer fails, then the Administrator wallet issues an IOU
to the student through the approve() function. Then, at some
later time, the student can redeem their tokens.
When students register for classes, they have some hard
unit cap they are not allowed to exceed. We propose the idea
of a soft unit cap that students are discouraged from exceeding, but are allowed to exceed through filling out FCEs. For
demonstration purposes, let the soft unit cap be 36 units because this is the minimum number of units a student must
register for in order to be considered a full-time student. But
in reality, this soft unit cap can be set by administration to
whatever value. Suppose a student registers for 36 units and
desires to register for an additional 12 unit course. SIO will
show how many tokens are needed to register for 12 more
units based on the time of day. The student must transfer
that amount of tokens back to the Administrator wallet by
calling the transferToOwner() function and provide proof by
submitting transaction hashes to SIO. SIO will check that the
transactions took place within the current registration period.
1 All

Hyperparameters

This section covers system hyperparameters that administration is allowed to set.
FCE Unit to Token Conversion Rate. When a student
fills out an FCE, they recieve some number of tokens in exchange. In the RegisToken smart contract, the number of
units is multiplied by a scalar in line 45 of the issueTokens
function. This scalar is a state variable that is set in line 30 of
the constructor. It is recommended to set this value to a large
number in order to allow for greater granularity for calculating the token price for registering for additional units. But
if the number is too large, then the contract will run out of
tokens.
Total Token Supply. The Administrator wallet is allowed
to set the total token supply when the smart contract is deployed. We recommend setting the total supply to a very
large number. Using engineer’s induction, we can easily
prove that Carnegie Mellon will be around for at least the
next 1000 years, so keeping that in mind would be helpful
in setting an appropriate total supply. If the token supply
runs out, the Administrator wallet can deploy another token
contract and configure SIO to accept transaction hashes that
point to valid token transfer transactions of from both contracts.

3.2

FCE System Changes

In order for this system to work, there needs to be an administrative wallet that deploys the RegisToken smart contract and becomes the owner of the smart contract. This address must be used to call the issueTokens function when a
student completes an FCE. This can be done by changing the
FCE to take in a student wallet address as a parameter and using Infura for easy and scalable API access to the Ethereum
network.

3.3

SIO System Changes

SIO must compute the token price for units as a function
of number of units over soft cap and time of day (discussed
in next section). The student must register a wallet address
in SIO under Student Information. When a student registers
for classes on SIO, SIO must accept transaction hashes and
verify that transaction hashes point to valid transactions and
that the following are true:
• Transactions were mined within this registration period.
Students are not allowed to double count transactions
from previous semesters.

code is provided at the end of this paper.

292

• Transactions called the transferToOwner function and
the token values in each transaction sum to greater than
or equal to the token price for the desired umber of units
the student is registering for.
• Transaction origins match the registered student wallet
address registered in SIO. Students are not allowed to
steal transactions with calls to transferToOwner from
other wallets and must use a single wallet.
The system maintains pseudonimity and does not leak student information because no course information is published
when the contract issues tokens for completed FCEs. Because this information is kept off chain, course schedules
cannot be linked to student wallet addresses.

3.4

Token Price per Unit

Figure 4: Token price for registering for additional units over the soft unit
cap.

Through months of game theoretic and economic research, we have come up with the optimal formula to compute the tokens per unit conversion rate at any given time.
Theorem 2 Let η = days left in the registration week, λ =
hours left in the day, and ξ = total number of units above
the soft cap.
The number of tokens/unit, τ , can be expressed as the
following.
τ=

(ηλ)2 (ξ 2 λ3 )
(λ2 )2 ηξ

The total number of units above soft cap is the number of
units for the class the student is trying to register for plus
the total number of units the student has already successfully registered for. The number of hours left in the day starts
counting at 8AM and stops after 5PM. The number of days
left in registration week starts on the Monday of registration
day and stops after all freshmen have registered on Thursday. Outside of valid registration time slots, the token price
for adding units is zero. Only students who have already registered are allowed to add more classes without transferring
tokens. If a student with junior standing registers on 8am
on Tuesday, the earliest time they can add classes for free is
after the rest of the students in the junior class have registered. Otherwise if the student desires to register for more
units than the soft unit cap allows at 8am, then they must
transfer tokens. Placing oneself on a waitlist also requires
tokens if it is during valid registration time slots.

3.5

Token Balances for First Year Students

There is no default action taken to give tokens to first year
students before they start the academic year. However, the
design allows the Administrator wallet to issue tokens to first
year students if desirable. However, the registration system
will not break if first year students start with zero token balance. If a first year student desires to register for units above
the soft unit cap, the token price is zero after first year registration day.

Figure 5: An example of a CryptoKitty.

4

Related Work

Tokens are extremely useful. Take Dogecoin for example
[3]. Shiba Inus everywhere love Dogecoin [2]. It has become
such a fun and friendly community. None of this would have
happened if it was not for the token in the middle of the community. The token has brought out many smiles.
The most prominent example of an ERC20 token is CryptoKitties [1]. Cat lovers everywhere love CryptoKitties.
Users are able to breed, play with, and collect kitties. There
are rare, very collectible, kitties which people seek after.
There is a lot of interaction with the token as people trade
them. The token has spurred a community of cat lovers to
action and joy. Tokens truly solve many problems and bring
much happiness.

5

Conclusion

Future work includes research on the economy of token
trading among students. When students graduate from CMU,
they are unable to spend their tokens on registration units,
so it is in their best interest to sell to underclassmen. Will
this cause the token value to crash? Another area of research
is extending this into an anonymous reputation system that
rewards student wallet addresses for consistently filling out
FCEs virtuously. The verification and incentivization models
are complex and worthy of investigation.

293

In this paper, we have investigated the problem space regarding low Faculty Course Evaluation survey response rates
and the course registration bottleneck. We designed a smart
contract of tokenized registration units in order to solve both
problems. The smart contract preserves pseudonymity of the
student and maintains minimal state. We discuss the changes
that need to be made off chain and how that interfaces with
the smart contract. The token system alleviates the registration bottleneck by requiring more tokens per registration unit
if it is earlier in the day and registration week, and requiring
little to zero tokens if it is later in the day and registration
week. This system is parameterizable can be made backward
compatible.

References
[1] Cryptokitties. https://www.cryptokitties.co/ Accessed: 2020-2-24.
[2] Doge meme. https://knowyourmeme.com/memes/
doge Accessed: 2020-2-24.
[3] Dogecoin. https://dogecoin.com/ Accessed: 2020-224.
[4] Erc20.
https://en.bitcoinwiki.org/wiki/
ERC20 Accessed: 2020-2-21.
[5] Faculty course evaluations (fces). Accessed: 2020-2-20.
[6] What is ethereum?
https://ethereum.org/
what-is-ethereum/ Accessed: 2020-2-21.
[7] V. Buterin. A next-generation smart contract and decentralized application platform. https://github.com/
ethereum/wiki/wiki/White-Paper Accessed: 20202-21.
[8] V. Buterin. @vitalikbuterin. https://twitter.com/
VitalikButerin Accessed: 2020-2-24.

294

6
1

Appendix: Code

pragma solidity >=0.6.0;

2
3

contract RegisToken {

4
5
6
7
8

// track address of owner
address payable public owner;
// constant for token multiplier value
uint TOKEN_SCALAR;

9
10
11
12
13

// ERC20 state variables
string public constant name = "RegisToken";
string public constant symbol = "RGT";
uint8 public constant decimals = 18;

14
15
16
17
18

// Keep track of data needed for ERC20 functions
mapping (address => uint256) public balances;
mapping(address => mapping (address => uint256)) public allowed;
uint256 public _totalSupply;

19
20
21
22
23
24

// Throws if called by any account other than the owner.
modifier ownerOnly() {
require(msg.sender == owner, "Caller is not the owner");
_;
}

25
26
27
28
29
30
31
32
33
34

// @param _token_scalar The multiplier from FCE units to tokens
// @param _total_supply The total supply of tokens
constructor(uint _token_scalar, uint _total_supply) public {
owner = msg.sender;
TOKEN_SCALAR = _token_scalar;
// owner starts with all the tokens
balances[owner] = _total_supply;
_totalSupply = _total_supply;
}

35
36
37
38

//---------------------------------------------------------------------------// Owner Only Functions
//----------------------------------------------------------------------------

39
40
41
42
43
44
45
46
47
48
49
50

// @brief Issue tokens to student by transfering funds to student.
// If transfer fails, give an IOU to the student
// @param _student Student address
// @param num_units Number of units for the fce filled out
function issueTokens(address _student, uint num_units) public ownerOnly {
uint num_tokens = num_units * TOKEN_SCALAR;
if (!transfer(_student, num_tokens)) {
uint total_owed = num_tokens + allowed[owner][_student];
approve(_student, total_owed);
}
}

51
52
53
54

//---------------------------------------------------------------------------// ERC20 Functions
//----------------------------------------------------------------------------

55

295

// Total number of tokens in circulation
function totalSupply() public view returns (uint) {
return _totalSupply - balances[address(0)];
}

56
57
58
59
60

// Get account balance
function balanceOf(address _owner) public view returns (uint256) {
return balances[_owner];
}

61
62
63
64
65

// Sender is my account, receiver is _to account.
function transfer(address _to, uint256 _amount) public returns (bool) {
if (balances[msg.sender] < _amount
|| _amount == 0) {
return false;
}

66
67
68
69
70
71
72

balances[msg.sender] -= _amount;
balances[_to] += _amount;
return true;

73
74
75

}

76
77

// Sender is my account, receiver is owner account.
function transferToOwner(uint256 _amount) public returns (bool) {
return transfer(owner, _amount);
}

78
79
80
81
82

// Pre: I must be authorized to transer funds from _from account.
function transferFrom(address _from, address _to, uint256 _amount)
public returns (bool) {
if (allowed[_from][msg.sender] < _amount
|| balances[_from] < _amount || _amount == 0) {
return false;
}
balances[_from] -= _amount;
allowed[_from][msg.sender] -= _amount;
balances[_to] += _amount;
return true;
}

83
84
85
86
87
88
89
90
91
92
93
94
95

// Authorize _spender to transfer funds on my behalf
function approve(address _spender, uint256 _amount) public returns (bool) {
allowed[msg.sender][_spender] = _amount;
return true;
}

96
97
98
99
100
101

}

296

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 10: RegisToken
Reviewer: I got my friends to fill out a madlib
Rating: 47/69
Confidence: 16 micrometers

This paper discusses Oxycontin, with applications to ennui. The paper begins with a moribund
demonstration of mental filtering, followed by examples of solipsism, and concludes with a smelly
repurposing of reptiles.
Overall, the presentation is glorious, with the small exception of page 291.34, where the discussion
of beer cans falls below expectations. The author(s) could perhaps merrily enslave this section
with a brief enveloping of the π/7th paragraph, but this would involve a reconsideration of the core
thesis.
The most surprising section, however, was on page 2, paragraph 666. The author(s) present a
highly novel lazor, perhaps the first of it’s kind. Though it seems like the graph (Fig. 512) is
misprinted — it appears to be missing the R-coordinate? Unclear.
Promising results, to be sure. The reviewers are regretful of the impact this will have on future
research.

297

298

The

SIGBOVIK 2020 Field Guide
32

to

Conspiracy Theories

Faking ancient computer science: A special SIGBOVIK tutorial
Roxanne van der Pol, Brian van der Bijl, Diederik M. Roijers
Keywords: ancient computer science, aliens, conspiracy theories,
Triangle of Conspiracy Succession, probability, Arcaicam
Esperantom, random number generator, pseudo-science

33

MORSE OF COURSE: Paper reveals time dimension wasted
Dougal Pugson
Keywords: Morse code, performance, Bash, protocols

34

The Pacman effect and the Disc-Sphere Duality
Francisco Ferreira, Fulvio Gesmundo, Lorenzo Gheri, Pietro Gheri, Riccardo Pietracaprina, Fangyi Zhou
Keywords: Pacman, peace, Earth, flat, round, disc, sphere

299

32

Faking Ancient Computer Science: a Special
SIGBOVIK Tutorial
Roxanne van der Pol, Brian van der Bijl, Diederik M. Roijers
Computationele Conspiracy Theory Vereniging (CCTV)
Utrecht, the Netherlands
February 2020
Abstract
In this tutorial, we provide answers to a key question that nobody
wanted an answer to: how to fake ancient computer science? We nonetheless argue that this is important, as we clearly assert that this is the
most robust vein of computer-science related conspiracy theories we could
mine. We outline a step-by-step procedure for creating an instance of the
class of ancient computer science conspiracy theories. We illustrate this
with a proof-of-concept, on the basis of Arcaicam Esperantom, which is
a pseudo-ancient predecessor of Esperanto. This tutorial thus provides
you with the essential tools for creating your very own computer-science
based conspiracy theory. Because ancient computer science is unlikely to
be made into reality – as other previous computer-science based conspiracy theories have been – it is our hope that this paper will become the
basis for many new conspiracy theories that are future proof.

1

Introduction

Conspiracy theories [2] form an essential part of everyday twenty-first century
life. While the fields of mathematics, history, physics, psychology, and so on
and so forth, have all contributed many interesting quaint conspiracy theories,
computer science (CS) is trailing behind. The main reason for this is that CSbased conspiracy theories such as “the government is constantly spying on you
through your IT” and “they can make fake videos of you saying all kinds of
humbug”, have sadly all been made into reality.
To mitigate this situation, we thus need a robust source of conspiracies to
which reality will not be able to simply catch up.1 After a considerable amount
of time, we have found that we believe that the most successful conspiracy theories have either (or both) of the following elements: 1) they deal with something
1 And,

as is typically the case, trump it with something considerably more bizarre.

300

ancient, and/or 2) they involve aliens. In fact, we have convinced ourselves that
80% of successful conspiracy theories have either or both these elements.2,3
We thus arrive at the main contributions of this paper. Specifically, we
provide a method to fake ancient computer science (possibly alien-inspired).
We apply this method as a proof-of-concept, by writing a well-known algorithm
– a random number generator – in an “ancient” language. For this language,
we have selected Arcaicam Esperantom [4]. This is in itself a constructed (i.e.,
fake) pseudo-ancient dialect of a constructed language: Esperanto [10]. We
immediately must confess that we do not do this accurately. However, we argue
that this is not a problem. We can arbitrarily omit words we do not know, which
we will then claim were lost in time, and because a good conspiracy theory may
not be too realistic. This is because a more unrealistic conspiracy theory filters
out people who are too skeptical. Such people might be convinced by rational
arguments later, which may cause the conspiracy theory to disappear. We argue
that it is better to attact only a core of highly gullible believers4 straight off the
bat(shit crazy).
One might object that using a
constructed language that was constructed as a pseudo-ancestral language for another constructed language is a bit much. We take the
opposite viewpoint: there is no such
thing as too much for a conspiracy
theory. Furthermore, Arcaicam Esperantom is uniquely positioned to be
the backbone of our proof-of-concept
conspiracy theory. This is because
the book on Arcaicam Esperantom Figure 1: This appears to be a picture of
[4] was written in Esperanto, which a statue of an ancient laptop. This imis already quite a time-investment for age is not relevant to the contents of this
the aspiring conspiracy theorist to be paper. But see, ancient computer! This
able to learn to read. Therefore, needs to be at least some form of cirthe conspiracy theory will gain mo- cumstantial evidence! Also, it was availmentum due to the sheer effort of able under a creative commons attribugetting to know it. Furthermore, its tion share alike 2.0 generic licence on
“descendant” – Esperanto – was first wikimedia commons, courtesy of the aupublished in Russian in 1887 [10]. thors: Dave and Margie Hill / Kleerup,
Not only does this make Esperanto al- under the title “File:Gravestone of a
most ancient itself; but there is noth- Woman - Getty Villa Collection.jpg”.
ing that attracts conspiracy theorists Many thanks!
2 For

this, we have amongst others, consulted a reliable, yet anonymous source.
you press us on this, we might reveal that this is our local contact with the Illuminati.
However, we cannot reveal his/her exact identity, as (s)he has recently had some issues with a
man who blames the Illuminati for the sexual identity of his pet frogs, which he simply cannot
accept.
4 Not to be confused with “beliebers” which are a completely different type of people.
3 If

301

quite as much as involving the commies too.5
Another objection might be that Esperanto, and Arcaicam Esperantom, are
constructed languages. This is an easy objection to rebut, by simply asking:
“Are they really, though? Take one look at Dr Zamenhof, this 19th century
guy is supposed to have just invented a whole new language by himself ?! No
way, it is way more likely he was just handed a dictionary by aliens!” This
provides us with nice side-ways access to the other great conspiracy booster we
have identified: aliens.
In Section 2 we survey the existing literature on conspiracy theory construction, in a highly condensed manner. In Section 3, we present our step-by-step
instructions for faking ancient computer science, along with a proof of concept.
In related work (Section 4), we make some unrelated points that are truly very
much besides the point.6

2

Background

The background always contains a lot
of hidden information. According to
the Triangle of Conspiracy Succession
Probability (TCSP) [7], when creating a conspiracy theory it is important to offer the general public enough
comprehensible information to draw
them in, yet remain vague enough to
make people want to dig for more (see
Figure 2). People won’t just go chas- Figure 2: This excerpt from the oriing after every other supposed truth, ginal pseudo-text [7] clearly illustrates
though. This information must be the TCSP.
presented as fact and it must make
sense at first and second glance, or at least at first glance. It must instil a sense
of anticipation and wonder; a duty to figure out the “great truth” which is hidden in plain sight. The “obvious” information supporting the theory will serve
as a lead to lesser known “truths”, ready to further ensnare the tenacious gullible public. The “truths” become less in abundance and harder to find, until, at
last, everything however improbable has been eliminated, and what remains is
nothing short of a truth, but not the truth. Because after all, there is no spoon
[9].
Finally, we would like to point out that this background section contains a
lot of additional information. To access this information, you only need to read
5 Yes yes, the Soviet Union did not exist until 1922, but no matter; they do not know that.
And even if they do, Russian is still a highly mysterious looking language, and provides ample
opportunity for the convenient mistranslation or misinterpretation if the book is actually
accessed.
6 But it nicely fills up the bibliography, which makes our paper look more credible. This is
important.

302

between the lines.

3

Proof of concept

As a proof of concept we take a very simple algorithm – a pseudo-random number generator. We can motivate this by making the observation that humans are
very bad at generating random numbers [5], and that it is important in all kinds
of other computation. Making these same two observation in a pseudo-ancient
text pseudo-establishes the pseudo-existence of the ancient research field.7
To construct a pseudo-ancient description of an algorithm, we follow these
steps:
1. Select a (relatively simple) algorithm,
2. create a pseudo-ancient description of it in modern English,
3. add pseudo-references to make the description seem more reliable,
4. (optional) translate this description to an intermediate language (Esperanto),
5. translate the resulting description into the target (pseudo-)ancient language (Arcaicam Esperantom),
6. obfuscate details and remove words,
7. create physical artefacts to photograph and subsequently discard.
Step 1 For our proof-of-concept, we use the so-called linear congruential generator (LCG) [6]. An LCG is described by the following formula:
xn = (axn−1 + c) mod m,

(1)

that is, a random number, xn is computed by taking the previously generated
random number xn−1 , multiplying it by an integer 0 < a < m, adding a constant
integer 0 < c < m and then taking the modulus m, i.e., computing the remainder
after division by m. To obtain the first random number x1 , a so-called seed, x0 ,
is used as the previous random number.
Step 2 The above description of an LCG is concise, but not very pseudoancient yet. Therefore, we create a more pseudo-ancient form in modern English:
We choose a start number.
First, we take the start number,
times another number, plus a third number.
We then divide by the maximal number that we want.
What is left over is the result.
This result is the next start number.
7 And

may also be used to increase the pseudo-likelihood of ancient alien computer scient-

ists.

303

Step 3 We now need to add some fluff. First, we add a reference to that
humans are not good at taking random numbers. We specifically use the term
“humans” to hint at the possibility of ancient aliens. We further add a reference
to the Codex Seraphinianus. This is a nice document, at present undeciphered,
which has a nice page with a machine that looks like it could be a Turing
machine. Referencing other material, nice and old, is a good way to spike more
pseudo-credibility.
Humans are not good at rolling dice in their mind.
We have seen this before.
Therefore, we need a calculation recipe,
to throw dice in an artificial way.
We choose a start number.
First we take the start number,
times another number, plus a third number.
We then divide by a the maximal number that we want.
What is left over is the result.
This result is the next start number.
The device on page [leave out] of the book by
brother Seraphinius can be used to do this.
Step 4 For our purposes, it is highly useful to translate the text to an intermediate language that is more like the language we want the final description
to be in. For this proof-of-concept this is Esperanto.
Homoj ne bone povas ruli^
gi ^
etkubojn en siaj mensoj.
Ni jam vidis ^
ci tion antaŭe.
Tial ni bezonas kalkulrecepton,
por artefarite ^
eti ^
etkubojn.
Ni elektas komencan nombron.
Unue, ni multigas la komencan nombron
per alia nombro, plus tria nombro.
Ni tiam dividas laŭ la maksimuma dezirata nombro.
Kio restas, estas la rezulto.
^
Ci tiu rezulto i^
gas la nova komenca nombro.
La aparato sur pa^
goj [...] de la libro de
frato Seraphinio povus fari ^
gin.
Step 5 and 6 We now translate the text to our pseudo-ancient language,
Arcaicam Esperantom. Furthermore, we leave out pesky little details (like the
page numbers we previously already omitted8 ), that could too easily expose
our conspiracy. For example, the Codex Seraphinianus is way too specific, and
8 Let

them search. We believe the machine on page 158 of the Codex Seraphinianus looks
nicely like a potential Turing machine. But who are we to impede the creativity of the aspiring
conspiracy theorist?

304

might be debunkable. Therefore, we say “Seraphi[...]” so that the reference
hints at the Codex Seraphinianus but might very well be something else in the
future if need be.
Homoy ned bonœ powait rulizzir argiitoyn in sihiayd mensoyd.
Yam widiims isityon antezœ.
// Thefariei Velianas sal
Ityal bezonaims calculretzepton,
// cluvenias turuce
[...] artepharitœ zhetir argiitoyn.
Comentzan nombron electaims.
Unne, comentzan nombron multigaims
[...] alian nombron, plus tridan nombron.
Ityam diwidaims selez macsimuman deziratan nombron.
Quion restas, estas rezulton.
Ityu rezultom izzat nowam comentzam nombrom.
Apparatom sobrez paghoyn [...] libres
phrates seraphi[..] powut pharir eghin.
In this iteration, zhetcuboyn has been replaced by the older form *argiitoyn
(via Old French Ergot — dewclaw9 ).
The text appearing in the C-style comment represents a margin-text, which
was likely added when a young, preoccupied Neapolitan monk transcribed the
corpus wherein this text allegedly first appeared.
We now have a lovely bit of ancient text ready to be “discovered” somewhere.
Step 7 To fully realise our proof-of-concept, we have contacted a retired forger
(who wishes to remain anonymous) to print our code on a genuine 9th-century
clay tablet. The result is shown in Figure 3. The only step that remains to be
done is to make sure that the tablet is photographed and then summarily lost.10
The fact that the physcial artefacts will inevitably and mysteriously vanish is
of course essential; we do not, under any circumstances want any scrutiny on
them. We further note that dating back the discovery to sometime during the
cold war is probably a good idea. This is because the photographs can then be
of a worse quality, which can further obfuscate possible pesky (visual) details
that might debunk the theory too easily.

4

Related Work

Conspiracy theories are of course an instance of bullshit [3]. However, conspiracy
theories are much more elaborate, and also require a form of self-deception. As
pointed out in the excellent (actual serious research) article by Von Hippel
and Trivers [8], this “. . . eliminates the costly cognitive load that is typically
associated with deceiving, and it can minimize retribution if the deception is
9 Alledgedly, after the late-Nikophorian dogwood-shortage and the war of Elohim visitation,
the Esperantii gradually phased out wood-based dice for goat-based alternatives.
10 And is presumably at Area 51 or some other cool place where the governments of this
world hide all the pseudo-evidence for most existing conspiracy theories.

305

Figure 3: Original Arcaicam Esperantom code tablet, discovered in 1973 by Russian explorer Ïåòð Ñòðàííûåâ whilst fleeing from local wildlife in the Lombard
swamplands.
discovered.” We would argue that conspiracy theorists have taken this to the
extreme and pulled their cognitive resources, in order to accept no responsibility
for any consequences whatsoever. Therefore society has no other retribution tool
than ridicule, from which a group of fervent conspiracy theorists can effectively
shield its members. This is marvellous, however, both the philosophical and
sociological aspects of conspiracy theories are beyond the scope of this paper,
as this paper is not in fact anywhere near serious.

5

Conclusion

We believe this resolves all remaining questions on this topic. No further research is needed.11
Instead, we just want to add a few soothing notes. Firstly, do not worry too
much about debunking. Of course, throughout the sections, we have done our
utmost to show methods that can help prevent premature debunking. However,
conspiracy theories typically do get debunked sooner or later. They are after all,
nothing but elaborate BS, so there is bound to be some things that expose this.
Don’t panic [1]. Conspiracy theories do not suffer that much from debunking as
one might think. Instead, the most fervent conspiracy theorists start believing
in a conspiracy theory more when there is a significant effort to debunk it. This
is because they may well think people are hiding the truth from them, rather
than just debunking some CT. So, the more intricate your web of deceptive little
tricks is, the more effort it will take to properly debunk the CT, which will feed
the CT like a hungry little monster.
Secondly, there is nothing that stops you from creating multiple instances of
the type of conspiracy theories described in this paper. It is an abstract class,
of which we hope to see many objects. Please apply our paper to create the
coolest and most creative conspiracy theories. And cite us. Please... do cite
us.12
11 https://xkcd.com/2268/,
12 Reviewer

at your service!
2 says: “cite this paper!”

306

Acknowledgements
We thank Nick Goris, Robert Bezem and Wouter van Ooijen for their constructive feedback and lovely squiggly red markings.

References
[1] Douglas Adams. Hitchhiker’s guide to the galaxy. Pan Books, 1979.
[2] Steve Clarke. Conspiracy theories and conspiracy theorizing. Philosophy
of the Social Sciences, 32(2):131–150, 2002.
[3] Harry G Frankfurt. On bullshit. Princeton University Press Princeton, NJ,
2005.
[4] Manuel Halvelik.
ARKAIKA ESPERANTO: La verda pralingvo.
www.universala-esperanto.net, 2010.
[5] Theodore P Hill. Random-number guessing and the first digit phenomenon.
Psychological Reports, 62(3):967–971, 1988.
[6] Donald Knuth. Seminumerical algorithms. In The Art of Computer Programming, pages 10–26. Addison-Weslety Professional, 1997.
[7] Roxanne van der Pol. The Triangle of Conspiracy Succession Probability.
Illuminati, because a triangle has four sides, 1605.
[8] William Von Hippel and Robert Trivers. The evolution and psychology of
self-deception. Behavioral and Brain Sciences, 34(1):1, 2011.
[9] The Wachowskis. The matrix, 1999. Warner Bros.
[10] L. L. Zamenhof.

Ìåæäóíàðîäíûé ÿçûê. Chaim Kelter, 1887.

307

MORSE
OF COURSE: Paper Reveals Time Dimension
33
Wasted
Dougal Pugson

Pugson’s C++ Crypt LLC
New York, USA
dougalpugson@gmail.com

Abstract

All modern communication protocols have been discovered
to send nonsensical and invalid morse code sequences in
addition to their intended data. This paper demonstrates that
making use of these ’wasted bits’ can effectively infinitely
increase the transmission rate of binary data. An implementation of timing-based dual-stream morse encoding is
provided in a modern programming language.
CCS Concepts • Transport Protocols; • Encoding; •
Implementation → Bash;
Keywords SIGBOVIK, morse code, performance, bash, protocols
ACM Reference Format:
Dougal Pugson. 2020. MORSE OF COURSE: Paper Reveals Time
Dimension Wasted. In Proceedings of SIGBOVIK (SIGBOVIK 2020).
ACM, New York, NY, USA, 2 pages. https://doi.org/10.1145/
nnnnnnn.nnnnnnn

1 Introduction

Communication between electronic entities can be concieved
of as a system of tubes 1 or pipes 2 . Into these pipes, electronic
satchels3 are inserted by the electronic system. The contents
of these “packets” are composed according to a specified
algorithmic protocol4 , in a way such that, when recieved by
the recipient 5 can be reassembled into the desired message6 .
1 c.f.

Stevens et alia.

2 As superbly illustrated in the inimitable interactive video title "Super Mario

Bros. 2".
3 So-called “packets”.
4 From the Byzantine πρωτόκολλον, meaning "First Page", referring to the
average amount of the design specification Engineers are expected to read
before beginning implementation.
5 recipiō, recipere, recēpı̄, receptum.
6 As expressed by the immortal MARSHALL McCLUHAN in the groundbreaking epistle "The Media is the Message",
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights for
third-party components of this work must be honored. For all other uses,
contact the owner/author(s).
SIGBOVIK 2020, April 2020, Pittsburgh, PA USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM.
https://doi.org/10.1145/nnnnnnn.nnnnnnn

Many protocols for sending data via the computer have
been developed, such as TCP, FTP, HTTP, et cetera. Upon inspection of these protocols, it was discovered that, in addition
to their intentional messages, they were also transmitting
nonsensical and invalid morse code sequences.
Let “.” represent a short transmission, and “-” represent a
long transmission. An example transmission measured from
a typical TCP communication is as follows:
.......................................................
.

This invalid morse code sequence is unparseable7 .
This paper will demonstrate that this wasted information
can be meaningfully replaced with useful data, effectively
infinitely increasing the effective transmission rate.
1.1

Mathematical Prolegomena

Let 𝔇 represent the number of bits per transmission in the
ordinary dimension (1 vs 0).
Let 𝔐 represent the number of usable bits per transmission
in the temporal dimension.
Let 𝐴𝛼 represent the average packet transmission length.
Let 𝑐 represent the fastest average length of time between
each transmission packet sent via the transmission tube
(that is, the limit provided by the combination of hardware
construction and the speed of light in an average vacuum)8 .
Then, the rate of data transfer 𝑟 of any protocol may be
expressed as follows:

𝑟=

𝔐
𝔇
+
𝐴𝛼 𝐴𝛼

For typical transmissions protocols, these following values
hold: 𝔐 = 0, 𝔇 = 1, and 𝐴𝛼 = 𝑐 With these values known,
the equation may then be evaluated.
In a culture like ours, long accustomed to splitting and
dividing all things as a means of control, it is sometimes a bit
of a shock to be reminded that, in operational and practical
fact, the medium is the message.
7 “sssssssssssssssssss”

is one possible interpretation

8 Though, as demonstrated by Dyson et al, brand and model can significantly

affect results.

308

SIGBOVIK 2020, April 2020, Pittsburgh, PA USA

Dougal Pugson

1.3
𝑟=

𝔐
𝔇
+
𝐴𝛼 𝐴𝛼
0 1
𝑟= +
𝑐 𝑐
1
𝑟=
𝑐

With the transmission rate of standard protocols established, we now examine the transmission rate of a temporal
encoding. In order for temporal encoding, i.e., morse code,
to be successfully transmitted, pauses will be required to be
inserted into the transmission stream. This means that, for
such encodings,

Conclusion

This simple technique and its concomitant arbitrarily large
improvement in the performance of all data transmission
protocols is certain to forever revolutionize economic activity
on earth, with profound implications in all aspects of modern
life.
That such an profound optimization has been hidden,
unbeknownst to man, for so many years instills a profound
humility in the author, and inspires great hope that many
such great leaps in human intellectual accomplishment still
remain to be made.

𝔐 > 1 |= 𝐴𝛼 > 𝑐

Let us assume that each transmission unit contains one bit
of information, and that, additionally, that transmission unit
may be either short (a delay of 0) or long (a delay of some
abritrary value 𝜆). This would make the information value
𝔐, as defined above, 2. Given this, we may again evaluate
the transmission rate equation with the following values in
order to calculate the transmission rate of our new temporal
encoding: 𝔐 = 2, 𝔇 = 1, and 𝐴𝛼 = 𝑐 + 𝜆
𝔐
𝔇
+
𝐴𝛼 𝐴𝛼
1
1
𝑟=
+
𝑐 +𝜆 𝑐 +𝜆
2
𝑟=
𝑐 +𝜆
𝑟=

With a small value 𝜆, we may drastically increase throughput, up to an effective doubling.
As time is continuous, any given time interval may be
divided into infinitely many fine gradations, e.g.,
𝑙𝑜𝑛𝑔, 𝑠ℎ𝑜𝑟𝑡, 𝑣𝑒𝑟𝑦𝑠ℎ𝑜𝑟𝑡, 𝑣𝑒𝑟𝑦𝑣𝑒𝑟𝑦𝑠ℎ𝑜𝑟𝑡 · · · 𝑣𝑒𝑟𝑦∞𝑠ℎ𝑜𝑟𝑡 .
Thus, 𝔐 may be arbitrarily large. The industrial applications of this surprising fact should not be lost upon the
reader.
1.2 Implementation
The author has provided an model implementation of timingbased encoding in the modern programming bash. The full
source code of this program may be found at
https://github.com/dpugson/morse-of-course.
Using utitilies such as tcpclient or bash’s built in TCP
support, this technique could easily be used across a network.

309

34

The Pacman Effect and the Disc-Sphere Duality
Francisco Ferreira
Fulvio Gesmundo
Riccardo Pietracaprina

Lorenzo Gheri
Fangyi Zhou

Pietro Gheri

Abstract
In this paper, we study the Pacman effect, where the Pacman disappears from the leftmost part
of the screen when reaching it, and re-appears from the rightmost part of the screen. We apply the
Pacman effect to unify the theory of flat and spherical Earth, proposing the Disc-Sphere Duality. We
conclude that a spherical Earth is basically the same as a flat Earth under the Pacman effect, bring
peace to believers of both theories.

1

Introduction

The aim of this paper is to bring peace! For years very smart people have been fighting over the
shape of our beautiful, albeit shitty, planet. Two equally reasonable positions have mainly emerged:
the Earth is a disc VS the Earth is an oblate spheroid (that we approximate with a sphere, from now
on). This two very different bidimensional manifolds have both equal rights to claim themselves the
proper geometrical model for Earth. Thus they started fighting. With what army? Well, Flat Earth
Societies are fighting in the blue corner alongside the multiple-millennia champion Disc. In the red
corner we see the young arrogant opponent, the Sphere, supported by its crew: Science and everyone
else.
They have been fighting round after round for centuries, with equal credibility, and the match seemed
impossible to settle. There was this dark moment in history where the evil newcomer was unfairly
holding our beloved champion against the ropes. All seemed lost. How can planes fly in circumferences around the globe if the globe is not a globe? That was a tough one, also a shitty move in the
authors’ opinion. Then Pacman, the fairest referee of all, came to save the day.
The main contribution of this work is none. However, we would like to remind everyone that the
Pacman effect is a thing. Airplanes can fly to the extreme boundary of Earth, but they are bound to
reappear on the diametrically opposite side of the disc, just like Pacman in the famous Namco video
game. Now when forced to apply the rigour of mathematics (reluctantly: reasoning is for nerds!
Nerds suck!), we see that the Pacman effect can be modelled as the identification of the boundary of
the disc to one single point.
Counterintuitively this Pacman effect sheds new light on the Truth. The disc with such identified
border is nothing but a surface diffeomorph to the sphere. As Einstein once said:
“It seems as though we must use sometimes the one theory and sometimes the other,
while at times we may use either. We are faced with a new kind of difficulty. We have two

310

Figure 1: A Screenshot of Pacman. When reaching the leftmost part of the screen, Pacman wraps to the rightmost
part of the screen.

contradictory pictures of reality; separately neither of them fully explains the phenomena
of light Earth, but together they do.”
We have discovered (maybe, “invented”?) what we call the “disc-sphere duality”.

2

Flat and Spherical Earth are Very Different Views of the World

Try and play Ultimate with a football, or football with a flying disc. You silly. Readers are encouraged
to use their imagination, or alternatively refer to Fig. 2.

3

Pacman Says: “Are they though?”

Are they though?

We show that the closed disc with the boundary identified to a point (with the quotient topology induced by the
topology of the plane) and the sphere (with its natural
Euclidean topology) are homeomorphic topological manifolds. In fact, the homeomorphism can be used to transfer
the differential structure of the sphere to the disc with the
boundary identified to a point.

311

(a) A Football

(b) Ultimate Frisbee, with a Disc

Figure 2: Flat and spherical objects are different.

Our reference is [2], but the background provided by any textbook in basic topology will suffice.
We set the following notation:
• D = {(x, y) ∈ R2 : x2 + y 2 < 1} is the (open) disc;
• D = {(x, y) ∈ R2 : x2 + y 2 ≤ 1} is the closed disc, with the topology induced by R2 ;
• S2 = {(x, y, z) ∈ R3 : x2 + y 2 + z 2 = 1} is the sphere;
Let ∼ be the equivalence relation on D which identifies the boundary to a point; write [∂] for the
equivalence class of the boundary. We have D/ ∼= D ∪ [∂].
The fact that D/ ∼ is homeomorphic to a sphere is an immediate consequence of the fact that they can
both be identified with the 1-point identification of the disc. We give an explicit homeomorphism,
using the stereographic projection of the sphere.
Recall the stereographic projection
Σ : S2 \ {(0, 0, 1)} → R2


y
x
,
.
(x, y, z) 7→
1−z 1−z
Then Σ homeomorphically maps S2 \ {(0, 0, 1)} onto R2 . This homeomorphism extends to the one
points compactifications setting Σ((0, 0, 1)) = ∞, where ∞ is the point at infinity of R2 .
The plane R2 is homeomorphic to the unit disc via a two dimensional tangent map and the homeomorphism extends to the one point compactifications:
tan :R2 ∪ {∞} → D

(x,y)

 (x, y) 7→ x2 +y2 ·
(0, 0) 7→ (0, 0)

 ∞
7→ [∂].

2
π

arctan(x2 + y 2 )

You wanted it? You’ve got it.

312

if (x, y) ∈ R2 , (x, y) 6= (0, 0)

4

No. They aren’t.

No. They aren’t. You silly.

5

Conclusion and Future Work

Our work clearly shows that, thanks to the Pacman effect, the two main mathematical models from
literature, applied to describe the planet Earth, are in fact the same. The first critique that comes to
mind to the presented results can be synthetically expressed as follows.
In order to be able to model the Earth as flat, you need to introduce ad hoc technicalities,
like the identification of the border to a point (a.k.a. Pacman effect), that de facto allow
you to use a spherical model, while calling it “flat”.
First of all the guy above likes Latin a lot and we don’t trust people that try to trick us with different
languages. Secondly we finds that who writes such critics is a quite biased and narrow-minded
person, not willing to accept point of views and opinions different from their owns.
The authors of this paper instead will not discriminate any model on basis of shape, race, colour, sex,
language, religion, political or other opinion, national or social origin, property, birth or other status
such as disability, age, marital and family status, sexual orientation and gender identity, health status,
place of residence, economic and social situation.

As for related and future work, finding out whether Australia factually exists and, if so, where it is
located, goes beyond the aim of this paper. Indeed, we have discovered a truly marvellous proof of
this, which this margin is too narrow to contain.

References
[1] Wikipedia
[2] Munkres, James R, Topology: A First Course, Prentice Hall International, 1974.

313

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 37: The Pacman effect and the DiscSphere Duality
Reviewer: <mcoblenz@andrew.cmu.edu>
Rating: <sigbovik@gmail.com>
Confidence: Re: SIGBOVIK 2020 Call for Papers and Reviews 2: Total Recall

I’m afraid I must object to triple-blind review on the grounds that it, too, may be biased because
the reviewers still know they are reviewers. Have you considered quadruple-blind review, in which
reviews are elicited from reviewers without informing them that they are writing reviews?

314

Serious Business
35

The SIGBOVIK 2020 field guide to plants
Jim McCann (editor)
Keywords: field guide, plants, to

36

Deep industrial espionage
Samuel Albanie, James Thewlis, Sebastien Ehrhardt, João F. Henriques
Keywords: late, very late, so extremely late

37

What is the best game console?: A free-market–based approach
Dr. Tom “I Only Write Chess Papers Now” Murphy VII Ph.D.
Keywords: invisible hand, handheld console, minimum of 3 keywords

315

35

The SIGBOVIK 2020 Field Guide to Plants
Edited by Jim McCann∗
Welcome, and thank you for purchasing or borrowing or stealing the SIGBOVIK 2020 Field Guide To
Plants. Identifying plants can be a daunting task, but this guide will help you make your way to a positive
identification with just a few simple questions. Consider your responses carefully. After all, we’re doing
science here.
Let’s begin.
Question 1: Are you in a field?
if yes – proceed to Question 2
if no – service not available at this location. Consider buying the “SIGBOVIK 2020 House Guide To
Plants.”
if not sure – you should check! Consider buying the “SIGBOVIK 2020 Field Guide To Fields.”

Question 2: Is the object you are identifying alive?
if yes – proceed to Question 3
if no – it is not a plant; perhaps it is a rock? Consider buying the “SIGBOVIK 2020 Field Guide To
Rocks.”

Question 3: Perform an exact count of the number of cells in the object. Do not shirk! Only an exact count
will be sufficient to accurately determine the answer to this question. An error of even one cell could change
entirely the classification result you attain.
Now, is your count:
many – proceed to Question 4.
one – it is not a plant; perhaps it is a single-celled organism? Consider buying the “SIGBOVIK 2020
Field Guide To Single-Celled Organisms.”
none – it is not a plant; are you sure it’s not a rock? Return to Question 2.

Question 4: Excise a cell from the object and perform a chemical assay on the cell walls. Does the assay
report cellulose is present?
if yes – proceed to Question 5
if no – it is not a plant; perhaps it is an animal? Consider buying the “SIGBOVIK 2020 Field Guide To
Animals.”

∗ ix@tchow.com

316

Question 5: Do you still have that cell you removed? You’re going to need it for Question 6.
if yes – proceed to Question 6
if yes but you left it in your car – we talked about this, Gary. This is just not professional. Second strike,
Gary. Second strike.
if no – well, get another cell! (If you can’t find another cell, return to Question 3.) Then proceed to
Question 6.

Question 6: Find a chloroplast in your cell.
if you found one – proceed to Question 7
if you don’t know what a chloroplast is – consider buying the “SIGBOVIK 2020 Field Guide To
Chloroplasts.”
if there are none in this cell – well, check the rest of the cells. Seriously, Gary, I’m wondering why we
even hired you. Come on, get to it. This is going on your performance review.
if there are no chloroplasts in the object – it is not a plant; consider buying the “SIGBOVIK 2020 Field
Guide To Fungi.”

Question 7: Observe the chloroplast over the course of several days. Is it carrying out photosynthesis?
if yes – proceed to Question 8
if no – this is not a plant; consider buying the “SIGBOVIK 2020 Field Guide To Objects With Nonfunctional Chloroplasts”

Question 8: Now, did you borrow, steal, or legitimately purchase this guide?
if borrowed – Congratulations, the object you have tested would generally be considered a plant! Consider buying the “SIGBOVIK 2020 Field Guide To Plants.”
if stolen – Seriously, Gary, theft of company property? This is the last straw. Consider buying the
“SIGBOVIK 2020 Field Guide To Plants,” or consider not coming in next Monday. No I’m not talking
about a vacation. I’m talking about you’re fired, Gary.
if purchased – Congratulations, the object you have tested would generally be considered a plant! Consider buying a second copy of the “SIGBOVIK 2020 Field Guide To Plants.”

317

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 31: The SIGBOVIK 2020 field guide to
plants
Reviewer: Prof. Jim McCann
Rating: 4
Confidence: Yes

There was an Old Man with a beard,
Who said, “It is just as I feared!—
Two Owls and a Hen, four Larks and a Wren,
Have all built their nests in my beard.”

318

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper Review of Paper 31: The SIGBOVIK
2020 field guide to plants
Reviewer: <ix@tchow.com>
Rating: <sigbovik@gmail.com>
Confidence: Re: Draft 2020 proceedings!

I think the review currently for the guide to plants should probably be attributed to the originator
of the couplet (Edward Lear) rather than to ”Prof. Jim McCann” – I’m not sure there’s much joke
added (since it ended up on a paper referencing me) and I’m not really comfortable with something
I didn’t write being attributed to me.
--Jim

319

Nearly published as a conference paper at SIGBOVIK 2019

36

D EEP I NDUSTRIAL E SPIONAGE
Samuel Albanie, James Thewlis, Sebastien Ehrhardt & João F. Henriques
Dept. of Deep Desperation,
UK (EU at the time of submission)

A BSTRACT
The theory of deep learning is now considered largely solved, and is well understood by researchers and influencers alike. To maintain our relevance, we
therefore seek to apply our skills to under-explored, lucrative applications of this
technology. To this end, we propose and Deep Industrial Espionage, an efficient
end-to-end framework for industrial information propagation and productisation.
Specifically, given a single image of a product or service, we aim to reverseengineer, rebrand and distribute a copycat of the product at a profitable pricepoint to consumers in an emerging market—all within in a single forward pass
of a Neural Network. Differently from prior work in machine perception which
has been restricted to classifying, detecting and reasoning about object instances,
our method offers tangible business value in a wide range of corporate settings.
Our approach draws heavily on a promising recent arxiv paper until its original
authors’ names can no longer be read (we use felt tip pen). We then rephrase the
anonymised paper, add the word “novel” to the title, and submit it a prestigious,
closed-access espionage journal who assure us that someday, we will be entitled
to some fraction of their extortionate readership fees.

1

I NTRODUCTION

In the early 18th Century, French Jesuit priest Franois Xavier d’Entrecolles radically reshaped the
geographical distribution of manufacturing knowledge. Exploiting his diplomatic charm and privileged status, he gained access to the intricate processes used for porcelain manufacture in the Chinese city of Jingdezhen, sending these findings back to Europe (over the course of several decades)
in response to its insatiable demand for porcelain dishes (Giaimo, 2014). This anecdote is typical
of corporate information theft: it is an arduous process that requires social engineering and expert
knowledge, limiting its applicability to a privileged minority of well-educated scoundrels.
Towards reducing this exclusivity, the objective of this paper is to democratize industrial espionage
by proposing a practical, fully-automated approach to the theft of ideas, products and services. Our
method builds on a rich history of analysis by synthesis research that seeks to determine the physical
process responsible for generating an image. However, in contrast to prior work that sought only
to determine the parameters of such a process, we propose to instantiate them with a just-in-time,
minimally tax-compliant manufacturing process. Our work points the way to a career rebirth for
those like-minded members of the research community seeking to maintain their raison d’être in the
wake of recent fully convolutional progress.
Concretely, we make the following four contributions: (1) We propose and develop Deep Industrial
Espionage (henceforth referred to by its cognomen, Espionage) an end-to-end framework which
enables industrial information propagation and hence advances the Convolutional Industrial Complex; (2) We introduce an efficient implementation of this framework through a novel application
of differentiable manufacturing and sunshine computing; (3) We attain qualitatively state-of-theart product designs from several standard corporations; (4) We sidestep ethical concerns by failing
to contextualise the ramifications of automatic espionage for job losses in the criminal corporate
underworld.

1

320

Nearly published as a conference paper at SIGBOVIK 2019

RESEARCH COLUMN
IDEAS

INPUT IMAGE

PRODUCTION COLUMN

V

STN

LSTM

TX

PROD. REVIEWS
STN

MIDDLE MGMT

STN

LSTM

TX

STN
CAM

MARKETING COLUMN
TX
CAM

Lvis

INFLUENCERS

TX

STN

LSTM

TX

LSTM

TX

STN

LSTM

STN

LSTM

TX

L$
SALES

DISTRIBUTION COLUMN
LSTM

TX

STN

DRONES

LSTM

D

COPYCAT PRODUCT

Figure 1: A random projection of the proposed multi-dimensional Espionage architecture. We
follow best-practice and organise business units as tranposed horizontally integrated functional
columns. The trunk of each column comprises stacks of powerful acronyms, which are applied
following a Greek visual feature extractor ΦV . Gradients with respect to the loss terms L$ and Lvis
flow liberally across the dimensions (see Sec. 3.1 for details). We adopt a snake-like architecture,
reducing the need for a rigid backbone and producing an altogether more sinister appearance.

2

R ELATED W ORK

Industrial Espionage has received a great deal of attention in the literature, stretching back to
the seminal work of Prometheus (date unknown) who set the research world alight with a wellexecuted workshop raid, a carefully prepared fennel stalk and a passion for open source manuals.
A comprehensive botanical subterfuge framework was later developed by Fortune (1847) and applied to the appropriation of Chinese camellia sinensis production techniques, an elaborate pilfering
orchestrated to sate the mathematically unquenchable British thirst for tea. More recent work has
explored the corporate theft of internet-based prediction API model parameters, thereby facilitating
a smorgasbord of machine learning shenanigans (Tramèr et al., 2016). In contrast to their method,
our Espionage reaches beyond web APIs and out into the bricks and mortar of the physical business
world. Astute readers may note that in a head-to-head showdown of the two approaches, their model
could nevertheless still steal our model’s parameters. Touché. Finally, we note that while we are not
the first to propose a convolutional approach to theft (BoredYannLeCun, 2018), we are likely neither
the last, adding further justification to our approach.
Analysis by Synthesis. Much of the existing work on analysis by synthesis in the field of computer vision draws inspiration from Pattern Theory, first described by Grenander (1976-81). The
jaw dropping work of Blanz et al. (1999) enabled a range of new facial expressions for Forrest
Gump. This conceptual approach was generalised to the OpenDR framework through the considerable technical prowess of Loper & Black (2014), who sought to achieve generic end-to-end (E2E)
differentiable rendering. Differently from OpenDR, our approach is not just E2E, but also B2B
(business-to-business) and B2C (business-to-consumer).

3

M ETHOD

The Espionage framework is built atop a new industrial paradigm, namely differentiable manufacturing, which is described in Sec. 3.1. While theoretically and phonaesthetically pleasing, this
approach requires considerable computational resources to achieve viability and would remain intractable with our current cohort of trusty laptops (acquired circa 2014). We therefore also introduce
an efficient implementation of our approach in Sec. 3.2 using a technique that was tangentially inspired by a recent episode of the Microsoft CMT submission gameshow while it was raining.

2

321

ARXIV

Nearly published as a conference paper at SIGBOVIK 2019

1
2
3
4
5
6
7

try:

# often fails on the first import - never understood why
from espionage import net
except Exception: # NOQA
pass # inspecting the exception will bring you no joy
if "x" in locals(): del x # DO NOT REMOVE THIS LINE
from espionage import net # if second fail, try re-deleting symlinks?
net.steal(inputs) # when slow, ask Seb to stop thrashing the NFS (again)

Figure 2: A concise implementation of our method can be achieved in only seven lines of code
3.1

D IFFERENTIABLE MANUFACTURING

Recent developments in deep learning have applied the ”differentiate everything” dogma to everything, from functions that are not strictly differentiable at every point (ReLU), to discrete random
sampling (Maddison et al., 2016; Jang et al., 2017) and the sensory differences between dreams and
reality. Inspired by the beautiful diagrams of Maclaurin et al. (2015), we intend to take this idea
to the extreme and perform end-to-end back-propagation through the full design and production
pipeline. This will require computing gradients through entire factories and supply chains. Gradients are passed through factory workers by assessing them locally, projecting this assessment by the
downstream gradient, and then applying the chain rule. The chain rule only requires run-of-the-mill
chains, purchased at any hardware store (fluffy pink chaincuffs may also do in a pinch), and greatly
improves the productivity of any assembly line. Note that our method is considerably smoother than
continuous manufacturing—a technique that has been known to the machine learning community
since the production of pig iron moved to long-running blast furnaces.
Two dimensions of the proposed Espionage framework is depicted in Fig. 1. At the heart of the
system is a pair of losses, one visual, Lvis , one financial L$ . For a given input image, the visual
loss encourages our adequately compensated supply line to produce products that bear more than
a striking resemblance to the input. This is coupled with a second loss that responds to consumer
demand for the newly generated market offering. Our system is deeply rooted in computer vision:
thus, while the use of Jacobians throughout the organisation ensures that the full manufacturing
process is highly sensitive to customer needs, the framework coordinates remain barycentric rather
than customer-centric. To maintain our scant advantage over competing espionage products, details
of the remaining n − 2 dimensions of the diagram are omitted.
3.2

S UN M ACROSYSTEMS
Ah! from the soul itself must issue forth
A light, a glory, a fair luminous cloud
Enveloping the Earth
Jeff Bezos

Differentiable manufacturing makes heavy use of gradients, which poses the immediate risk of steep
costs. The issue is exacerbated by the rise of costly cloud1 services, which have supported an entire
generation of vacuous investments, vapour-ware and hot gas. Despite giving birth to the industrial
revolution, smog and its abundance of cloud resources (see Fig. 4 in Appendix A, or any British
news channel), the United Kingdom, has somehow failed to achieve market leadership in this space.
Emboldened with a “move fast and break the Internet” attitude (Fouhey & Maturana, 2012), we
believe that it is time to reverse this trend. Multiple studies have revealed that sunshine improves
mood, disposition, and tolerance to over-sugared caipirinhas. It is also exceedingly environmentally friendly, if we ignore a few global warming hiccups.2 The question remains, how does this
bright insight support our grand computational framework for Espionage? To proceed, we must first
consider prior work in this domain.
1
Not to be confused with Claude by our French-speaking readers, according to Sebastien’s account of a
recent McD’oh moment.
2
Up to about 5 billion AD, when the Sun reaches its red giant phase and engulfs the Earth.

3

322

Nearly published as a conference paper at SIGBOVIK 2019

Figure 3: Top row: A collection of unconstrained, natural images of products. Bottom row: Photographs of the physical reconstructions generated by our method. Note that the proposed Espionage
system can readily produce full houses, speakers, water bottles and street signs—all from a single
image sample. When generating books, Espionage does not achieve an exact reconstruction, but
still seeks to preserve the philosophical bent. Failure case: the precise layout of keys in technology
products such as keyboards are sometimes altered.
An early example of sunshine computing is the humble sundial. This technology tells the time
with unrivalled accuracy and reliability, and automatically implements ”daylight saving hours” with
no human intervention. Sunshine-powered sundials are in fact part of a new proposal to replace
atomic clocks in GPS satellites (patent pending). With some obvious tweaks, these devices can form
the basis for an entire sunshine-based ID-IoT product line, with fast-as-light connectivity based on
responsibly-sourced, outdoors-bred photons. This is not to be confused with the electron-based
”fast-as-lightning” transmission of cloud computing, an expression coined by the cloud computing
lobbyists in a feeble attempt to suggest speed.
The cloud lobby has been raining on our parade for too long and it is time to make the transition.
We proceed with no concrete engineering calculations as to whether this is viable, but instead adopt
a sense of sunny optimism that everything will work out fine. Thus, with a blue sky above, sandals
on our feet and joy in hearts, we propose to adopt a fully solar approach to gradient computation.
3.3

I MPLEMENTATION THROUGH A U NICORN S TARTUP

The appearance of rainbows through the interaction of legacy cloud computing and novel sunshine
computing suggests that our framework can easily attain unicorn status. Because branding is everything, our first and only point of order was to choose the aforementioned rainbow as our logo and
basis for marketing material. This cherished symbol expresses the diversity of colours that can be
found in hard cash3 .
A quick back-of-the-envelope calculation showed that our startup’s VC dimension is about 39 Apples, shattering several points, hopes and dreams. This quantity was rigorously verified using the
advanced accounting analytics of a 40-years-old, 100MB Microsoft Excel spreadsheet that achieved
semi-sentience in the process.

4

E XPERIMENTS

Contemporary researchers often resort to the use of automatic differentiation in order to skip writing
the backward pass, in a shameful effort to avoid undue mathematical activity. We instead opt to
explicitly write the backward pass and employ symbolic integration to derive the forward pass.
Thanks to advances in computational algebra (Wolfram, 2013), this method almost never forgets the
+C. Our method can then be implemented in just seven lines of Python code (see Fig. 2).
To rigorously demonstrate the scientific contribution of our work, we conducted a large-scale experiment on a home-spun dataset of both branded and unbranded products. Example outcomes of this
experiment can be seen in Fig. 3.
3
For the most vibrant rainbow we conduct all transactions in a combination of Swiss Francs and Australian
Dollars

4

323

Nearly published as a conference paper at SIGBOVIK 2019

Efficacy was assessed quantitatively through a human preference study. Unfortunately, lacking both
US and non-US credit cards, we were unable to procure the large sample pool of Amazon Mechanical Turkmen and Turkwomen required to achieve statistically significant results. We therefore
turned to our immediate family members to perform the assessments. To maintain the validity of the
results, these experiments were performed doubly-blindfolded, following the rules of the popular
party game “pin the tail on the donkey”. The instructions to each blood relative stated simply that if
they loved us, they would rate the second product more highly than the first. While there was considerable variance in the results, the experiment was a conclusive one, ultimately demonstrating both
the potential of our approach and the warm affection of our loved-ones. Comparisons to competing
methods were conducted, but removed from the paper when they diminished the attractiveness of
our results.
Reproducibility: Much has been written of late about the nuanced ethics of sharing of pretrained
models and code by the sages of the field (see e.g. OpenAI (2019) and Lipton (2019) for complementary perspectives). As adequately demonstrated by the title of this work, we are ill-qualified to
contribute to this discussion, choosing instead to fall back to the tried and true research code release
with missing dependencies, incorrectly set hyper-parameters, and reliance on the precise ordering
of ls with Linux Kernel 2.6.32 and ZFS v0.7.0-rc4. This should allow us replace public concern
about our motives with pity for our technical incompetence.

5

C ONCLUSION

The theory of deep learning may be solved but the music need not stop. In this work, we have made
a brief but exciting foray into a new avenue of career opportunities for deep learning researchers
and enthusiasts. Nevertheless, we acknowledge that there may not be room enough for us all in the
espionage racket and so we also advocate responsible preparation for the bitter and frosty depths of
the upcoming AI employment winter. To this end, we have prepared a new line of reasonably priced
researcher survival kits—each will include a 25-year supply of canned rice cakes, a handful of pistachios, a “best hit” compilation of ML tweets in calendar form, and an original tensor-boardgame,
a strategic quest for the lowest loss through the trading of GPUs and postdocs. Collectively, these
items will keep spirits high and bring back fond memories of those halycon days when all that was
required to get a free mug was a copy of your résumé. The kits will be available to purchase shortly
from the dimly lit end of the corporate stands at several upcoming conferences.

R EFERENCES
Volker Blanz, Thomas Vetter, et al. A morphable model for the synthesis of 3d faces. In Siggraph,
volume 99, pp. 187–194, 1999.
BoredYannLeCun. #ConvolutionalCriminal https://twitter.com/boredyannlecun/
status/1055609048412930048, 2018.
Robert Fortune. Three Years’ Wanderings in the Northern Provinces of China: Including a Visit to
the Tea, Silk, and Cotton Countries; with an Account of the Agriculture and Horticulture of the
Chinese, New Plants, Etc. Number 34944. J. Murray, 1847.
David F Fouhey and Daniel Maturana. The kardashian kernel, 2012.
C. Giaimo. One of the earliest industrial spies was a french missionary stationed in china. Altas
Obscura, 2014.
U Grenander. Lectures in pattern theory i, ii and iii: Pattern analysis, pattern synthesis and regular
structures. Springer-Verlag, 1976-81.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. 2017.
URL https://arxiv.org/abs/1611.01144.
Zachary C. Lipton.
Openai trains language model,
mass hysteria ensues.
http://approximatelycorrect.com/2019/02/17/
openai-trains-language-model-mass-hysteria-ensues/, 2019.
5

324

Nearly published as a conference paper at SIGBOVIK 2019

Matthew M Loper and Michael J Black. Opendr: An approximate differentiable renderer. In European Conference on Computer Vision, pp. 154–169. Springer, 2014.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning, pp. 2113–
2122, 2015.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
OpenAI. Better language models and their implications.
better-language-models/, 2019.

https://openai.com/blog/

Prometheus. Fire: an insider’s guide. In Proceedings of the Mt. Olympus Conference on Technology,
date unknown.
Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine
learning models via prediction apis. In 25th {USENIX} Security Symposium ({USENIX} Security
16), pp. 601–618, 2016.
Stephen Wolfram. Computer algebra: a 32-year update. In Proceedings of the 38th International
Symposium on Symbolic and Algebraic Computation, pp. 7–8. ACM, 2013.

6

325

Nearly published as a conference paper at SIGBOVIK 2019

A

A PPENDIX

Figure 4: UK cloud-cover at the time of submission.

7

326

37

What is the best game console?
A free-market–based approach
Dr. Tom “I Only Write Chess Papers Now” Murphy VII Ph.D.∗
1 April 2020

1

Introduction

This is a tale of parallels: Two worlds interconnected by a beautiful symmetry: The two
worlds being: Video Games, and, symmetrically: the Stock Market.
Since Curry and Howard were first found to be isomorphic, mathematics has regularly
deployed connections between seemingly unrelated fields to solve problems. Here, again,
we weave such a tangled web. We make use of an elegant bijection between game consoles
and publicly-traded securities to use well-known theorems from one domain (the efficient
market hypothesis) to solve an open problem in another: What is the best game console?
This question has vexed us for some time, as has the stock market. Even in the earliest
days of video game consoles, it was very annoying when your friend had a ColecoVision
and you had an Atari 2600, even if the friend had the expansion that allowed it to play
Atari games. The friend would beat you in the game of Combat, but you could swear
it was because of the console’s inferior, imprecise controllers. At the end of the 1980s
the console wars really began to heat up, with zealous gamers forming factions around
popular brands like Nintendo and Sega. Each had their own mascots and lifestyle magazines. The number of bits were growing exponentially. Few families could afford multiple
video game systems, and those that did found their houses torn asunder by infighting.
Which console would be hooked to VHF Channel 3, and which to the slightly superior
Channel 4?
The antipathy continues to this day and does not seem to be resolvable by traditional
means (spec comparison tables, forum posts, console exclusives). Perhaps the problem
is too emotional to be solved by direct analysis. This is where the current approach
really shines: By transforming the problem into a different domain (one ruled by the
emotionless homo economicus [4]) we can address the problem with pure reason.
The stock market is completely rational, by definition [1]. Prices of securities reflect
the exact actual value of the underlying physical good (for example, a basket of option
contracts intended to synthetically reproduce the inverse of the day-to-day change in
forward 3-month USD LIBOR, as determined by Eurodollar futures [2]) at each moment
∗ Copyright

© 2020 the Regents of the Wikiplia Foundation. Appears in SIGBOVIK 2020 with the trailing stop
of the Association for Computational Heresy; IEEEEEE! press, Verlag-Verlag volume no. 0x40-2A. 100 basis
points

1

327

in time. This is why they change so often: The value of everything around you is rapidly
changing, thousands of times per second. We can use this to understand what console
is best (i.e., has the greatest value), by constructing an isomorphism.

2

Methodology

To create an isomorphism, we need to represent each object in one domain (game consoles)
with an object in the other (stock market). This is surprisingly easy to accomplish for
many consoles. Game consoles have a standard set of abbreviations that are used so
that people don’t have to write out e.g. “Nintendo Entertainment System” every time. As
it turns out, many of these abbreviations are also symbols of publicly-traded securities
in the stock market.
There are many dozens of video game consoles [5, 6], not all of which have standardized
abbreviations [3]. Therefore, the isomorphism here is technically a partial best-effort
isomorphism. We find 14 consoles that have a natural counterpart in the stock market.
These are given in Table 1, which is the next three pages.

3

Putting my money where my mouth is

Having identified 14 suitable console–
security pairs, the next step is to invest
money in the market. In March 2019,
in preparation for SIGBOVIK 2020, I purchased shares of each of these 14 securities. In order to create a balanced
portfolio, I acquired approximately $100
USD of each; since prices range from less
than 3¢ to $131, this of course meant
buying a different quantity of each. The
actual amounts are given in Figure 3.
Some of these symbols trade on foreign exchanges using other currencies, which is a
headache at tax time.

Figure 1: This video game store in Ambergris Caye, Belize, sells games for the fabled
PlayStation 2/3 .

There are many consoles that have no
corresponding symbol on any exchange
(i.e. they are privately held). This includes popular consoles like Nintendo 64 (N64), Wii
(WII), XBox 360 (X360), Neo Geo Pocket Color (NGPC), and so on. There are some whose
securities were too exotic for even the ambitious author to acquire. For example, Playstation Vita (PSV), trades on the Johannesberg Stock Exchange, which is not among the
140 exchanges supported by Interactive Brokers. DreamCast, abbreviated DC, is futures
on Class III Milk (milk solids used to make cheese and whey). While this trades readily
on the Chicago Mercantile Exchange, you have to be careful about buying Class III Milk
futures because you might end up with a bunch of Class III Milk instead of money or
video games. Many obscure or fabled consoles (Figure 1) were treated as out-of-scope.

328

Console
Nintendo Game Boy

Code
GB

When I was a game boy of about ten years old,
my parents forbade me from having a Game
Boy, even if purchased with a sock full of
dimes that I had personally found around the
house and thus made my own property, because “if Nintendo is an addiction, this is like
a flask.” These same parents also referred to
Nintendo games as “tapes.” This little pocket
monster from 1989 was the birthplace of the
Pokémon video game series.
Fun Fact: The second most popular IEEE
standard, IEEE-1394 (a.k.a. “FireWire”), has
its connector designed after the Game Boy’s
link cable.
Nintendo DS

NDS

Nintendo’s “Dual Screen” handheld, released
November 2004. The title Nintendogs was
conceived due to a typo of this console’s
name.

Sega Genesis

GEN

Genesis does what Nintendon’t! This 16-bit
console from 1989 competed directly against
the Super Nintendo. In South Korea, it was
known as the Super Gam*Boy. Who knows
what the * stands for?

Sega Game Gear

GG

This color hand-held platform came out in
1991, competing against the 4-shades-ofbeige Game Boy. With 3–5 hours of play time
on six AA batteries, what’s not to love?

Symbol
GB

Exchange
Trades as
TSXV
Ginger Beef Corp.
(Toronto)
Ginger Beef Corp., through its subsidiaries, engages
in the operation of franchised take out/delivery service
restaurants and the production of frozen and ready-toserve deli Chinese food products for distribution to retail
outlets. The company was founded on April 26, 2000
and is headquartered in Calgary, Canada.

NDSN1

NASDAQ
Nordson Corp.
(New York)
Nordson Corp. engages in the engineering, manufacture
and market of products and systems used for adhesives,
coatings, sealants, biomaterials and other materials. It
operates through three segments: Adhesive Dispensing,
Advanced Technology, and Industrial Coating Systems.
The company was founded in 1954 and is headquartered
in Westlake, OH.
GEN

NYSE
Genesis Healthcare Inc.
(New York)
Genesis Healthcare, Inc. is a holding company, which
engages in the provision of inpatient services through
skilled nursing and assisted and senior living communites. It also offers rehabilitation and respiratory therapy services. It operates through the following segments: Inpatient Services, Rehabilitation Therapy Services, and Other Services. The company was founded in
1985 and is headquartered in Kennett Square, PA.
GG

NYSE
Goldcorp Inc.
(New York)
Goldcorp is one of the world’s fastest growing senior gold
producers, with operations and development projects located in safe jurisdictions throughout the Americas. The
Company is committed to responsible mining practices
and is well positioned to deliver sustained, industryleading growth and performance. The company is headquartered in Vancouver, British Columbia.

1 The standard abbreviation is NDS, but an additional N can be added to emphasize that this is Nintendo’s Nintendo DS, not another company’s
Nintendo DS.

329

Console
PlayStation (original)

Code
PSX

PlayStation began as a CD-ROM expansion
for the SNES! But then Nintendo was like
j/k we are going to make one with Philips
instead! But then Nintendo was like j/k
also about that, and made the Nintendo 64.
Philips went on to release the abysmally bad
console called CD-i, making full spiteful use
of their contractual rights to Nintendo characters with abysmally bad titles like Hotel
Mario. Sony went on to make the PlayStation, mostly for revenge.
Sony Playstation 3

PS3

The X in “PSX” stands for the × symbol in
the PlayStation’s official occult incantation:
4
× . The 3 in “PS3” stands for the
other three symbols. This console followed
the PlayStation 2 and was released in 2006.
It’s sort of like the PlayStation 2 but moreso.

Sony Playstation 4

PS4

Well, what do you know: They keep making PlayStations. In fact, the PlayStation 5
was announced in 2019, but is still privately
traded; it is expected to IPO in late 2020.

PlayStation Portable

PSP

Symbol
PSX

Exchange
Trades as
NYSE
Phillips 66
(New York)
Phillips 66 engages in the processing, transportation,
storage, and marketing of fuels and other related products. The company operates through the following segments: Midstream, Chemicals, Refining and Marketing
& Specialties. Phillips 66 was founded on April 30, 2012
and is headquartered in Houston, TX.

PS3

FWB (Frank- Agilysys Inc.
furt)
Agilysys, Inc. operates as a technology company. It offers innovative software for point-of-sale, payment gateway, reservation and table management, guest offers
management, property management, inventory and procurement, analytics, document management, and mobile and wireless solutions and services to the hospitality industry. The firm also serves the gaming for both
corporate and tribal; hotels resort and cruise; foodservice management; and restaurants, universities, stadia,
and healthcare sectors. The company was founded in
1963 and is headquartered in Alpharetta, GA.
PS4

FWB (Frank- Phoenix Solar Aktiengefurt)
sellschaft
Phoenix Solar AG operates as holding company, which
engages in the development, manufacture, sale, and
operation of photovoltaic plants and systems. It operates through the following segments: USA, Middle East,
Asia/Pacific, Europe, and Holding Company. The company was founded on November 18, 1999 and is headquartered in Sulzemoos, Germany.

Unique for using optical discs for storing
its games, this 2005 portable disc-man was
fairly successful.
It is technically more
powerful than the contemporaneous Nintendo DS, but ultimately sold 80 million
fewer units than it.

NYSEARCA
Invesco Global Listed Private
(New York)
Equity ETF
The Invesco Global Listed Private Equity ETF (Fund) is
based on the Red Rocks Global Listed Private Equity Index (Index). The Fund will normally invest at least 90%
of its total assets in securities, which may include American depository receipts and global depository receipts,
that comprise the Index.

Nintendo Entertainment System

NES

NES

This grey 8-bit family computer from 1985
was the breakthrough console for Nintendo,
before we even knew that we would have to
keep getting consoles every few years. Several Nintendo franchises were born here:
Zelda, Metroid, Kirby, Punch-Out!!, and
Wii Fitness.

PSP

NYSEARCA
Nuverra Environmental So(New York)
lutions Inc.
Nuverra Environmental Solutions, Inc. engages in the
provision of water logistics and oilfield services. It focuses on the development and ongoing production of
oil and natural gas from shale formations in the United
States. It operates through the following segments:
Northeast Division, Southern Division, Rocky Mountain Division, and Corporate or other. The company
was founded on May 29, 2007 and is headquartered in
Scottsdale, AZ.

330

Console

Code

Symbol

Super Nintendo Entertainment Sys- SNES
tem
At the time this console was released in 1990,
it was believed that all progress followed a
trajectory consisting of X, Super X, Mega X,
Hyper X, Giga X, Ulimate X, and then X Infinite. This was eventually disproved by its
successor, the Nintendo 64.

SNES

Nintendo GameCube

GCN

GCN

Technically a rectangular prism, the GameCube is a tiny-disc–based system that followed the Nintendo 64. It was released in
2001.

Nintendo Switch

NS

Exchange

Trades as

NASDAQ
Senestech Inc.
(New York)
SenesTech, Inc. engages in the development and commercialization of a proprietary technology for the management of animal pest populations, primarily rat populations through fertility control. Its first fertility control product candidate is ContraPest. The company was
founded in July 2004 and is headquartered in Flagstaff,
AZ.
TSXV
Goldcliff Resource Corpora(Toronto)
tion
Goldcliff Resource Corp. operates as a mine development company, which engages in the acquisition, development and exploration of mineral properties. It holds
interest in the projects Panorama Ridge Gold, Ainsworth
Silver, and Pine Grove. The company was founded by
Leonard William Saleken in 1986 and is headquartered
in Vancouver, Canada.

Following the relatively unsuccessful and
confusingly-named Wii U, Nintendo somehow made itself quite relevant again with this
hybrid home/portable console (this is perhaps to what the “Switch” refers). Released
in 2017, its competition includes powerful
boxes such as the XBox One and PS4.

NYSE
NuStar Energy L.P.
(New York)
NuStar Energy L.P. engages in the terminalling, storage, and marketing of petroleum products. The company
also engages in the transportation of petroleum products
and anhydrous ammonia. It operates through three segments: Pipeline, Storage, and Fuels Marketing. NuStar
Energy L.P. was founded in 1999 and is headquartered
in San Antonio, Texas.

WonderSwan Color

WSC

WSC

The WonderSwan Color is a hand-held gaming console released by Bandai in 2000. It
followed the WonderSwan (makes sense) and
preceded the SwanCrystal (??). The console
was modestly successful in Japan with about
100 games, but ultimately lost in popularity
to Nintendo’s Game Boy Advance.

XBox One

XONE2

Microsoft is expert at creative counting. We
have Windows: 1, 2, 3, 3.1, 3.11 For Workgroups, 95, CE, 98, 98b, NT, ME, 2000, XP,
Server 2003, Vista, 7, 8, 8.1, 10. For XBox:
XBox, 360, One, One S, One X, Series X
(which will be called simply “XBox”). Only the
XBox One is publicly traded.

NS

NASDAQ
Willscot Corporation
(New York)
Willscot Corp. operates as a holding company, which
provides modular space and portable storage markets.
The firm offers furniture rental, transportation and logistics, storage & facilities services and commercial real
estate services. It also provides office trailers, portable
sales offices, modular complexes, and modular office
packages. The company was founded in 1944 and is
headquartered in Baltimore, MD.
XONE

NASDAQ
ExOne Co.
(New York)
The ExOne Co. engages in the development, manufacture, and marketing of 3D printing machines. It offers 3D printing solutions to industrial customers in the
aerospace, automotive, heavy equipment, energy, and oil
and gas industries. The company was founded in 1995
and is headquartered in North Huntingdon, PA.

Table 1: Isomorphism between video game consoles (left
columns) and stock market (right columns).

2 There

is not consensus on this abbreviation; Wikipedia uses XBO for example.

331

Figure 2: Market excitement on the day the experiment ends! The contract at the top
(Euro–US Dollar exchange rate) is well-behaved, with a bid–ask spread of half a basis
point. Ginger Beef Corp GB’s spread is nuts: It is the difference between a total valuation
of $1.13M and $2.54M Canadian. Genesis Healthcare has unlocked an achievement:
Trading was halted with the last sale at $1.44 (H1.44) due to its price dropping so much.

4

A watched dollar never bills

The next step of the process is to wait it out. If you wish to follow along, place an opaque
sheet of paper over the time-series in Figure 4 and move it at the desired pace, revealing
information about the performance of each investment. Major video-game events are
labeled on this figure, so you can also do a spit-take in real time as decade-tenured
Nintendo CEO Reggie Fils-Aimé steps down and is replaced by a man whose name is
really... Doug Bowser?

5

Putting my mouth where my money is

Finally, after a year of investing, it is time to divest and reap the monetary harvests.
Selling stocks is the simple inverse of buying them.
One surprise is that on March 9th , the first trading day after one year of holding, the
stock markets were very active (I think that this was excitement due to an announcement
of a new type of beer from the Corona beverage company?). Two issues arose (Figure 2):
• The bid–ask spread became quite wide. There are really two prices for a security:
The “bid” (the highest amount that someone is currently willing to pay to buy it)
and the “ask” (the lowest amount that someone is willing to sell it for). Gamers can
perhaps think of this like the “new” vs. “used” price for a console. When the market
is functioning correctly, these prices are typically within a few cents of one another.
The issue was even worse for symbols that have low volume (number of trades/day).

332

For example, GB had a bid of 0.085 CAD and an asking price of 0.190—more than
twice the price! Since I tried to get a favorable price when selling the portfolio, this
delayed some divestment for several days.
• Due to much market excitement, trading was halted for many securities! This happens automatically on some exchanges when the price changes more than some
amount (say, 15%) in one day, in order to prevent “flash crashes” (this is where an
SSD drive fails and loses important banking data). Halted trading of course also
delays divestment.
The final balance is displayed in Figure 3; these are the prices actually paid or proceeds
actually received.
Symbol
XONE
WSC
SNES
PSX
PSP
NS
NDSN
GG
GEN
NES
GCN
GB
PS3
PS4
Total:

Date
(2019)
3/8
3/8
3/8
3/8
3/8
3/8
3/8
3/8
3/8
3/8
3/8
3/8
3/12
3/12
5

Bought
(Num @ Price)
10 @ 9.302
10 @ 9.97
100 @ 1.0973
1 @ 94.1073
9 @ 11.1273
4 @ 26.88
1 @ 131.2673
10 @ 10.62
75 @ 1.36
11 @ 9.4973
1000 @ 0.08
500 @ 0.185
5 @ 18.29
3000 @ 0.0267
NaN

Cost Basis
93.02 USD
99.70 USD
109.73 USD
94.10 USD
100.15 USD
107.52 USD
131.27 USD
106.20 USD
102.00 USD
104.47 USD
80.00 CAD
92.50 CAD
91.45 EUR
80.10 EUR
1366.34 USD

Date
(2020)
3/9
3/9
3/9
3/9
3/9
3/9
3/9
3/9
3/9
3/9
3/9
3/12
3/9
3/10
91/20

Sold
(Num @ Price)
10 @ 5.1927
10 @ 13.17
5 @ 2.05
1 @ 65.601
9 @ 10.395
4 @ 16.29
1 @ 134.64
3 @ 49.16
75 @ 1.415
11 @ 2.53
1000 @ 0.12
500 @ 0.085
5 @ 22.2
3000 @ 0.013
NaN

P&L
(USD)
($41.09)
$32.00
($99.48)
($28.50)
($6.60)
($42.36)
$3.37
$41.28
$4.13
($76.64)
$29.29
($37.25)
$24.23
($45.46)
($243.08)

Figure 3: Actual transaction data from the experiment. Each security has a date on which
some quantity (around $100 USD) was bought (“Cost Basis”) and a date on which the
entirety was sold for some consequent profit or loss (P&L). Note that the number bought
is not always equal to the number sold. In the case of SNES, there was a 20:1 reverse
split (reverse splits happen to keep the price from getting embarrassingly low: It would
be 10.2¢ in this case). GG was acquired, so the sale was actually of a different symbol
(NEM). The P&L column and totals have been converted to USD using historic forex rates.
Prices omit transaction fees, which are significant especially for non-US exchanges.

6

Results

Most of these investments lost money, as did the overall portfolio. Both the SNES and
NES performed very badly, which was a disappointment to me, since these are my two
favorites. But this is exactly why this approach is needed: It removes the emotional

333

Purchase
PAX East
New
Nintendo
CEO

E3 2019
Fortnite
World Cup

PAX
West

Apple Arcade
Launches
PlayStation 5
Announced

Stadia
Launch

Xbox Series X
Announced

GDC 2020
Canceled

CES 2020

Sale

Figure 4: Relative daily closing price for the fourteen publicly-traded video game stocks
in the experiment. The closing price is adjusted in arrears for dividend payments and
splits. Each price is 1.0 on the date of acquisition (9 March 2019) and other closing
prices are given relative to this value. For example, GCN was briefly worth more than
twice its starting price at the end of September.

component. Both of these fell relentlessly throughout the year and lost more than 90%
of their value. The best overall were the Game Gear (GG) and WonderSwan Color (WSC)
with PlayStation 3 and GameCube also performing well.
It is interesting how little the game consoles react to major video game news (Figure 4).
The announcement of the PlayStation 5 in October didn’t seem to cause any movement;
this is especially odd for the PlayStation 4 whose value is normally quite volatile. One
hypothesis is that this information was already “priced in.” October was when the name
“PlayStation 5” was officially unveiled; the successor to the PlayStation 4 had been rumored for some time, but its name was a complete mystery. Arguably, only the cancelation
of GDC had any real effect on prices.3
3 Also the event marked “Sale”, which is when the experiment ended. It seems like a joke that this would
affect the market, but this is literally true for extremely low-volume stocks where my transaction may have been
a significant portion of the day’s activity. You can tell a low-volume stock from long flat horizontal lines.

334

7

Is this the most expensive SIGBOVIK paper?

Probably not.

8

Are video games a good investment?

The efficient market hypothesis tells us that every contract is priced according to the true
value of the underlying good. In this sense, every transaction is value-neutral, and money
can only be made or lost by predicting the future value of goods. With no information
about the thing being bought, as is the case for a basket of random stocks, the investment
should technically not gain or lose money (in expectation).
Of course, we do know another theorem about the stock market, from economics: The
market always goes up in the long-run! The US Government considers a year to be “long
term” for the sake of capital gains tax, so we should expect that randomly selected stocks
increase in value over this long term.
So why, in fact, did this investment lose 17.8% of its value? This is because the stocks
are not randomly selected; they represent video game consoles. Because video games are
high-tech items which rapidly become out of date, they actually tend to depreciate with
time. Only a few beloved classic consoles gained in value, for nostalgia reasons.
So this finally allows us to answer the question: What is the best game console? And
the answer, established with an elegant isomorphism:
All video game consoles are bad.

References
[1] Louis Bachelier. Théorie de la spéculation. Annales scientifiques de l’École Normale
Supérieure, 3e série, 17:21–86, 1900.
[2] Citigroup Global Markets Holdings Inc. VelocityShares Short LIBOR ETN. https:
//www.velocityshares.com/etns/product/dlbr/.
[3] The Mega-Man Homepage. Glossary: Game systems. http://www.mmhp.net/Glossary.
html.
[4] Vilfredo Pareto and AN Page. Manuale di economia politica (manual of political economy). Schwier, AS, Transl, 1906.
[5] Wikipedia. List of handheld game consoles. http://en.wikipedia.org/wiki/List_of_
handheld_game_consoles.
[6] Wikipedia. List of home video game consoles. http://en.wikipedia.org/wiki/List_
of_home_video_game_consoles.

335

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 30: What is the best game console?: A
free-market–based approach
Reviewer: Count Dracula, Sesame St, Cyberspace
Rating: Great execution of absurd idea.
Confidence: Based on counting irrelevant but objective things like vowel
pairs in the paper, absurdly high. But measures like that are
not confidence-inspiring, so on balance, low.

Masterful job of optimizing an “objective” measure by gaming the data.
This is a poster-child for the application of Goodhart’s Law and the vacuousness of so-called
“objective” rankings that blindly count stuff – like h-index and impact factors

336

Funny Business
38

Can a paper be written entirely in the title? 1. Introduction:
The title pretty much says it all. 2. Evaluation: It gets the job
done. However, the title is a little long. 3. Conclusion: Yes.
Daniel Gaston
Keywords: innovation, strength, progress

39

Erdös-Bacon-Sabbath numbers: Reductio ad absurdum

Maria Klawe et al. (incl. Harry Q. Bovik!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1
Keywords: Paul Erdös, Kevin Bacon, Black Sabbath

337

38

Can a Paper Be Written Entirely in the Title? 1. Introduction:
The Title Pretty Much Says it All. 2. Evaluation: It Gets the Job
Done. However, the Title is a Little Long. 3. Conclusion: Yes.
Daniel Gaston
University of Delaware

338

CONFIDENTIAL COMMITTEE MATERIALS

SIGBOVIK’20 3-Blind Paper Review
Paper 9: Can a paper be written entirely in the
title? 1. Introduction: The title pretty much
says it all. 2. Evaluation: It gets the job done.
However, the title is a little long. 3. Conclusion:
Yes.
Reviewer: Skirt Steak
Rating: 100%
Confidence: Kim Possibly Confident

While the immediate academic implications of this paper are deep and far-reaching, a more thorough analysis reveals that it also has carefully hidden, previously undiscovered lore on the hit
American television series Cory in the House, featuring Kyle Massey and Jason Dolley.
The first tip-off is found even before reaching the paper itself in the author’s pseudonym, which is
a pseudonym for ”Cory Baxter”. This is a subtle nod to Cory Baxter, the main character of Cory
in the House.
In the second paragraph, we find the real hidden jewel of lore. By taking the first letter of every
sentence in this paragraph and placing them in reverse order, a working URL is revealed. Each
time the URL is opened, it has a 50% to redirect to either of two pages: Webkinz.com, and the
Wikipedia page for Jeff Bezos. It is a little known fact that Cory’s favorite game is Webkinz. It
follows that Cory’s favorite multibillionaire American internet and aerospace entrepreneur, media
proprietor, and investor is Jeff Bezos.
An extra nugget of lore is planted in the final paragraph, in the last sentence, in the last letter.
By examining the lower edge of the bottom right serif with an electron microscope, you will find
Cory himself. This has huge implications for the hit American television series Cory in the House,
featuring Kyle Massey and Dan Schneider. The only explanation for this observation is that Cory
is only slightly taller than a hydrogen atom, and the entire series had to be recorded by shrinking
down the rest of the cast and set to an atomic size.

339

The effort required to make these details subtle yet discernible suggests that the author intended
to communicate them secretly to some as-of-yet unidentified group or individual. But the public
deserves to hear the truth.

340

Erdös-Bacon-Sabbath Numbers
Reductio ad Absurdum

39

Maria Klawe
Harvey Mudd College
Claremont CA 91711

Daniel V. Klein
Google
Pittsburgh PA 15206

D K Fackler
University of Edgestow
Thulcandra

Mary Shaw
Carnegie Mellon University
Pittsburgh PA 15203

Michael Ancas
Crazy Pants Productions
McDonald PA 15057

Augie Fackler
Google
Pittsburgh PA 15206

Sarah R. Allen
Google
Pittsburgh PA 15206

Harry Q. Bovik
Carnegie Mellon University
Doha Qatar

Abstract

Derivative Introduction [2]

A small Erdös number – the “collaborative
distance” of authorship between oneself and Paul
Erdös – has long been a source of pride for
mathematicians, computer scientists and other
geeks. Utilizing similar collaborative distance
metrics, a small Bacon number (the degree of
separation from Kevin Bacon) has been a source
of pride for actors, while a small Sabbath number
(the degree of separation from Black Sabbath) has
been a source of pride for musicians. Previous
research in Erdös-Bacon number minimization has
reduced the Erdös number of a number of
computer scientists to two, which is believed
optimal, although the reduction of the Bacon
number to four was clearly suboptimal. We extend
and improve on this previous work to provide a
Erdös-Bacon-Sabbath number minimization that is
believed to be close to optimal in all axes.

Paul Erdös co-authored nearly 1500 papers (until
his death in 1996), working with nearly 500
collaborators achieving the status of the most
prolific mathematician in modern times [4].
Mathematicians thus humorously defined Erdös
numbers. A person’s Erdös number is the distance
between that person and Paul Erdös in the
academic paper collaboration graph [3].
Succinctly, Paul Erdös is the unique person with
Erdös number zero; all of Erdös’ immediate
co-authors have Erdös number one; in general, if
you publish an academic paper with a collaborator
who has Erdös number N and none of your other
co-authors has Erdös number less than N, your
Erdös number is N + 1.
A similar Bacon number [16] has been proposed
for actor Kevin Bacon, except using collaborations
in film instead of collaborations in academic
papers. Likewise, a similar Sabbath number [12]
has been proposed to connect to the members of

341

the musical group Black Sabbath,
collaborations in musical performances.

using

Erdös-Bacon-Sabbath numbers were subsequently
defined [5] to be the sum of each person’s Erdös,
Bacon, and Sabbath numbers.
There is a long tradition of posthumous
publication [7], and authors claiming to have
collaborated with Erdös have brought his total
number of known publications to 1525, his
collaborator count to 511, and the Erdös number of
the chutzpah-bearing mathematician to one. The
latest publications co-written with Paul Erdös
appeared more than ten years after his death. With
additional rumored works in progress, Erdös’s
publication list is expected to grow. In fact, Paul
Erdös himself has published a solo work 15 years
after his death [8].
In this paper, we describe and demonstrate a
technique called Erdös-Bacon-Sabbath Number
Minimization.

Rules of the Games
The rules of Erdös number calculation are clear:
author a paper in a peer-reviewed publication,
either with someone connected by co-authorship
with Paul Erdös, or with Paul Erdös himself (the
latter being unlikely unless pre-demise work is
used in a posthumous publication, or if you are
better a communicating with the dead than Edgar
Cayce). Simply putting Paul Erdös’ name on your
paper does not count (and changing your name to
Paul Erdös for purposes of publication is definitely
cheating).

Bacon1, or with Kevin Bacon himself. We tried
contacting Kevin Bacon, but his agent refused to
put us in touch. We contemplated inserting a
fair-use clip of Kevin Bacon from an unrelated
movie, but knew that would be cheating (but we
did think of it).
Finally, the rules of Sabbath number calculation
[12] are also clear: connections between a
musician and a band or solo artist can only be
made if they actually performed or recorded
together. However, “session musicians” are valid
connections, so musicians who perform live or
record with an artist, but are not strictly committed
to that band are valid. A new recording [11] can
serve as adequate proof, but singing along to a
Black Sabbath record is cheating. We also
contemplated contacting Ozzy Osbourne, but he
postponed the 2019 tour that would have taken
him through the Pittsburgh area, and then
cancelled the tour altogether.
The theoretically achievable absolute minimum
Erdös-Bacon-Sabbath number is two: if Kevin
Bacon (who has a Bacon Number of zero) were to
become a member of Black Sabbath (thus
receiving a Sabbath number of zero), and was to
publish a paper with a person with an Erdös
number of one (since Paul Erdös is dead, and
ineligible as a co-author). The practically
achievable
minimum
Erdös-Bacon-Sabbath
number is four (by authoring a paper with a
someone with an Erdös number of one, appear in a
movie with Kevin Bacon, and perform with a
member of Black Sabbath), but the realistically
achievable minimum is somewhat higher than that.

The rules of Bacon number calculation [16] are
also clear: act in or be otherwise credited in a film
with someone connected by film credit with Kevin
1

The Internet Movie Database http://www.imdb.com is
typically used for verification of film/video/YouTube
credits.

342

Computation of Erdös, Bacon, and
Sabbath Numbers
In [1], Maria Klawe co-authored with Paul Erdös
and has an Erdös number of one, thus
guaranteeing that all authors on this paper have an
Erdös number no greater than two.
Daniel V. Klein has a Bacon number of 2, having
appeared in [9] with Steve Guttenberg, who
appeared in [10] with Kevin Bacon. Mike Ancas
also has a Bacon number of 2, having appeared as
an extra in [17] with Tom Hanks, who appeared in
[18] with Kevin Bacon. All authors on this paper
also appear in the documentary about the writing
of this paper [11], and thus have a Bacon number
no greater than three.
Additionally, Mike Ancas also has a Sabbath
number of 1, having been a member of the
Bloomsburg PA High School rock band The
Rubber Band. In 1971 The Rubber Band
performed as one of several warm-up acts for
Ronnie James Dio (who in 1979 became the lead
singer for Black Sabbath). The authors of this
paper have recorded a special musical piece,
composed specifically for this paper, and captured
in [11]. Their Sabbath number is therefore no
greater than two.2
The combined Erdös-Bacon-Sabbath number for
all the authors of this paper is therefore no greater
than seven, surpassing the rarified company of the
only three other people hitherto known to have an
Erdös-Bacon-Sabbath number of eight: Stephen
Hawking, Ray Kurzweil, and Daniel Levitin, a
professor of psychology and behavioral
2

Early research on low Sabbath Numbers included DK
Fackler, who has a Sabbath number of 2. DK performed
with Roger Daltrey [14] in 1994, who in turn performed
in Wembley Stadium with “The Who” at Live Aid
(1985), where Black Sabbath also performed [15].

neuroscience at McGill University [6]. They now
are tied with the previous record-low
Erdös-Bacon-Sabbath number (held by Lawrence
Krauss [13]) with an Erdös-Bacon-Sabbath
number of seven!
Not to brag or nothin’, but Maria Klawe (E=1,
B=3, S=2) and Daniel V. Klein (E=2, B=2, S=2)
have now beaten that record with an
Erdös-Bacon-Sabbath number of six. Finally, with
a near-optimal EBS number minimization, Mike
Ancas (E=2, B=2, S=1) has algorithmically
achieved an incredible Erdös-Bacon-Sabbath
number of five!

Sheet Music & Documentary
You’ve read the paper, now read the music (in the
appendix)! Lastly, you get to watch the
documentary at [11].

Acknowledgements
Thanks, Mom! Thanks, Dad! Thanks to spouse(s)
and/or partner(s), significant other(s), groupies,
and random tinder dates!

References
[1] Paul Erdös, Frank Harary, and Maria Klawe.
“Residually complete graphs.” In Proc. Sympos.
Combinatorial Mathematics and Optimal Design,
pages 117–123, Colorado State University, Fort
Collins, Colorado, 1978
[2] Charles Garrod, Maria Klawe, Iain Matthews,
et. magnum al. “An Algorithm for Erdos-Bacon
Number Minimization.” In A Record of The
Proceedings of SIGBOVIK 2010, pages 3-4,
Carnegie
Mellon
University,
Pittsburgh,
Pennsylvania, 2010

343

[3] Caspar Goffman. “What is your Erdös
number?” The American Mathematical Monthly,
76(7):791, 1969
[4] Jerry Grossman, Patrick Ion, and Rodrigo De
Castro. Erdös number project, 2010.
http://www.oakland.edu/enp/.
[5]
Richard
Sear.
“Erdos-Bacon-Sabbath
numbers.” Times Higher Education, 2016.
http://blogs.surrey.ac.uk/physics/2012/09/15/erdos
-bacon-sabbath-numbers/comment-page-1/
[6] Scott Seckel. “Rarified air: Do you have an
Erdos-Bacon-Sabbath number?” University of
Arizona, 2016.
https://asunow.asu.edu/20160126-creativity-lawre
nce-krauss-erdos-bacon-sabbath-score
[7] Myles McWeeney. “Voices from beyond the
grave - authors whose legacies live on through
posthumous publishing” The Independent
(Ireland), 2017.
https://www.independent.ie/entertainment/books/v
oices-from-beyond-the-grave-authors-whose-legac
ies-live-on-through-posthumous-publishing-35804
952.html
[8] Paul Erdös. “Some Problems On The
Distribution Of Prime Numbers.” In: Ricci G.
(eds) Teoria dei numeri. C.I.M.E. Summer
Schools, vol 5. Springer, Berlin, Heidelberg
https://doi.org/10.1007/978-3-642-10892-1_3

https://www.imdb.com/title/tt11833110/
https://youtu.be/QI6qkP3yLxw
[12] Bill M. “The Black Sabbath Game”, 2003
http://web.archive.org/web/20031121110828/http:/
/www.imjustabill.com/blacksabbathgame/index.ht
ml
[13] Scott Seckel. “This guy? He knows people”
Arizona State University, 2016.
https://sese.asu.edu/about/news/article/1645
[14] Roger Daltrey. Live in Chicago, 1994.
http://www.thewholive.net/concert/index.php?id=1
423
[15] Black Sabbath at “Live Aid”, 1985.
https://ultimateclassicrock.com/black-sabbath-live
-aid/
[16] Isabel Teotonio, “Google adds Six Degrees of
Kevin Bacon to search engine”, The Star (Toronto,
Canada), 2012.
https://www.thestar.com/entertainment/2012/09/13
/google_adds_six_degrees_of_kevin_bacon_to_se
arch_engine.html
[17] Marielle Heller, “A Beautiful Day in the
Neighborhood”, 2019
https://www.imdb.com/title/tt3224458/
[18] Ron Howard, “Apollo 13”,
https://www.imdb.com/title/tt0112384

[9] Ravi Godse. “Help Me Help You”, 2009.
https://www.imdb.com/title/tt1417297/
[10]
Barry
Levinson.
“Diner”,
https://www.imdb.com/title/tt0083833/

1982.

[11] Michael Ancas, Daniel Klein. “Creating a
Tiny Erdös-Bacon-Sabbath Number for Fun (and
No Profit) (2020)”

344

1995

Appendix: “Erdös Bacon Sabbath Number Reduction” in A Maj
Daniel V. Klein, op 2+2+3

345

346

